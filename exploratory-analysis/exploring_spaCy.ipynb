{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring spaCy\n",
    "Elena Cimino\n",
    "e.cimino@pitt.edu\n",
    "\n",
    "## Goal\n",
    "The goal for this notebook is to explore the Python library spaCy and compare it with NLTK. My project would benefit from POS-tagging and lemmatization and I have experience with NLTK. spaCy is new to me, and I would like to explore it a little and compare it to NLTK. \n",
    "\n",
    "Note: This was originally done in the jupyter notebook [exploring_balc.ipynb](https://nbviewer.jupyter.org/github/Data-Science-for-Linguists-2019/ESL-Article-Acquisition/blob/master/exploratory-analysis/BALC_clean.ipynb) but has since been moved here.\n",
    "\n",
    "### Table of Contents:\n",
    "1. [Setup](#setup): setting up the notebook, reading in files, loading libraries\n",
    "2. [Tokenization](#token): comparing spaCy's tokenization to NLTK's\n",
    "3. [Lemmatizing](#lemma): comparing spaCy's lemmatizing to NLTK's\n",
    "4. [Ambiguity](#ambiguity): comparing how spaCy and NLTK handle ambiguity\n",
    "5. [Conclusion](#conclusion): wrapping up what was found out in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## Set-up\n",
    "Loading in libraries, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint   # turn off pretty printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Level</th>\n",
       "      <th>Original_Text</th>\n",
       "      <th>Normalized_Essay</th>\n",
       "      <th>Revised_Essay</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_count</th>\n",
       "      <th>TTR</th>\n",
       "      <th>Guiraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200607296</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200607296\\n\\n\\n\\nNow I tell you...</td>\n",
       "      <td>Now I tell you why my worst holiday ever in th...</td>\n",
       "      <td>Now I tell you why my worst holiday ever in th...</td>\n",
       "      <td>[Now, I, tell, you, why, my, worst, holiday, e...</td>\n",
       "      <td>207</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>7.089490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200607457</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200607457\\n\\n\\n\\n              ...</td>\n",
       "      <td>My worst holiday Last year I have just had the...</td>\n",
       "      <td>My worst holiday Last year I have just had the...</td>\n",
       "      <td>[My, worst, holiday, Last, year, I, have, just...</td>\n",
       "      <td>180</td>\n",
       "      <td>0.572222</td>\n",
       "      <td>7.677167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200600487</td>\n",
       "      <td>5</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 5 200600487\\n\\n\\n\\n\\nEvery body i...</td>\n",
       "      <td>Every body in this life have a favourite posse...</td>\n",
       "      <td>Every body in this life have a favourite posse...</td>\n",
       "      <td>[Every, body, in, this, life, have, a, favouri...</td>\n",
       "      <td>229</td>\n",
       "      <td>0.445415</td>\n",
       "      <td>6.740350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Filename Level                                      Original_Text  \\\n",
       "0  200607296     3  \\t\\t\\t\\tCEPA 3 200607296\\n\\n\\n\\nNow I tell you...   \n",
       "1  200607457     4  \\t\\t\\t\\tCEPA 4 200607457\\n\\n\\n\\n              ...   \n",
       "2  200600487     5  \\t\\t\\t\\tCEPA 5 200600487\\n\\n\\n\\n\\nEvery body i...   \n",
       "\n",
       "                                    Normalized_Essay  \\\n",
       "0  Now I tell you why my worst holiday ever in th...   \n",
       "1  My worst holiday Last year I have just had the...   \n",
       "2  Every body in this life have a favourite posse...   \n",
       "\n",
       "                                       Revised_Essay  \\\n",
       "0  Now I tell you why my worst holiday ever in th...   \n",
       "1  My worst holiday Last year I have just had the...   \n",
       "2  Every body in this life have a favourite posse...   \n",
       "\n",
       "                                              tokens  token_count       TTR  \\\n",
       "0  [Now, I, tell, you, why, my, worst, holiday, e...          207  0.492754   \n",
       "1  [My, worst, holiday, Last, year, I, have, just...          180  0.572222   \n",
       "2  [Every, body, in, this, life, have, a, favouri...          229  0.445415   \n",
       "\n",
       "    Guiraud  \n",
       "0  7.089490  \n",
       "1  7.677167  \n",
       "2  6.740350  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cepa_df = pd.read_pickle('../private/cepa1.pkl')\n",
    "cepa_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='token'></a>\n",
    "## Exploring tokenization\n",
    "Because it's something new, let's start with the exploration of spaCy first. Then, we'll compare with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now now ADV RB\n",
      "I -PRON- PRON PRP\n",
      "tell tell VERB VBP\n",
      "you -PRON- PRON PRP\n",
      "why why ADV WRB\n",
      "my -PRON- ADJ PRP$\n",
      "worst bad ADJ JJS\n",
      "holiday holiday NOUN NN\n",
      "ever ever ADV RB\n",
      "in in ADP IN\n",
      "the the DET DT\n",
      "last last ADJ JJ\n",
      "summer summer NOUN NN\n",
      "I -PRON- PRON PRP\n",
      "wented went VERB VBD\n",
      "withe withe VERB VBP\n",
      "my -PRON- ADJ PRP$\n",
      "family family NOUN NN\n",
      "in in ADP IN\n",
      "the the DET DT\n",
      "India india PROPN NNP\n",
      "and and CCONJ CC\n",
      "this this DET DT\n",
      "story story NOUN NN\n",
      "I -PRON- PRON PRP\n",
      "will will VERB MD\n",
      "tell tell VERB VB\n",
      "you -PRON- PRON PRP\n",
      "what what NOUN WP\n",
      "happened happen VERB VBD\n",
      "for for ADP IN\n",
      "the the DET DT\n",
      "short short ADJ JJ\n",
      "story story NOUN NN\n",
      "when when ADV WRB\n",
      "I -PRON- PRON PRP\n",
      "go go VERB VBP\n",
      "the the DET DT\n",
      "first first ADJ JJ\n",
      "the the DET DT\n",
      "weathe weathe NOUN NN\n",
      "is be VERB VBZ\n",
      "very very ADV RB\n",
      "very very ADV RB\n",
      "rain rain NOUN NN\n",
      "now now ADV RB\n",
      "bady bady VERB VBP\n",
      "for for ADP IN\n",
      "the the DET DT\n",
      "children child NOUN NNS\n",
      "play play VERB VB\n",
      "out out PART RP\n",
      "when when ADV WRB\n",
      "I -PRON- PRON PRP\n",
      "go go VERB VBP\n",
      "in in ADP IN\n",
      "the the DET DT\n",
      "hotel hotel NOUN NN\n",
      "all all DET DT\n",
      "may may VERB MD\n",
      "family family NOUN NN\n",
      "was be VERB VBD\n",
      "have have VERB VB\n",
      "the the DET DT\n",
      "headk headk NOUN NN\n",
      "in in ADP IN\n",
      "there there ADV RB\n",
      "and and CCONJ CC\n",
      "all all DET DT\n",
      "was be VERB VBD\n",
      "sleep sleep NOUN NN\n",
      "put put VERB VBN\n",
      "for for ADP IN\n",
      "my -PRON- ADJ PRP$\n",
      "I -PRON- PRON PRP\n",
      "can;t can;t NOUN NN\n",
      "sleep sleep NOUN NN\n",
      "because because ADP IN\n",
      "I -PRON- PRON PRP\n",
      "not not ADV RB\n",
      "love love VERB VBP\n",
      "the the DET DT\n",
      "area area NOUN NN\n",
      "in in ADP IN\n",
      "the the DET DT\n",
      "morning morning NOUN NN\n",
      "all all ADJ PDT\n",
      "the the DET DT\n",
      "my -PRON- ADJ PRP$\n",
      "family family NOUN NN\n",
      "weak weak ADJ JJ\n",
      "up up PART RP\n",
      "and and CCONJ CC\n",
      "going go VERB VBG\n",
      "irant irant ADJ JJ\n",
      "but but CCONJ CC\n",
      "is be VERB VBZ\n",
      "the the DET DT\n",
      "strees stree NOUN NNS\n",
      ", , PUNCT ,\n",
      "children child NOUN NNS\n",
      "and and CCONJ CC\n",
      "the the DET DT\n",
      "food food NOUN NN\n",
      "is be VERB VBZ\n",
      "very very ADV RB\n",
      "dearty dearty ADJ JJR\n",
      "earia earia NOUN NN\n",
      "I -PRON- PRON PRP\n",
      "not not ADV RB\n",
      "liked like VERB VBD\n",
      "becuse becuse NOUN NN\n",
      "is be VERB VBZ\n",
      "not not ADV RB\n",
      "nice nice ADJ JJ\n",
      "area area NOUN NN\n",
      "so so ADV RB\n",
      "darty darty NOUN NN\n",
      "and and CCONJ CC\n",
      "people people NOUN NNS\n",
      "there there ADV EX\n",
      "is be VERB VBZ\n",
      "not not ADV RB\n",
      "nice nice ADJ JJ\n",
      "all all ADV RB\n",
      "there there ADV EX\n",
      "have have VERB VBP\n",
      "not not ADV RB\n",
      "happy happy ADJ JJ\n",
      "only only ADV RB\n",
      "sawted sawted ADJ JJ\n",
      "for for ADP IN\n",
      "my -PRON- ADJ PRP$\n",
      "sister sister NOUN NN\n",
      "whem whem PRON PRP\n",
      "she -PRON- PRON PRP\n",
      "take take VERB VBP\n",
      "some some DET DT\n",
      "flower flower NOUN NN\n",
      "for for ADP IN\n",
      "the the DET DT\n",
      "mam mam NOUN NN\n",
      "and and CCONJ CC\n",
      "also also ADV RB\n",
      "when when ADV WRB\n",
      "you -PRON- PRON PRP\n",
      "there there ADV RB\n",
      "you -PRON- PRON PRP\n",
      "see see VERB VBP\n",
      "what what NOUN WP\n",
      "I -PRON- PRON PRP\n",
      "means mean VERB VBZ\n",
      "maby maby VERB VBP\n",
      "some some DET DT\n",
      "body body NOUN NN\n",
      "liked like VERB VBD\n",
      "go go VERB VB\n",
      "ther ther NOUN NN\n",
      "but but CCONJ CC\n",
      "for for ADP IN\n",
      "I -PRON- PRON PRP\n",
      "did do VERB VBD\n",
      "n't not ADV RB\n",
      "liked like VERB VBN\n",
      "And and CCONJ CC\n",
      "for for ADP IN\n",
      "what what ADJ WDT\n",
      "happand happand NOUN NN\n",
      "I -PRON- PRON PRP\n",
      "tell tell VERB VBP\n",
      "for for ADP IN\n",
      "my -PRON- ADJ PRP$\n",
      "family family NOUN NN\n",
      "I -PRON- PRON PRP\n",
      "wanted want VERB VBD\n",
      "to to PART TO\n",
      "go go VERB VB\n",
      "in in ADP IN\n",
      "my -PRON- ADJ PRP$\n",
      "country country NOUN NN\n",
      "I -PRON- PRON PRP\n",
      "did do VERB VBD\n",
      "n't not ADV RB\n",
      "like like VERB VB\n",
      "her -PRON- PRON PRP\n",
      "fainally fainally ADV RB\n",
      "I -PRON- PRON PRP\n",
      "wanted want VERB VBD\n",
      "tell tell VERB VB\n",
      "for for ADP IN\n",
      "hem hem PRON PRP\n",
      "why why ADV WRB\n",
      "your -PRON- ADJ PRP$\n",
      "earea earea ADJ JJ\n",
      "like like INTJ UH\n",
      "thes the NOUN NNS\n",
      "you -PRON- PRON PRP\n",
      "shold shold ADJ JJ\n",
      "clean clean ADJ JJ\n",
      "and and CCONJ CC\n",
      "your -PRON- ADJ PRP$\n",
      "people people NOUN NNS\n",
      "so so ADP IN\n",
      "nerves nerve NOUN NNS\n",
      "... ... PUNCT .\n"
     ]
    }
   ],
   "source": [
    "y = cepa_df.Revised_Essay[0]\n",
    "doc = nlp(y)\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My -PRON- ADJ PRP$\n",
      "worst bad ADJ JJS\n",
      "holiday holiday NOUN NN\n",
      "Last last ADJ JJ\n",
      "year year NOUN NN\n",
      "I -PRON- PRON PRP\n",
      "have have VERB VBP\n",
      "just just ADV RB\n",
      "had have VERB VBN\n",
      "the the DET DT\n",
      "worst bad ADJ JJS\n",
      "holiday holiday NOUN NN\n",
      "ever ever ADV RB\n",
      ". . PUNCT .\n",
      "It -PRON- PRON PRP\n",
      "was be VERB VBD\n",
      "too too ADV RB\n",
      "board board NOUN NN\n",
      ". . PUNCT .\n",
      "My -PRON- ADJ PRP$\n",
      "Lonely lonely ADJ JJ\n",
      "sister sister NOUN NN\n",
      "had have VERB VBD\n",
      "got get VERB VBN\n",
      "married marry VERB VBN\n",
      ". . PUNCT .\n",
      "She -PRON- PRON PRP\n",
      "was be VERB VBD\n",
      "making make VERB VBG\n",
      "me -PRON- PRON PRP\n",
      "Laugh laugh PROPN NNP\n",
      "and and CCONJ CC\n",
      "play play VERB VB\n",
      "with with ADP IN\n",
      "me -PRON- PRON PRP\n",
      ". . PUNCT .\n",
      "But but CCONJ CC\n",
      "now now ADV RB\n",
      "I`m i`m VERB VB\n",
      "alone alone ADV RB\n",
      "with with ADP IN\n",
      "my -PRON- ADJ PRP$\n",
      "male male ADJ JJ\n",
      "brothers brother NOUN NNS\n",
      ". . PUNCT .\n",
      "I -PRON- PRON PRP\n",
      "ca can VERB MD\n",
      "nt not ADV RB\n",
      "stand stand VERB VB\n",
      "them -PRON- PRON PRP\n",
      "they -PRON- PRON PRP\n",
      "are be VERB VBP\n",
      "too too ADV RB\n",
      "noisy noisy ADJ JJ\n",
      ". . PUNCT .\n",
      "In in ADP IN\n",
      "the the DET DT\n",
      "Spring spring PROPN NNP\n",
      "holiday holiday NOUN NN\n",
      "my -PRON- ADJ PRP$\n",
      "brothers brother NOUN NNS\n",
      "and and CCONJ CC\n",
      "I -PRON- PRON PRP\n",
      "travelled travel VERB VBD\n",
      "to to ADP IN\n",
      "Australia australia PROPN NNP\n",
      "with with ADP IN\n",
      "our -PRON- ADJ PRP$\n",
      "parents parent NOUN NNS\n",
      ". . PUNCT .\n",
      "It -PRON- PRON PRP\n",
      "was be VERB VBD\n",
      "really really ADV RB\n",
      "great great ADJ JJ\n",
      "and and CCONJ CC\n",
      "nice nice ADJ JJ\n",
      "place place NOUN NN\n",
      ", , PUNCT ,\n",
      "but but CCONJ CC\n",
      "I -PRON- PRON PRP\n",
      "didn`t didn`t VERB VBP\n",
      "enjoyed enjoy VERB VBD\n",
      "it -PRON- PRON PRP\n",
      "because because ADP IN\n",
      "no no DET DT\n",
      "girls girl NOUN NNS\n",
      "were be VERB VBD\n",
      "with with ADP IN\n",
      "me -PRON- PRON PRP\n",
      ". . PUNCT .\n",
      "I -PRON- PRON PRP\n",
      "asked ask VERB VBD\n",
      "my -PRON- ADJ PRP$\n",
      "cousin cousin NOUN NN\n",
      "to to PART TO\n",
      "come come VERB VB\n",
      "with with ADP IN\n",
      "us -PRON- PRON PRP\n",
      "but but CCONJ CC\n",
      "she -PRON- PRON PRP\n",
      "refused refuse VERB VBD\n",
      "that that ADP IN\n",
      "because because ADP IN\n",
      "she -PRON- PRON PRP\n",
      "joined join VERB VBD\n",
      "a a DET DT\n",
      "sports sport NOUN NNS\n",
      "club club NOUN NN\n",
      ".There .there PUNCT ,\n",
      "was be VERB VBD\n",
      "alot alot NOUN NN\n",
      "of of ADP IN\n",
      "strange strange ADJ JJ\n",
      "animals animal NOUN NNS\n",
      ". . PUNCT .\n",
      "My -PRON- ADJ PRP$\n",
      "brother brother NOUN NN\n",
      "was be VERB VBD\n",
      "taking take VERB VBG\n",
      "photoes photoe NOUN NNS\n",
      "for for ADP IN\n",
      "the the DET DT\n",
      "animals animal NOUN NNS\n",
      "and and CCONJ CC\n",
      "I -PRON- PRON PRP\n",
      "was be VERB VBD\n",
      "too too ADV RB\n",
      "board board NOUN NN\n",
      ". . PUNCT .\n",
      "I -PRON- PRON PRP\n",
      "was be VERB VBD\n",
      "walking walk VERB VBG\n",
      "like like ADP IN\n",
      "a a DET DT\n",
      "sick sick ADJ JJ\n",
      "person person NOUN NN\n",
      ". . PUNCT .\n",
      "I -PRON- PRON PRP\n",
      "hated hat VERB VBD\n",
      "my -PRON- ADJ PRP$\n",
      "self self NOUN NN\n",
      "and and CCONJ CC\n",
      "prayed pray VERB VBD\n",
      "to to PART TO\n",
      "go go VERB VB\n",
      "back back ADV RB\n",
      "to to ADP IN\n",
      "our -PRON- ADJ PRP$\n",
      "country country NOUN NN\n",
      "to to PART TO\n",
      "see see VERB VB\n",
      "my -PRON- ADJ PRP$\n",
      "sister sister NOUN NN\n",
      ". . PUNCT .\n",
      "I -PRON- PRON PRP\n",
      "missed miss VERB VBD\n",
      "her -PRON- PRON PRP\n",
      "so so ADV RB\n",
      "much much ADV RB\n",
      ". . PUNCT .\n",
      "After after ADP IN\n",
      "few few ADJ JJ\n",
      "day day NOUN NN\n",
      "we -PRON- PRON PRP\n",
      "came come VERB VBD\n",
      "back back ADV RB\n",
      "home home ADV RB\n",
      "then then ADV RB\n",
      "I -PRON- PRON PRP\n",
      "became become VERB VBD\n",
      "the the DET DT\n",
      "happiest happy ADJ JJS\n",
      "person person NOUN NN\n",
      "in in ADP IN\n",
      "the the DET DT\n",
      "world world NOUN NN\n",
      ". . PUNCT .\n"
     ]
    }
   ],
   "source": [
    "z = cepa_df.Revised_Essay[1]\n",
    "doc = nlp(z)\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the essays have already been tokenized with NLTK, we can just look at the pos tags for those existing tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Now', 'RB'), ('I', 'PRP'), ('tell', 'VBP'), ('you', 'PRP'), ('why', 'WRB'), ('my', 'PRP$'), ('worst', 'JJS'), ('holiday', 'NN'), ('ever', 'RB'), ('in', 'IN'), ('the', 'DT'), ('last', 'JJ'), ('summer', 'NN'), ('I', 'PRP'), ('wented', 'VBD'), ('withe', 'JJ'), ('my', 'PRP$'), ('family', 'NN'), ('in', 'IN'), ('the', 'DT'), ('India', 'NNP'), ('and', 'CC'), ('this', 'DT'), ('story', 'NN'), ('I', 'PRP'), ('will', 'MD'), ('tell', 'VB'), ('you', 'PRP'), ('what', 'WDT'), ('happened', 'VBD'), ('for', 'IN'), ('the', 'DT'), ('short', 'JJ'), ('story', 'NN'), ('when', 'WRB'), ('I', 'PRP'), ('go', 'VBP'), ('the', 'DT'), ('first', 'JJ'), ('the', 'DT'), ('weathe', 'NN'), ('is', 'VBZ'), ('very', 'RB'), ('very', 'RB'), ('rain', 'RB'), ('now', 'RB'), ('bady', 'VBZ'), ('for', 'IN'), ('the', 'DT'), ('children', 'NNS'), ('play', 'VBP'), ('out', 'RP'), ('when', 'WRB'), ('I', 'PRP'), ('go', 'VBP'), ('in', 'IN'), ('the', 'DT'), ('hotel', 'NN'), ('all', 'DT'), ('may', 'MD'), ('family', 'NN'), ('was', 'VBD'), ('have', 'VBP'), ('the', 'DT'), ('headk', 'NN'), ('in', 'IN'), ('there', 'EX'), ('and', 'CC'), ('all', 'DT'), ('was', 'VBD'), ('sleep', 'JJ'), ('put', 'NN'), ('for', 'IN'), ('my', 'PRP$'), ('I', 'PRP'), ('can', 'MD'), (';', ':'), ('t', 'VB'), ('sleep', 'NN'), ('because', 'IN'), ('I', 'PRP'), ('not', 'RB'), ('love', 'VB'), ('the', 'DT'), ('area', 'NN'), ('in', 'IN'), ('the', 'DT'), ('morning', 'NN'), ('all', 'PDT'), ('the', 'DT'), ('my', 'PRP$'), ('family', 'NN'), ('weak', 'JJ'), ('up', 'RB'), ('and', 'CC'), ('going', 'VBG'), ('irant', 'JJ'), ('but', 'CC'), ('is', 'VBZ'), ('the', 'DT'), ('strees', 'NNS'), (',', ','), ('children', 'NNS'), ('and', 'CC'), ('the', 'DT'), ('food', 'NN'), ('is', 'VBZ'), ('very', 'RB'), ('dearty', 'JJ'), ('earia', 'NN'), ('I', 'PRP'), ('not', 'RB'), ('liked', 'VBD'), ('becuse', 'NN'), ('is', 'VBZ'), ('not', 'RB'), ('nice', 'JJ'), ('area', 'NN'), ('so', 'RB'), ('darty', 'JJ'), ('and', 'CC'), ('people', 'NNS'), ('there', 'EX'), ('is', 'VBZ'), ('not', 'RB'), ('nice', 'JJ'), ('all', 'RB'), ('there', 'RB'), ('have', 'VBP'), ('not', 'RB'), ('happy', 'JJ'), ('only', 'RB'), ('sawted', 'VBD'), ('for', 'IN'), ('my', 'PRP$'), ('sister', 'NN'), ('whem', 'NN'), ('she', 'PRP'), ('take', 'VB'), ('some', 'DT'), ('flower', 'NN'), ('for', 'IN'), ('the', 'DT'), ('mam', 'NN'), ('and', 'CC'), ('also', 'RB'), ('when', 'WRB'), ('you', 'PRP'), ('there', 'EX'), ('you', 'PRP'), ('see', 'VBP'), ('what', 'WP'), ('I', 'PRP'), ('means', 'VBP'), ('maby', 'JJ'), ('some', 'DT'), ('body', 'NN'), ('liked', 'VBD'), ('go', 'VB'), ('ther', 'RB'), ('but', 'CC'), ('for', 'IN'), ('I', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('liked', 'VBN'), ('And', 'CC'), ('for', 'IN'), ('what', 'WP'), ('happand', 'NN'), ('I', 'PRP'), ('tell', 'VBP'), ('for', 'IN'), ('my', 'PRP$'), ('family', 'NN'), ('I', 'PRP'), ('wanted', 'VBD'), ('to', 'TO'), ('go', 'VB'), ('in', 'IN'), ('my', 'PRP$'), ('country', 'NN'), ('I', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('like', 'IN'), ('her', 'PRP$'), ('fainally', 'RB'), ('I', 'PRP'), ('wanted', 'VBD'), ('tell', 'NN'), ('for', 'IN'), ('hem', 'JJ'), ('why', 'WRB'), ('your', 'PRP$'), ('earea', 'NN'), ('like', 'IN'), ('thes', 'NNS'), ('you', 'PRP'), ('shold', 'VBP'), ('clean', 'JJ'), ('and', 'CC'), ('your', 'PRP$'), ('people', 'NNS'), ('so', 'RB'), ('nerves', 'NNS'), ('...', ':')]\n"
     ]
    }
   ],
   "source": [
    "# now looking at nltk\n",
    "for token in [cepa_df.tokens[0]]:\n",
    "    print(nltk.pos_tag(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'PRP$'), ('worst', 'JJS'), ('holiday', 'NN'), ('Last', 'JJ'), ('year', 'NN'), ('I', 'PRP'), ('have', 'VBP'), ('just', 'RB'), ('had', 'VBN'), ('the', 'DT'), ('worst', 'JJS'), ('holiday', 'NN'), ('ever', 'RB'), ('.', '.'), ('It', 'PRP'), ('was', 'VBD'), ('too', 'RB'), ('board', 'NN'), ('.', '.'), ('My', 'NNP'), ('Lonely', 'RB'), ('sister', 'NN'), ('had', 'VBD'), ('got', 'VBN'), ('married', 'VBN'), ('.', '.'), ('She', 'PRP'), ('was', 'VBD'), ('making', 'VBG'), ('me', 'PRP'), ('Laugh', 'NNP'), ('and', 'CC'), ('play', 'NN'), ('with', 'IN'), ('me', 'PRP'), ('.', '.'), ('But', 'CC'), ('now', 'RB'), ('I', 'PRP'), ('`', '``'), ('m', 'VB'), ('alone', 'RB'), ('with', 'IN'), ('my', 'PRP$'), ('male', 'NN'), ('brothers', 'NNS'), ('.', '.'), ('I', 'PRP'), ('cant', 'VBP'), ('stand', 'VBP'), ('them', 'PRP'), ('they', 'PRP'), ('are', 'VBP'), ('too', 'RB'), ('noisy', 'JJ'), ('.', '.'), ('In', 'IN'), ('the', 'DT'), ('Spring', 'NN'), ('holiday', 'NN'), ('my', 'PRP$'), ('brothers', 'NNS'), ('and', 'CC'), ('I', 'PRP'), ('travelled', 'VBD'), ('to', 'TO'), ('Australia', 'NNP'), ('with', 'IN'), ('our', 'PRP$'), ('parents', 'NNS'), ('.', '.'), ('It', 'PRP'), ('was', 'VBD'), ('really', 'RB'), ('great', 'JJ'), ('and', 'CC'), ('nice', 'JJ'), ('place', 'NN'), (',', ','), ('but', 'CC'), ('I', 'PRP'), ('didn', 'VBP'), ('`', '``'), ('t', 'RB'), ('enjoyed', 'VBN'), ('it', 'PRP'), ('because', 'IN'), ('no', 'DT'), ('girls', 'NNS'), ('were', 'VBD'), ('with', 'IN'), ('me', 'PRP'), ('.', '.'), ('I', 'PRP'), ('asked', 'VBD'), ('my', 'PRP$'), ('cousin', 'NN'), ('to', 'TO'), ('come', 'VB'), ('with', 'IN'), ('us', 'PRP'), ('but', 'CC'), ('she', 'PRP'), ('refused', 'VBD'), ('that', 'IN'), ('because', 'IN'), ('she', 'PRP'), ('joined', 'VBD'), ('a', 'DT'), ('sports', 'NNS'), ('club', 'NN'), ('.There', 'NNP'), ('was', 'VBD'), ('alot', 'NN'), ('of', 'IN'), ('strange', 'JJ'), ('animals', 'NNS'), ('.', '.'), ('My', 'PRP$'), ('brother', 'NN'), ('was', 'VBD'), ('taking', 'VBG'), ('photoes', 'NNS'), ('for', 'IN'), ('the', 'DT'), ('animals', 'NNS'), ('and', 'CC'), ('I', 'PRP'), ('was', 'VBD'), ('too', 'RB'), ('board', 'NN'), ('.', '.'), ('I', 'PRP'), ('was', 'VBD'), ('walking', 'VBG'), ('like', 'IN'), ('a', 'DT'), ('sick', 'JJ'), ('person', 'NN'), ('.', '.'), ('I', 'PRP'), ('hated', 'VBD'), ('my', 'PRP$'), ('self', 'NN'), ('and', 'CC'), ('prayed', 'NN'), ('to', 'TO'), ('go', 'VB'), ('back', 'RB'), ('to', 'TO'), ('our', 'PRP$'), ('country', 'NN'), ('to', 'TO'), ('see', 'VB'), ('my', 'PRP$'), ('sister', 'NN'), ('.', '.'), ('I', 'PRP'), ('missed', 'VBD'), ('her', 'PRP'), ('so', 'RB'), ('much', 'JJ'), ('.', '.'), ('After', 'IN'), ('few', 'JJ'), ('day', 'NN'), ('we', 'PRP'), ('came', 'VBD'), ('back', 'RB'), ('home', 'NN'), ('then', 'RB'), ('I', 'PRP'), ('became', 'VBD'), ('the', 'DT'), ('happiest', 'JJS'), ('person', 'NN'), ('in', 'IN'), ('the', 'DT'), ('world', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for token in [cepa_df.tokens[1]]:\n",
    "    print(nltk.pos_tag(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenization looks pretty comparable across the two, which is great. What about just lemmatizing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lemma'></a>\n",
    "## Lemmatization\n",
    "Now, let's look at some of the lemmatizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now now\n",
      "I -PRON-\n",
      "tell tell\n",
      "you -PRON-\n",
      "why why\n",
      "my -PRON-\n",
      "worst bad\n",
      "holiday holiday\n",
      "ever ever\n",
      "in in\n",
      "the the\n",
      "last last\n",
      "summer summer\n",
      "I -PRON-\n",
      "wented went\n",
      "withe withe\n",
      "my -PRON-\n",
      "family family\n",
      "in in\n",
      "the the\n",
      "India india\n",
      "and and\n",
      "this this\n",
      "story story\n",
      "I -PRON-\n",
      "will will\n",
      "tell tell\n",
      "you -PRON-\n",
      "what what\n",
      "happened happen\n",
      "for for\n",
      "the the\n",
      "short short\n",
      "story story\n",
      "when when\n",
      "I -PRON-\n",
      "go go\n",
      "the the\n",
      "first first\n",
      "the the\n",
      "weathe weathe\n",
      "is be\n",
      "very very\n",
      "very very\n",
      "rain rain\n",
      "now now\n",
      "bady bady\n",
      "for for\n",
      "the the\n",
      "children child\n",
      "play play\n",
      "out out\n",
      "when when\n",
      "I -PRON-\n",
      "go go\n",
      "in in\n",
      "the the\n",
      "hotel hotel\n",
      "all all\n",
      "may may\n",
      "family family\n",
      "was be\n",
      "have have\n",
      "the the\n",
      "headk headk\n",
      "in in\n",
      "there there\n",
      "and and\n",
      "all all\n",
      "was be\n",
      "sleep sleep\n",
      "put put\n",
      "for for\n",
      "my -PRON-\n",
      "I -PRON-\n",
      "can;t can;t\n",
      "sleep sleep\n",
      "because because\n",
      "I -PRON-\n",
      "not not\n",
      "love love\n",
      "the the\n",
      "area area\n",
      "in in\n",
      "the the\n",
      "morning morning\n",
      "all all\n",
      "the the\n",
      "my -PRON-\n",
      "family family\n",
      "weak weak\n",
      "up up\n",
      "and and\n",
      "going go\n",
      "irant irant\n",
      "but but\n",
      "is be\n",
      "the the\n",
      "strees stree\n",
      ", ,\n",
      "children child\n",
      "and and\n",
      "the the\n",
      "food food\n",
      "is be\n",
      "very very\n",
      "dearty dearty\n",
      "earia earia\n",
      "I -PRON-\n",
      "not not\n",
      "liked like\n",
      "becuse becuse\n",
      "is be\n",
      "not not\n",
      "nice nice\n",
      "area area\n",
      "so so\n",
      "darty darty\n",
      "and and\n",
      "people people\n",
      "there there\n",
      "is be\n",
      "not not\n",
      "nice nice\n",
      "all all\n",
      "there there\n",
      "have have\n",
      "not not\n",
      "happy happy\n",
      "only only\n",
      "sawted sawted\n",
      "for for\n",
      "my -PRON-\n",
      "sister sister\n",
      "whem whem\n",
      "she -PRON-\n",
      "take take\n",
      "some some\n",
      "flower flower\n",
      "for for\n",
      "the the\n",
      "mam mam\n",
      "and and\n",
      "also also\n",
      "when when\n",
      "you -PRON-\n",
      "there there\n",
      "you -PRON-\n",
      "see see\n",
      "what what\n",
      "I -PRON-\n",
      "means mean\n",
      "maby maby\n",
      "some some\n",
      "body body\n",
      "liked like\n",
      "go go\n",
      "ther ther\n",
      "but but\n",
      "for for\n",
      "I -PRON-\n",
      "did do\n",
      "n't not\n",
      "liked like\n",
      "And and\n",
      "for for\n",
      "what what\n",
      "happand happand\n",
      "I -PRON-\n",
      "tell tell\n",
      "for for\n",
      "my -PRON-\n",
      "family family\n",
      "I -PRON-\n",
      "wanted want\n",
      "to to\n",
      "go go\n",
      "in in\n",
      "my -PRON-\n",
      "country country\n",
      "I -PRON-\n",
      "did do\n",
      "n't not\n",
      "like like\n",
      "her -PRON-\n",
      "fainally fainally\n",
      "I -PRON-\n",
      "wanted want\n",
      "tell tell\n",
      "for for\n",
      "hem hem\n",
      "why why\n",
      "your -PRON-\n",
      "earea earea\n",
      "like like\n",
      "thes the\n",
      "you -PRON-\n",
      "shold shold\n",
      "clean clean\n",
      "and and\n",
      "your -PRON-\n",
      "people people\n",
      "so so\n",
      "nerves nerve\n",
      "... ...\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(y)\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Now\n",
      "I I\n",
      "tell tell\n",
      "you you\n",
      "why why\n",
      "my my\n",
      "worst worst\n",
      "holiday holiday\n",
      "ever ever\n",
      "in in\n",
      "the the\n",
      "last last\n",
      "summer summer\n",
      "I I\n",
      "wented wented\n",
      "withe withe\n",
      "my my\n",
      "family family\n",
      "in in\n",
      "the the\n",
      "India India\n",
      "and and\n",
      "this this\n",
      "story story\n",
      "I I\n",
      "will will\n",
      "tell tell\n",
      "you you\n",
      "what what\n",
      "happened happened\n",
      "for for\n",
      "the the\n",
      "short short\n",
      "story story\n",
      "when when\n",
      "I I\n",
      "go go\n",
      "the the\n",
      "first first\n",
      "the the\n",
      "weathe weathe\n",
      "is is\n",
      "very very\n",
      "very very\n",
      "rain rain\n",
      "now now\n",
      "bady bady\n",
      "for for\n",
      "the the\n",
      "children child\n",
      "play play\n",
      "out out\n",
      "when when\n",
      "I I\n",
      "go go\n",
      "in in\n",
      "the the\n",
      "hotel hotel\n",
      "all all\n",
      "may may\n",
      "family family\n",
      "was wa\n",
      "have have\n",
      "the the\n",
      "headk headk\n",
      "in in\n",
      "there there\n",
      "and and\n",
      "all all\n",
      "was wa\n",
      "sleep sleep\n",
      "put put\n",
      "for for\n",
      "my my\n",
      "I I\n",
      "can can\n",
      "; ;\n",
      "t t\n",
      "sleep sleep\n",
      "because because\n",
      "I I\n",
      "not not\n",
      "love love\n",
      "the the\n",
      "area area\n",
      "in in\n",
      "the the\n",
      "morning morning\n",
      "all all\n",
      "the the\n",
      "my my\n",
      "family family\n",
      "weak weak\n",
      "up up\n",
      "and and\n",
      "going going\n",
      "irant irant\n",
      "but but\n",
      "is is\n",
      "the the\n",
      "strees strees\n",
      ", ,\n",
      "children child\n",
      "and and\n",
      "the the\n",
      "food food\n",
      "is is\n",
      "very very\n",
      "dearty dearty\n",
      "earia earia\n",
      "I I\n",
      "not not\n",
      "liked liked\n",
      "becuse becuse\n",
      "is is\n",
      "not not\n",
      "nice nice\n",
      "area area\n",
      "so so\n",
      "darty darty\n",
      "and and\n",
      "people people\n",
      "there there\n",
      "is is\n",
      "not not\n",
      "nice nice\n",
      "all all\n",
      "there there\n",
      "have have\n",
      "not not\n",
      "happy happy\n",
      "only only\n",
      "sawted sawted\n",
      "for for\n",
      "my my\n",
      "sister sister\n",
      "whem whem\n",
      "she she\n",
      "take take\n",
      "some some\n",
      "flower flower\n",
      "for for\n",
      "the the\n",
      "mam mam\n",
      "and and\n",
      "also also\n",
      "when when\n",
      "you you\n",
      "there there\n",
      "you you\n",
      "see see\n",
      "what what\n",
      "I I\n",
      "means mean\n",
      "maby maby\n",
      "some some\n",
      "body body\n",
      "liked liked\n",
      "go go\n",
      "ther ther\n",
      "but but\n",
      "for for\n",
      "I I\n",
      "did did\n",
      "n't n't\n",
      "liked liked\n",
      "And And\n",
      "for for\n",
      "what what\n",
      "happand happand\n",
      "I I\n",
      "tell tell\n",
      "for for\n",
      "my my\n",
      "family family\n",
      "I I\n",
      "wanted wanted\n",
      "to to\n",
      "go go\n",
      "in in\n",
      "my my\n",
      "country country\n",
      "I I\n",
      "did did\n",
      "n't n't\n",
      "like like\n",
      "her her\n",
      "fainally fainally\n",
      "I I\n",
      "wanted wanted\n",
      "tell tell\n",
      "for for\n",
      "hem hem\n",
      "why why\n",
      "your your\n",
      "earea earea\n",
      "like like\n",
      "thes thes\n",
      "you you\n",
      "shold shold\n",
      "clean clean\n",
      "and and\n",
      "your your\n",
      "people people\n",
      "so so\n",
      "nerves nerve\n",
      "... ...\n"
     ]
    }
   ],
   "source": [
    "for token in cepa_df.tokens[0]:\n",
    "    print(token, lemmatizer.lemmatize(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this instance, spaCy seems to outperform NLTK a bit. For example, it lemmatizes the word 'wented' as 'went', whereas NLTK lemmatizes this as 'wented'. Additionally, for the word 'strees', both NLTK and spaCy tagged it as a plural common noun ('NNS'), but NLTK's did not then lemmatize the word as 'stree', while spaCy did. NLTK also lemmatizes 'was' (past tense 'be') as 'wa' instead of 'be'. \n",
    "\n",
    "One thing that is rather annoying about spaCy's lemmatizing is that all pronouns are lemmatized as '-PRON-', whereas NLTK does lemmatize those as their own entries. So if someone was interested in examining pronouns, they may either want to avoid spaCy or write a function that would allow them to store anything as -PRON- as its text entry or something.\n",
    "\n",
    "Overall, I'm pretty happy with spaCy so far! It's also convenient that everything you need to use is in the library, so after you import it and load in your target language, it's fairly easy to use and streamlined.\n",
    "\n",
    "Let's keep chugging along though!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ambiguity'></a>\n",
    "## Ambiguity\n",
    "Here, I'll test two small sentences to see the difference in how spaCy and NLTK deal with ambiguity, and again compare the tagging and lemmatizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRP -PRON-\n",
      "like VBP like\n",
      "to TO to\n",
      "bow VB bow\n",
      "and CC and\n",
      "look VB look\n",
      "at IN at\n",
      "bows NNS bow\n",
      "on IN on\n",
      "presents NNS present\n",
      ". . .\n",
      "I PRP -PRON-\n",
      "wented VBD went\n",
      "to IN to\n",
      "a DT a\n",
      "store NN store\n",
      "on IN on\n",
      "Fifth NNP fifth\n",
      "Avenue NNP avenue\n",
      ". . .\n"
     ]
    }
   ],
   "source": [
    "# Make my own example, to test ambiguity\n",
    "test = \"I like to bow and look at bows on presents.\"\n",
    "test2 = \"I wented to a store on Fifth Avenue.\"\n",
    "\n",
    "# Spacy\n",
    "t = nlp(test)\n",
    "for tok in t:\n",
    "    print(tok, tok.tag_, tok.lemma_)\n",
    "    \n",
    "t = nlp(test2)\n",
    "for tok in t:\n",
    "    print(tok, tok.tag_, tok.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I\n",
      "like like\n",
      "to to\n",
      "bow bow\n",
      "and and\n",
      "look look\n",
      "at at\n",
      "bows bow\n",
      "on on\n",
      "presents present\n",
      ". .\n",
      "[('I', 'PRP'), ('like', 'VBP'), ('to', 'TO'), ('bow', 'VB'), ('and', 'CC'), ('look', 'VB'), ('at', 'IN'), ('bows', 'NNS'), ('on', 'IN'), ('presents.', 'NN')]\n",
      "I I\n",
      "wented wented\n",
      "to to\n",
      "a a\n",
      "store store\n",
      "on on\n",
      "Fifth Fifth\n",
      "Avenue Avenue\n",
      ". .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'), ('wented', 'VBD'), ('to', 'TO'), ('a', 'DT'), ('store', 'NN'), ('on', 'IN'), ('Fifth', 'NNP'), ('Avenue.', 'NNP')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK\n",
    "for tok in nltk.word_tokenize(test):\n",
    "    print(tok, lemmatizer.lemmatize(tok))\n",
    "print(nltk.tag.pos_tag(test.split()))\n",
    "\n",
    "for tok in nltk.word_tokenize(test2):\n",
    "    print(tok, lemmatizer.lemmatize(tok))\n",
    "nltk.tag.pos_tag(test2.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They both deal with ambiguity fairly well, and again the tagging seems comparable across both. SpaCy is a lot faster than NLTK though!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK and spaCy have reliable POS-tagging across the both of them, but from what I have seen, spaCy has a bit better lemmatizer than NLTK does. It's also faster, which is something to consider since I'm going to be using this on an entire corpus. It's a bit annoying that spaCy lemmatizes _any and all_ pronouns as -PRON- but I won't be looking at pronouns in this project, so it's not a huge dealbreaker for me. For the task of lemmatizing, I'll use spaCy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
