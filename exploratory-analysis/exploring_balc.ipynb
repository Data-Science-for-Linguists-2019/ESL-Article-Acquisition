{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elena Cimino, e.cimino@pitt.edu, 2019.02.12 - 2019.02.20\n",
    "\n",
    "# The Corpus: \n",
    "- BALC is a corpus compiled by researchers at the British University in Dubai of the writing of native Arabic speakers (L1 Arabic) in the United Arab Emirates who speak/have learned English as a Foreign Language (EFL). \n",
    "- The corpus has 1,865 texts that are handwritten. \n",
    "- All texts are available as plain text documents; some are also accompanied by jpeg image files of the essays.\n",
    "- There are several sources for the texts.\n",
    "\n",
    "# The Layout: \n",
    "- There are subdirectories with a folder that holds the files from each individual source. \n",
    "- There is a \"CEPA Images\" folder that has the images and texts for each text.\n",
    "- There is also a \"total\" folder that has every text in the corpus in this folder. \n",
    "- Therefore, each text in the corpus should be available in its individual folder as well as the \"total\" folder.\n",
    "- Additionally, each text from the CEPA examination should be available in _three_ different locations: \n",
    "    - The \"CEPA Images\" folder;\n",
    "    - The individual \"CEPA_#\" folder; \n",
    "    - The \"total\" folder.\n",
    "- Folders with a \"\\_em\" tag are tagged for student corrections\n",
    "    - CEPA levels 1 - 4 are tagged this way\n",
    "    - `<i>...</i>` = insertion\n",
    "    - `<o>...</o>` = emphasized/bolding\n",
    "    - `<x>...</x>` = cross out\n",
    "    \n",
    "# The Exploration: \n",
    "- Double-checking the format of the files in the different texts\n",
    "- Double-checking number of files and looking for repeated files\n",
    "- I'm mainly interested in the CEPA files right now, since the proficiency levels (1-6, with 1 being the lowest perceived proficiency level and 6 the highest) can be compared against the proficiency levels found in the other corpus I'm looking at: Pitt ELI Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%pprint            # to turn off pretty printing\n",
    "\n",
    "cor_dir = \"../private/BUiD Arab Learner Corpus v.1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the CEPA Images folder first. This folder has both .txt and .jpg files of the CEPA essays in the corpus.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CEPA Images folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1750"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['../private/BUiD Arab Learner Corpus v.1/CEPA Images/1/200604231.txt', '../private/BUiD Arab Learner Corpus v.1/CEPA Images/1/200602456.txt', '../private/BUiD Arab Learner Corpus v.1/CEPA Images/1/200600490.txt', '../private/BUiD Arab Learner Corpus v.1/CEPA Images/1/200605503.txt', '../private/BUiD Arab Learner Corpus v.1/CEPA Images/1/200604379.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# globbing \"CEPA images folder\"\n",
    "# Text files\n",
    "cepa_im_1 = glob.glob(cor_dir+'CEPA Images/1/*.txt')\n",
    "cepa_im_2 = glob.glob(cor_dir+'CEPA Images/2/*.txt')\n",
    "cepa_im_3 = glob.glob(cor_dir+'CEPA Images/3/*.txt')\n",
    "cepa_im_4 = glob.glob(cor_dir+'CEPA Images/4/*.txt')\n",
    "cepa_im_5 = glob.glob(cor_dir+'CEPA Images/5/*.txt')\n",
    "cepa_im_6 = glob.glob(cor_dir+'CEPA Images/6/*.txt')\n",
    "cepa_im = cepa_im_1 + cepa_im_2 + cepa_im_3 + cepa_im_4 + cepa_im_5 + cepa_im_6\n",
    "\n",
    "# JPG files\n",
    "cepa_pics_1 = glob.glob(cor_dir+'CEPA Images/1/*.jpg')\n",
    "cepa_pics_2 = glob.glob(cor_dir+'CEPA Images/2/*.jpg')\n",
    "cepa_pics_3 = glob.glob(cor_dir+'CEPA Images/3/*.jpg')\n",
    "cepa_pics_4 = glob.glob(cor_dir+'CEPA Images/4/*.jpg')\n",
    "cepa_pics_5 = glob.glob(cor_dir+'CEPA Images/5/*.jpg')\n",
    "cepa_pics_6 = glob.glob(cor_dir+'CEPA Images/6/*.jpg')\n",
    "cepa_pics = cepa_pics_1 + cepa_pics_2 + cepa_pics_3 + cepa_pics_4 + cepa_pics_5 + cepa_pics_6\n",
    "\n",
    "# Do they have the same amount of files?\n",
    "len(cepa_im)\n",
    "len(cepa_pics)\n",
    "cepa_im[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many text files are there in each level?\n",
    "len(cepa_im_1)\n",
    "len(cepa_im_2)\n",
    "len(cepa_im_3)\n",
    "len(cepa_im_4)\n",
    "len(cepa_im_5)\n",
    "len(cepa_im_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "298"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many jpg files are there in each level?\n",
    "len(cepa_pics_1)\n",
    "len(cepa_pics_2)\n",
    "len(cepa_pics_3)\n",
    "len(cepa_pics_4)\n",
    "len(cepa_pics_5)\n",
    "len(cepa_pics_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check and see if any files repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1750"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['200610622', '200612021', '200603474', '200612380', '200606487']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['200604231', '200602456', '200600490', '200605503', '200604379']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_pics = []\n",
    "for fn in cepa_pics:\n",
    "    x = re.findall(r'\\d{9}', fn)\n",
    "    if len(x) > 0:\n",
    "        im_pics.append(x[0])\n",
    "len(im_pics)\n",
    "im_pics[:5]\n",
    "\n",
    "im_texts = []\n",
    "for fn in cepa_im:\n",
    "    x = re.findall(r'\\d{9}', fn)\n",
    "    if len(x) > 0:\n",
    "        im_texts.append(x[0])\n",
    "len(im_texts)\n",
    "im_texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imt_fd = nltk.FreqDist(im_texts)\n",
    "imp_fd = nltk.FreqDist(im_pics)\n",
    "\n",
    "len([f for f in imt_fd if imt_fd[f] > 1])\n",
    "len([f for f in imp_fd if imp_fd[f] > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'200602841': 2, '200604231': 1, '200602456': 1, '200600490': 1, '200605503': 1, '200604379': 1, '200608556': 1, '200602047': 1, '200604608': 1, '200604620': 1, ...})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oh, there's just one, and it's in the text file folder, let's see which file it is and how many occurences there are\n",
    "imt_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../private/BUiD Arab Learner Corpus v.1/CEPA Images/4/200602841.txt: The best movie  that I have recently is (The Dafenshi Code) and one of the\n",
      "leading roles is played by a famouse novie star ( gorge clonie ) he plays the hero.\n",
      "The movie is about a super vikan that tries to speak the dafenshi book from the \n",
      "vadicane musiam but he misses up and gets tailled by a cope which tries\n",
      "toprevint he from using a code inthe book that is said to cause\n",
      "the end of the world (Armagedon) in the end good triamps over evil\n",
      "and every thing goes back to normal.I liked the movie because it`s \n",
      "about a mistrey that no one knew how to solve accept the evil vikan\n",
      "and because it`s based on history and real events,but I was abite\n",
      "disapoint ed because the ending was to easy to pradict I like the endings\n",
      "to be so twisted and unpradictable that you have to watch the movie 2 or\n",
      "3 times to under stand what really happined.Never the less I enjoyed\n",
      "the movie I liked it so nuch that I  saw it 6 limes in one week.\n",
      "\n",
      "../private/BUiD Arab Learner Corpus v.1/CEPA Images/5/200602841.txt: The best movie  that I have recently is (The Dafenshi Code) and one of the\n",
      "leading roles is played by a famouse novie star ( gorge clonie ) he plays the hero.\n",
      "The movie is about a super vikan that tries to speak the dafenshi book from the \n",
      "vadicane musiam but he misses up and gets tailled by a cope which tries\n",
      "toprevint he from using a code inthe book that is said to cause\n",
      "the end of the world (Armagedon) in the end good triamps over evil\n",
      "and every thing goes back to normal.I liked the movie because it`s \n",
      "about a mistrey that no one knew how to solve accept the evil vikan\n",
      "and because it`s based on history and real events,but I was abite\n",
      "disapoint ed because the ending was to easy to pradict I like the endings\n",
      "to be so twisted and unpradictable that you have to watch the movie 2 or\n",
      "3 times to under stand what really happined.Never the less I enjoyed\n",
      "the movie I liked it so nuch that I  saw it 6 limes in one week.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's loop through and get the names and contents of each file\n",
    "for file in cepa_im:\n",
    "    if '200602841' in file:\n",
    "        f = open(file)\n",
    "        txt = f.read()\n",
    "        f.close()\n",
    "        print(file+\":\", txt)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../private/BUiD Arab Learner Corpus v.1/CEPA Images/5/200602841.jpg\n"
     ]
    }
   ],
   "source": [
    "for f in cepa_pics:\n",
    "    if '200602841' in f:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are more pictures of essays than there are text files. In both cases, there are fewer higher level students, which isn't suprising. However, this discrepancy is a lot smaller in the .jpg file camp than in the text files. Not only that, but we're losing _a lot_ of data using just the text files -- 4 out of the 6 proficiency levels have less than half of the amount of files that are present in the .jpg files. Finally, there are two repeating files, which are identical but are in different proficiency levels: 4 and 5. Thanks to the .jpgs, we know it should be a level 5.\n",
    "\n",
    "Let's check out the other two folders and see what's going on with that. Do they have the missing files or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other CEPA file folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1676"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['../private/BUiD Arab Learner Corpus v.1/cepa1_em/CEPA 1 200611825.txt', '../private/BUiD Arab Learner Corpus v.1/cepa1_em/CEPA 1 200606381.txt', '../private/BUiD Arab Learner Corpus v.1/cepa1_em/CEPA 1 200603548.txt', '../private/BUiD Arab Learner Corpus v.1/cepa1_em/CEPA 1 200610508.txt', '../private/BUiD Arab Learner Corpus v.1/cepa1_em/CEPA 1 200603206.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# globbing each *separate* cepa folder \n",
    "cepa_sub_1 = glob.glob(cor_dir+'cepa1_em/*.txt')\n",
    "cepa_sub_2 = glob.glob(cor_dir+'cepa2_em/*.txt')\n",
    "cepa_sub_3 = glob.glob(cor_dir+'cepa3_em/*.txt')\n",
    "cepa_sub_4 = glob.glob(cor_dir+'cepa4_em/*.txt')\n",
    "cepa_sub_5 = glob.glob(cor_dir+'cepa5/*.txt')\n",
    "cepa_sub_6 = glob.glob(cor_dir+'cepa6/*.txt')\n",
    "cepa_sub = cepa_sub_1 + cepa_sub_2 + cepa_sub_3 + cepa_sub_4 + cepa_sub_5 + cepa_sub_6\n",
    "\n",
    "# Checking number of files and first 5\n",
    "len(cepa_sub)\n",
    "cepa_sub[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "293"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cepa_sub_1)\n",
    "len(cepa_sub_2)\n",
    "len(cepa_sub_3)\n",
    "len(cepa_sub_4)\n",
    "len(cepa_sub_5)\n",
    "len(cepa_sub_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1676"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# globbing \"total\" folder\n",
    "total = glob.glob(cor_dir+\"total/*.txt\")\n",
    "\n",
    "cepa_total = []\n",
    "for file in total:\n",
    "    cepa = re.compile(r'CEPA', flags=re.I)\n",
    "    matchobj = cepa.search(file)\n",
    "    if matchobj:\n",
    "        cepa_total.append(file)\n",
    "\n",
    "len(cepa_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "293"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cepa_tot_str = \" \".join(cepa_total)\n",
    "len(re.findall(r'\\s+1\\s+', cepa_tot_str))\n",
    "len(re.findall(r'\\s+2\\s+', cepa_tot_str))\n",
    "len(re.findall(r'\\s+3\\s+', cepa_tot_str))\n",
    "len(re.findall(r'\\s+4\\s+', cepa_tot_str))\n",
    "len(re.findall(r'\\s+5\\s+', cepa_tot_str))\n",
    "len(re.findall(r'\\s+6\\s+', cepa_tot_str))\n",
    "\n",
    "# Weird. Looks like we're losing two files for level 6, but everything else is the same -- including total files.\n",
    "# Let's investigate further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CEPA 200621158', 'CEPA 200619773']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maybe the two missing files don't have a level number after them?\n",
    "re.findall(r'CEPA \\d{9}', cepa_tot_str, flags=re.I)\n",
    "\n",
    "# No they don't. Let's look for them in cepa_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../private/BUiD Arab Learner Corpus v.1/cepa6/CEPA 200621158.txt:\n",
      "﻿\t\t\t\tCEPA 200621158\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t      The worst weekend ever!\n",
      "\n",
      "\n",
      "\n",
      "Last month I had the worst weekend ever in my life.\n",
      "I just came back from school, it was Wednesday at 1:10 pm, the moment I entered the house my father informed me of the death of a member of our family. We took off to Abu Dhabi to attend the funrel and stayed there untill 9:00 pm, then we went back home and I was really tired and sad. I went to bed the moment we reached home. The next day when I woke up I felt sick, so I went to the doctor who told me to stay in bed all day. I felt a lot better the next day but I didn’t have any chance to have some fun because I had to study for a math test on Saturday. It was a very  rough weekend but that’s how it turned up to be. I’m  hoping the next weekends are a lot more fun and having better news.\n",
      "\n",
      "\n",
      "../private/BUiD Arab Learner Corpus v.1/cepa6/CEPA 200619773.txt:\n",
      "﻿\t\t\t\tCEPA 200619773\n",
      "\n",
      "\n",
      "\n",
      "Summer  vacation, 2005, Australia. That would be the absolutue perfect vaction ever. It was mostly the well-known parts of Australia as well as the tourist regions that I visited and explored. Even some of the beautiful forests were open for visitors, so I took that chance and had a wonderful time exploring.\n",
      "I had gone with my best friend Alyazia, and it was the first time for the both of us to travel away from home by ourselves and without family. We had been friends since childhood and I guess that made the vacation so fabulous.\n",
      "Besides the famous tourist spots, like the Opera House, we went to several theme parks where we had so much fun going on rides and playing games and winning prizes. The zoos we went to were also a enjoyable where we saw the kangaroos and koalas and such animals. We did not forget to go to the movie halls as well, since Alyazia and I are such movie lovers, and we enjoyed the movies that we watched as well.\n",
      "What makes this vacation so special is the fact that I travelled to a wonderful country with my bestest friend and we had the opportunity to do everything that we love so much together. I hope that I can go on more vacations like this one in the future.\n",
      "\n",
      "\n",
      "../private/BUiD Arab Learner Corpus v.1/cepa6/CEPA 6 200619773.txt:\n",
      "﻿\t\t\t\tCEPA 6 200619773\n",
      "\n",
      "\n",
      "\n",
      "Summer  vacation, 2005, Australia. That would be the absolutue perfect vaction ever. It was mostly the well-known parts of Australia as well as the tourist regions that I visited and explored. Even some of the beautiful forests were open for visitors, so I took that chance and had a wonderful time exploring.\n",
      "I had gone with my best friend Alyazia, and it was the first time for the both of us to travel away from home by ourselves and without family. We had been friends since childhood and I guess that made the vacation so fabulous.\n",
      "Besides the famous tourist spots, like the Opera House, we went to several theme parks where we had so much fun going on rides and playing games and winning prizes. The zoos we went to were also a enjoyable where we saw the kangaroos and koalas and such animals. We did not forget to go to the movie halls as well, since Alyazia and I are such movie lovers, and we enjoyed the movies that we watched as well.\n",
      "What makes this vacation so special is the fact that I travelled to a wonderful country with my bestest friend and we had the opportunity to do everything that we love so much together. I hope that I can go on more vacations like this one in the future.\n",
      "\n",
      "\n",
      "../private/BUiD Arab Learner Corpus v.1/cepa6/CEPA 6 200621158.txt:\n",
      "﻿\t\t\t\tCEPA 6 200621158\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t      The worst weekend ever!\n",
      "\n",
      "\n",
      "\n",
      "Last month I had the worst weekend ever in my life.\n",
      "I just came back from school, it was Wednesday at 1:10 pm, the moment I entered the house my father informed me of the death of a member of our family. We took off to Abu Dhabi to attend the funrel and stayed there untill 9:00 pm, then we went back home and I was really tired and sad. I went to bed the moment we reached home. The next day when I woke up I felt sick, so I went to the doctor who told me to stay in bed all day. I felt a lot better the next day but I didn’t have any chance to have some fun because I had to study for a math test on Saturday. It was a very  rough weekend but that’s how it turned up to be. I’m  hoping the next weekends are a lot more fun and having better news.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fn in cepa_sub:\n",
    "    if '200621158' in fn:\n",
    "        f = open(fn)\n",
    "        txt = f.read()\n",
    "        f.close()\n",
    "        print(fn+\":\")\n",
    "        print(txt)\n",
    "        print()\n",
    "    elif '200619773' in fn:\n",
    "        f = open(fn)\n",
    "        txt = f.read()\n",
    "        f.close()\n",
    "        print(fn+\":\")\n",
    "        print(txt)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'../private/BUiD Arab Learner Corpus v.1/total/CEPA 6 200619773.txt' in cepa_total\n",
    "'../private/BUiD Arab Learner Corpus v.1/total/CEPA 6 200621158.txt' in cepa_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least we know they're the same file. We can check with the .jpg images in the image folder where it's actually supposed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to repeat the counter procedure from above \n",
    "\n",
    "# First with subfolder...\n",
    "nums_sub = []\n",
    "for fn in cepa_sub:\n",
    "    x = re.findall(r'\\d{9}', fn)\n",
    "    if len(x) > 0:\n",
    "        nums_sub.append(x[0])\n",
    "    \n",
    "# ... then for total folder\n",
    "nums_tot = []\n",
    "for fn in cepa_total:\n",
    "    x = re.findall(r'\\d{9}', fn)\n",
    "    if len(x) > 0:\n",
    "        nums_tot.append(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1671"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1671"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nums_sub)\n",
    "len(nums_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting number of occurences of each file name\n",
    "sub_fd = nltk.FreqDist(nums_sub)\n",
    "tot_fd = nltk.FreqDist(nums_tot)\n",
    "\n",
    "# Now checking numbers\n",
    "rep_s = [f for f in sub_fd if sub_fd[f] > 1]\n",
    "rep_t = [f for f in sub_fd if tot_fd[f] > 1]\n",
    "\n",
    "len(rep_s)\n",
    "len(rep_t)\n",
    "\n",
    "rep_s == rep_t\n",
    "\n",
    "# # Each has 12 repeating files, but at least they're the same files. And this is on top of the two repeated files from before, '200619773' and '200621158'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['200607856', '200607880', '200607857', '200607777', '200607875', '200607861', '200607902', '200607910', '200612324', '200611115', '200621158', '200619773']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the repeated files\n",
    "rep_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa1_em/CEPA 1 200607856.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa1_em/CEPA 1 200607880.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa1_em/CEPA 1 200607857.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa1_em/CEPA 1 200607777.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa1_em/CEPA 1 200607875.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa1_em/CEPA 1 200607861.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa1_em/CEPA 1 200607902.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa1_em/CEPA 1 200607910.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa2_em/CEPA 2 200612324.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa2_em/CEPA 2 200607856.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa2_em/CEPA 2 200607857.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa2_em/CEPA 2 200607880.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa2_em/CEPA 2 200607777.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa2_em/CEPA 2 200607910.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa2_em/CEPA 2  200612324.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa2_em/CEPA 2 200607902.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa2_em/CEPA 2 200607875.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa2_em/CEPA 2 200607861.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa3_em/CEPA 3  200611115.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa3_em/CEPA 3 200611115.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa6/CEPA 200621158.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa6/CEPA 200619773.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa6/CEPA 6 200619773.txt\n",
      "Repeated file: ../private/BUiD Arab Learner Corpus v.1/cepa6/CEPA 6 200621158.txt\n"
     ]
    }
   ],
   "source": [
    "# Now let's match the repeats to the full file names\n",
    "for item in cepa_sub:\n",
    "    if any(search in item for search in rep_s):\n",
    "        print(\"Repeated file:\", item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placement, per images: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607777.jpg\n",
      "Placement, per images: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607856.jpg\n",
      "Placement, per images: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607880.jpg\n",
      "Placement, per images: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607857.jpg\n",
      "Placement, per images: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200612324.jpg\n",
      "Placement, per images: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607902.jpg\n",
      "Placement, per images: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607861.jpg\n",
      "Placement, per images: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607875.jpg\n",
      "Placement, per images: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607910.jpg\n",
      "Placement, per images: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/3/200611115.jpg\n",
      "Placement, per images: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/6/200621158.jpg\n",
      "Placement, per images: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/6/200619773.jpg\n"
     ]
    }
   ],
   "source": [
    "# Where does the image file say to place them?\n",
    "for item in cepa_pics:\n",
    "    if any(search in item for search in rep_s):\n",
    "        print(\"Placement, per images:\", item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this is fun. Both the separate CEPA subfolders (cepa_sub) and the CEPA files in the total folder have the same number of files (1676). This is 74 fewer files than the .jpg in CEPA Images, but still more than the .txt in CEPA Images. __However__, several essays have been repeated.\n",
    "\n",
    "- In two different folders under the same name:\n",
    "    - 200607856 (1 & 2) -- .jpg is in __2__\n",
    "    - 200607880 (1 & 2) -- .jpg is in __2__\n",
    "    - 200607857 (1 & 2) -- .jpg is in __2__\n",
    "    - 200607777 (1 & 2) -- .jpg is in __2__\n",
    "    - 200607875 (1 & 2) -- .jpg is in __2__\n",
    "    - 200607861 (1 & 2) -- .jpg is in __2__\n",
    "    - 200607902 (1 & 2) -- .jpg is in __2__\n",
    "    - 200607910 (1 & 2) -- .jpg is in __2__\n",
    "- In the same proficiency folder but with 2 different name conventions\n",
    "    - 200612324 (`CEPA 2 200612324` and `CEPA 2  200612324`)\n",
    "    - 200611115 (`CEPA 3 200612324` and `CEPA 2  300612324`)\n",
    "    - 200621158 (`CEPA 200621158` and `CEPA 6 200621158`)\n",
    "    - 200619773 (`CEPA 200619773` and `CEPA 6 200619773`)\n",
    "    \n",
    "Not great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look into some of these repeated files\n",
    "Let's dig deeper and see what we can find..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_files = {}\n",
    "for file in cepa_sub:\n",
    "    f = open(file)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = file.rindex('/')+1\n",
    "    name = file[start:-4]\n",
    "    sub_files[name] = txt\n",
    "    \n",
    "cep_total_files = {}\n",
    "for file in cepa_total:\n",
    "    f = open(file)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = file.rindex('/')+1\n",
    "    name = file[start:-4]\n",
    "    cep_total_files[name] = txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1676"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1676"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_files.keys())\n",
    "len(cep_total_files.keys())\n",
    "\n",
    "# So two files were thrown out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿\t\t\t\tCEPA 5 200602841\n",
      "\n",
      "\n",
      "\n",
      "The best movie  that I have recently is (The Dafenshi Code) and one of the\n",
      "leading roles is played by a famouse movie star ( gorge clonie ) he plays the hero.\n",
      "The movie is about a super villan that tries to steall the dafenshi book from the  vadicane musiam but he misses up and gets tailled by a cope which tries toprevint he from using a code in the book that is said to cause the end of the world (Armagedon) in the end good triamps over evil\n",
      "and every thing goes back to normal.I liked the movie because it`s \n",
      "about a mistrey that no one knew how to solve accept the evil villan\n",
      "and because it`s based on history and real events,but I was abite\n",
      "disapoint ed because the ending was to easy to pradict I like the endings\n",
      "to be so twisted and unpradictable that you have to watch the movie 2 or\n",
      "3 times to under stand what really happined.Never the less I enjoyed\n",
      "the movie I liked it so nuch that I  saw it 6 limes in one week.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for f in sub_files.keys():\n",
    "    if '200602841' in f :\n",
    "        print(sub_files[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿\t\t\t\tCEPA 5 200602841\n",
      "\n",
      "\n",
      "\n",
      "The best movie  that I have recently is (The Dafenshi Code) and one of the\n",
      "leading roles is played by a famouse movie star ( gorge clonie ) he plays the hero.\n",
      "The movie is about a super villan that tries to steall the dafenshi book from the  vadicane musiam but he misses up and gets tailled by a cope which tries toprevint he from using a code in the book that is said to cause the end of the world (Armagedon) in the end good triamps over evil\n",
      "and every thing goes back to normal.I liked the movie because it`s \n",
      "about a mistrey that no one knew how to solve accept the evil villan\n",
      "and because it`s based on history and real events,but I was abite\n",
      "disapoint ed because the ending was to easy to pradict I like the endings\n",
      "to be so twisted and unpradictable that you have to watch the movie 2 or\n",
      "3 times to under stand what really happined.Never the less I enjoyed\n",
      "the movie I liked it so nuch that I  saw it 6 limes in one week.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for f in cep_total_files.keys():\n",
    "    if '200602841' in f :\n",
    "        print(sub_files[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like the level \"4\" repeated file was thrown out, which aligns with the observation from the .jpg files in CEPA Images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿\t\t\t\tCEPA 1 200607875\n",
      "\n",
      "\n",
      "\n",
      "on day way have to go to the Abu Dudai by my fathera and mather and sistere and brater why have Aba Debai its not good becouse my breter it’s acsdend by car I’ts not good becous Abudadai it’s a bad\n",
      "\n",
      "﻿\t\t\t\tCEPA 2 200607875\n",
      "\n",
      "\n",
      "\n",
      "on day way have to go to the Abu Dudai by my fathera and mather and sistere and brater why have Aba Debai its not good becouse my breter it’s acsdend by car I’ts not good becous Abudadai it’s a bad\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for f in cep_total_files.keys():\n",
    "    if '200607875' in f :\n",
    "        print(sub_files[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿\t\t\t\tCEPA 1 200607857\n",
      "\n",
      "\n",
      "\n",
      "The perfect holiday is verey important sapject in our life. In summer holiday I taravel to the tarche. I taravel to my faimly. I saw the talfrek and the park. I abavsayou to taravel thes cantre. I look forward to hearing from yours.\n",
      "\n",
      "﻿\t\t\t\tCEPA 2 200607857\n",
      "\n",
      "\n",
      "\n",
      "The perfect holiday is verey important sapject in our life. In summer holiday I taravel to the tarche. I taravel to my faimly. I saw the talfrek and the park. I abavsayou to taravel thes cantre. I look forward to hearing from yours.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for f in cep_total_files.keys():\n",
    "    if '200607857' in f :\n",
    "        print(sub_files[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿\t\t\t\tCEPA 2 200607910\n",
      "\n",
      "\n",
      "\n",
      "I think writing about Imagive you have just had the worst holiday ever! is very important becouce aplays an important rolx in our Life. No an adoubt that Imagive you have just had the worst holidayev! I the is no good the worst holiday ever. you went to zoo and park. They you went there with many friends and sister, father, mather and family. She eat many feed and druing milk. The go to the park foraday, and play a football and tennis and play with friend. I gave many apple and mant. In go to the zoo meany cat and cow in the zoo. I eat meay cats in the zoo. My freads play with my cat open the dour in the cat go in the open the drua. Is very import holiday the is it was so bad and friend go the doctor in the morining it very eary in the day. I hope reader I gave a bout just had the worst holiday ever! index.\n",
      "\n",
      "﻿\t\t\t\tCEPA 1 200607910\n",
      "\n",
      "\n",
      "\n",
      "I think writing about Imagive you have just had the worst holiday ever! is very important becouce aplays an important rolx in our Life. No an adoubt that Imagive you have just had the worst holidayev! I the is no good the worst holiday ever. you went to zoo and park. They you went there with many friends and sister, father, mather and family. She eat many feed and druing milk. The go to the park foraday, and play a football and tennis and play with friend. I gave many apple and mant. In go to the zoo meany cat and cow in the zoo. I eat meay cats in the zoo. My freads play with my cat open the dour in the cat go in the open the drua. Is very import holiday the is it was so bad and friend go the doctor in the morining it very eary in the day. I hope reader I gave a bout just had the worst holiday ever! index.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for f in cep_total_files.keys():\n",
    "    if '200607910' in f :\n",
    "        print(sub_files[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeff\\t\\t\\t\\tCEPA 2 200606678\\n\\n\\n\\nMy most beautiful place is a Al mimzer , thir is in Dubai , I can see there a s<x>a</x>e p<x>e</x>pals and famil is go to wok neer the sae, I like is pl<x>a</x>ce becuse is kwit and  li<x>k</x>e the s<x>a</x>e ev<x>a</x>ry we<x>a</x>kied <o> I</o> go the Al Mimzer , and the yng like is Ploce .\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\ufeff\\t\\t\\t\\tCEPA 2 200607910\\n\\n\\n\\nI think writing about Imagive you have just had the worst holiday ever! is very important becouce aplays an important rolx in our Life. No an adoubt that Imagive you have just had the worst holidayev! I the is no good the worst holiday ever. you went to zoo and park. They you went there with many friends and sister, father, mather and family. She eat many feed and druing milk. The go to the park foraday, and play a football and tennis and play with friend. I gave many apple and mant. In go to the zoo meany cat and cow in the zoo. I eat meay cats in the zoo. My freads play with my cat open the dour in the cat go in the open the drua. Is very import holiday the is it was so bad and friend go the doctor in the morining it very eary in the day. I hope reader I gave a bout just had the worst holiday ever! index.\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cep_total_files['CEPA 2 200606678']\n",
    "cep_total_files[\"CEPA 2 200607910\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay... the \\ufeff looks like an encoding thing. I can fix that next time around. The HTML tagging in the 200606678 file is the student correction tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My most beautiful place is a Al mimzer , thir is in Dubai , I can see there a sae pepals and famil is go to wok neer the sae, I like is place becuse is kwit and  like the sae evary weakied I go the Al Mimzer , and the yng like is ploce .'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\ufeff\\t\\t\\t\\tCEPA 2 200606678\\n\\n\\n\\nMy most beautiful place is a Al mimzer , thir is in Dubai , I can see there a s<x>a</x>e p<x>e</x>pals and famil is go to wok neer the sae, I like is pl<x>a</x>ce becuse is kwit and  li<x>k</x>e the s<x>a</x>e ev<x>a</x>ry we<x>a</x>kied <o> I</o> go the Al Mimzer , and the yng like is Ploce .\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's compare some of these files to the counterparts in the CEPA Images folder\n",
    "f = open('../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200606678.txt')\n",
    "txt = f.read()\n",
    "f.close()\n",
    "txt\n",
    "\n",
    "sub_files['CEPA 2 200606678']\n",
    "# so, no steps taken to do anything about student corrections (even crossing out) in the Images folder -- sub_files seems to be identical to total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I go withe my frineds to usa  and  i  go to  hilten  hotiel and i go to   walad Disny  and to the Deths with myfrind and i go  to   see  amaths  for footbaull amarican in the club.\\nther was   a wonderful plac .\\ni see a good pepulc  ina mavica. and  igo to  san francesco in is  a begear  valage in usa . thear are a may  peple in the valage   and    i see  a  dolphen and  clad reacing car itis for drges vease  it   is   very  beatful  the ar  a fauters car in  the club.\\nand   in  see  many restrants in  the  usa  and ther are a  cinestuwen thear ar many cincy in usa and many dooges  and agood  persnts.\\nand  i  see  in  usa  many  pepoles  its good peplets and  i go  to  the cinma   it  is  very bag  and many peples  in the cinma.\\nwin i  bak  to my  contrey  UAE  i remamber that and  in  tell   my  famile  abowt USA amd i teu may frinfs    in the  UAE about  USA and the good peple  in   .USA .\\nTher  wakand  its   good  becwas  the pepols  in   usa  it  in a  good pepols.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\ufeff        CEPA 3 200606782\\n\\n\\n\\n\\nI go withe my Frineds to USA  and  i  go to  hilton  hotiel and i go to   walad Disny  and to the Deths with myFrind and i go  to   see  amaths  for footbaull amarican in the club.\\nth<o>a</o>r was   a wonderful plac .\\ni see a good pepole  ina mavica. and  igo to  san fran cesco in is  a begear  valage in usa . thear are a mny  peple i<x>n</x>  the valage   and    i see  a  dolphen and  clad reacing car itis for drgey vease  i<o>t</o>  i<x>s</x>   very  beatful  the ar  a Fasters car in  the club.\\nand   in  see  many restrants in  the  usa  and ther are a  cinestuwen thear ar many cincy in usa and many dooges  and agood  persnts.\\nand  i  see  in  USA  many  pepoles  its good peplets and  i go  to  the cinma   it  is  very bag  and many peples  in the cinma.\\nwin i  bak  to my  contrey  UAE  i remamber that and  in  tell   my  famile  about USA amd i tell may frinfs in the UAE about  USA and the good peple  in   .US<o>a</o> .\\nTher  wakand  its   good  becwas  the pepols  in  USA  it  in a  good pepols. \\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\ufeff        CEPA 3 200606782\\n\\n\\n\\n\\nI go withe my Frineds to USA  and  i  go to  hilton  hotiel and i go to   walad Disny  and to the Deths with myFrind and i go  to   see  amaths  for footbaull amarican in the club.\\nth<o>a</o>r was   a wonderful plac .\\ni see a good pepole  ina mavica. and  igo to  san fran cesco in is  a begear  valage in usa . thear are a mny  peple i<x>n</x>  the valage   and    i see  a  dolphen and  clad reacing car itis for drgey vease  i<o>t</o>  i<x>s</x>   very  beatful  the ar  a Fasters car in  the club.\\nand   in  see  many restrants in  the  usa  and ther are a  cinestuwen thear ar many cincy in usa and many dooges  and agood  persnts.\\nand  i  see  in  USA  many  pepoles  its good peplets and  i go  to  the cinma   it  is  very bag  and many peples  in the cinma.\\nwin i  bak  to my  contrey  UAE  i remamber that and  in  tell   my  famile  about USA amd i tell may frinfs in the UAE about  USA and the good peple  in   .US<o>a</o> .\\nTher  wakand  its   good  becwas  the pepols  in  USA  it  in a  good pepols. \\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('../private/BUiD Arab Learner Corpus v.1/CEPA Images/3/200606782.txt')\n",
    "txt = f.read()\n",
    "f.close()\n",
    "txt\n",
    "\n",
    "# vs...\n",
    "cep_total_files['CEPA 3 200606782']\n",
    "\n",
    "# vs \n",
    "sub_files['CEPA 3 200606782']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So some inconsistencies as well in some capitalization here as well..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What we learned: \n",
    "The CEPA Images file has the fewest amount of CEPA text files, but it does have the full amount of essays as pictures. The text files in this folder are un-tagged. Next we have the subfolders for each level of CEPA with error corrections. These seem to be as the same as the files in the total file. Both of these files have much more than the CEPA Image text count, but still does not have all of them. Balancing across levels is more evenly distributed with these higher text counts.\n",
    "\n",
    "There are some naming convention concerns, mainly repeated files across and within proficiency levels in the CEPA folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
