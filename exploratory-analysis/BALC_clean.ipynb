{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BALC cleaning \n",
    "## Subset: CEPA files \n",
    "\n",
    "2019.02.08 — 2019.02.21\n",
    "\n",
    "## Summary of code\n",
    "- Created a corpus dictionary with the file name, original text, and perceived level of proficiency\n",
    "- Initial set up of cepa_df based off the corpus dictionary, which included\n",
    "    - Filenames\n",
    "    - Original Text\n",
    "- Cleaning up the text for cepa_df by making:\n",
    "    - A \"Normalized\" Text with standardized tagging (if applicable)\n",
    "    - A \"Revised\" Essay, which removes student correction tags and removes crossed out text\n",
    "- Building up cepa_df with: \n",
    "    - Tokenized essays (built off of the revised essay)\n",
    "    - Token counts\n",
    "    - TTR count\n",
    "- Making sure everything read in correctly\n",
    "- Visualizing a bit of data about the various levels (token_count, TTR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "Prior to February 21, 2019, I was having many issues with the coding system in the BALC corpus, since it is not consistent across all files, or sometimes even within files. Originally, I was trying to process the whole corpus from the \"BUiD Arab Learner Corpus v.1/total/\" file, which contains every text file in the corpus. What I came to realize is that most of the issues were with a relatively small part of the corpus. Out of 1,865 files in the corpus, non-CEPA files only clock in at 189. Additionally, these files do not come with any proficiency rating. While the proficiency or level rating for the CEPA files are better stated as something along the lines of _perceived proficiency of a student's rating_ (as opposed to be taken as a measure of the student's actual proficency with English), it still serves as a jumping off point and a measure to compare the writing of students in this corpus with students in other corpora.\n",
    "\n",
    "Therefore, I opted to work from the /total/ folder, for several reasons:\n",
    "- For one, it has more files than the subsets of CEPA Images folders, and the distribution of files across levels is more even in /total/ than in the CEPA Images folder (as can be seen in my exploration [here](https://github.com/Data-Science-for-Linguists-2019/ESL-Article-Acquisition/blob/master/exploratory-analysis/exploring_balc.ipynb) (compare outputs 3, 4, and 13).\n",
    "- It is easier to access each file in this file as opposed to the /cepa_#/ folders, which are separated into each individual level.\n",
    "- Finally, it has content tagged with student error corrections, which could be used to make the essays more comparable, given that these are handwritten essays.\n",
    "    - So if students realized they used the wrong word/letter and crossed it out, or realized they had forgotten a word and inserted one, this is marked in essays levels 1-4 on CEPA, but _not_ in the CEPA Images folder.\n",
    "    - In CEPA Images, student corrections are generally ignored, and whatever is written gets put in the text files without taking into account crossing out words/letters, for example.\n",
    "    - Again, this can be seen in my exploration [in output cell 31](https://github.com/Data-Science-for-Linguists-2019/ESL-Article-Acquisition/blob/master/exploratory-analysis/exploring_balc.ipynb).\n",
    "\n",
    "So in this notebook, I read in the CEPA files from the corpus, accounting for the \"bad\" (repeated) files, and then take efforts to normalize, \"revise\", and clean the essays. I also tokenize, get token counts and TTR, and start on some basic analysis (descriptive and quantitative). There's still more to do: POS-tagging, and maybe lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%pprint            # to turn off pretty printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../private/BUiD Arab Learner Corpus v.1/total/CEPA 3 200607296.txt', '../private/BUiD Arab Learner Corpus v.1/total/CEPA 4 200607457.txt', '../private/BUiD Arab Learner Corpus v.1/total/CEPA 5 200600487.txt', '../private/BUiD Arab Learner Corpus v.1/total/CEPA 4 200608016.txt', '../private/BUiD Arab Learner Corpus v.1/total/CEPA 1 200611825.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor_dir = \"../private/BUiD Arab Learner Corpus v.1/total/\"\n",
    "corpus = glob.glob(cor_dir+'*.txt')\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some duplicates in the total folder, which I'm using for the corpus root. The majority of these are cross-listed in two different proficiency levels (a few are in the same folder, but have two different file names with different spacing). \n",
    "\n",
    "I was able to locate the correctly labeled files (`clean_files`) in the CEPA Images folder, where all the original handwritten essays are located. Therefore, during my exploration of the corpus, I picked the duplicated (`bad_files`) as well as the clean files. I'll be using these two variables to point my code in the correct direction when it comes to these specific files, so that we don't have essays incorrectly labeled or duplicated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['200607856', '200607880', '200607857', '200607777', '200607875', '200607861', '200607902', '200607910', '200612324', '200611115', '200621158', '200619773']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607777.jpg', '../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607856.jpg', '../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607880.jpg', '../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607857.jpg', '../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200612324.jpg', '../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607902.jpg', '../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607861.jpg', '../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607875.jpg', '../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607910.jpg', '../private/BUiD Arab Learner Corpus v.1/CEPA Images/3/200611115.jpg', '../private/BUiD Arab Learner Corpus v.1/CEPA Images/6/200621158.jpg', '../private/BUiD Arab Learner Corpus v.1/CEPA Images/6/200619773.jpg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opening the pickle files that have the problem files... \n",
    "f_in = open(\"duplicated_files.pickle\",\"rb\")\n",
    "bad_files = pickle.load(f_in)\n",
    "f_in.close()\n",
    "\n",
    "# ... and the clean files\n",
    "f_in = open(\"clean_files.pickle\",\"rb\")\n",
    "clean_files = pickle.load(f_in)\n",
    "f_in.close()\n",
    "\n",
    "# checking the variables and making sure this looks right\n",
    "bad_files\n",
    "clean_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, those are looking good! `bad_files` is a list of the duplicated files, and `clean_files` is the list of the correct location. The `clean_files` gives us which location in total we should head to to get the file (e.g. level 2, if a file is cross-listed in levels 1 and 2).\n",
    "\n",
    "Let's make sure the code will spit out the correct file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607777.jpg\n",
      "use: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607856.jpg\n",
      "use: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607880.jpg\n",
      "use: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607857.jpg\n",
      "use: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200612324.jpg\n",
      "use: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607902.jpg\n",
      "use: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607861.jpg\n",
      "use: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607875.jpg\n",
      "use: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/2/200607910.jpg\n",
      "use: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/3/200611115.jpg\n",
      "use: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/6/200621158.jpg\n",
      "use: ../private/BUiD Arab Learner Corpus v.1/CEPA Images/6/200619773.jpg\n"
     ]
    }
   ],
   "source": [
    "# Making sure the code will pull the correct file for the duplicated files -- we're reading from /total/, but we can get the nececssary information from these filenames\n",
    "# we'll need the two numbers after \"CEPA Images\": the first number is the level, and the second number is the \"short\" file name we'll be using\n",
    "for item in clean_files:\n",
    "    if any(search in item for search in bad_files):\n",
    "        print(\"use:\", item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1652"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up a cepa dict\n",
    "corpus = glob.glob(cor_dir+'*.txt')\n",
    "corpus_dict = {}\n",
    "for file in corpus:\n",
    "    start = file.rindex('/')+1\n",
    "    name = file[start:-4]\n",
    "    x = re.findall(r'^((C|c).*?\\d{3,}( -)?)', name)   \n",
    "    if len(x) > 0:\n",
    "            x = x[0][0].split()\n",
    "            if x[-1] not in bad_files:  # duplicated files - ignoring them for now, will put in a separate dict and concatenate\n",
    "                f = open(file, encoding='utf-8-sig')   # fixes \\ufeff code at beginning of strings\n",
    "                txt = f.read()\n",
    "                f.close()\n",
    "                corpus_dict[x[-1]] = {\"Level\": x[1], \"Text\": txt}\n",
    "len(corpus_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Level': '1', 'Text': '\\t\\t\\t\\tCEPA 1 200601970\\n\\n\\n\\nYou have just had the perfect holiday you went Yaman There go <o>my  father and my brather. You</o> saw and did hadr<o>a</o> mot and sanaa moll. It was so wonderful m<o>y</o> famely\\n'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_dict[\"200601970\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_dict[\"200600215\"][\"Level\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Level': '5', 'Text': '\\t\\t\\t\\tCEPA 5 200600215\\n\\n\\n\\nLast summer holiday was the worst holiday I have ever had. It was bad holiday because evrythings happened suddenly and without any prepairing.\\nLast summer holiday, my family decided to spend  the holiday in India, so my father booked us a tickets to India. We all prepaired the bags for travelling on Tuesday.\\nWe were on the airport befor one hour of plane flying. We were told that the plane had something wrong and it would be late. We waited for three hours in the airport. Then, we flew to India. It took us three hours. When we arrived we started looking for taxi for a long time. After that we found small bus to take us to the hotel. \\nIn the way of the hotel, I saw many dogs in the street and I was afraid. In the entrence of the hotel there were many poor children with dirty cats. We spent this day in hotel because of  the bad weather.\\nNext day, I suggested going to the park. My father bought the lunch and took us to the park. We played in some games because the others were broken. In the afternoon, we became hungry so much but the food was bad and nobody ate it\\nIn the evening, while my brother played in the street with some children. he had an accident and his leg was broken. Endly, we left India with some bad memorise. \\n'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_dict[\"200600215\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '200607777'] ../private/BUiD Arab Learner Corpus v.1/total/CEPA 2 200607777.txt\n",
      "['2', '200607856'] ../private/BUiD Arab Learner Corpus v.1/total/CEPA 2 200607856.txt\n",
      "['2', '200607880'] ../private/BUiD Arab Learner Corpus v.1/total/CEPA 2 200607880.txt\n",
      "['2', '200607857'] ../private/BUiD Arab Learner Corpus v.1/total/CEPA 2 200607857.txt\n",
      "['2', '200612324'] ../private/BUiD Arab Learner Corpus v.1/total/CEPA 2 200612324.txt\n",
      "['2', '200607902'] ../private/BUiD Arab Learner Corpus v.1/total/CEPA 2 200607902.txt\n",
      "['2', '200607861'] ../private/BUiD Arab Learner Corpus v.1/total/CEPA 2 200607861.txt\n",
      "['2', '200607875'] ../private/BUiD Arab Learner Corpus v.1/total/CEPA 2 200607875.txt\n",
      "['2', '200607910'] ../private/BUiD Arab Learner Corpus v.1/total/CEPA 2 200607910.txt\n",
      "['3', '200611115'] ../private/BUiD Arab Learner Corpus v.1/total/CEPA 3 200611115.txt\n",
      "['6', '200621158'] ../private/BUiD Arab Learner Corpus v.1/total/CEPA 6 200621158.txt\n",
      "['6', '200619773'] ../private/BUiD Arab Learner Corpus v.1/total/CEPA 6 200619773.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\t\\t\\t\\tCEPA 2 200607777\\n\\n\\n\\nThe perfet holiday in UAE. You went suren cantres to one holiday. went with you me sister and mather. I’m see in Abu Dhabie “shate alraha” is a very nice sea and I went “Al maryna moll”. alfter is going to AlAin. it very nice, I went “  Mouten of hafied” and “mobazar AlKadra”. In Dubai went to the “meina salam” and “almaha” is a nice hotiel in Dubai. In RAK is go in masafi and see the water of masafi. wheth alfugaira isvery nise cantry because is have more of mouten”\\nThe ajnan amalguin is avery smoler than save cantres. it was so wonderful because I’m going xxx with father and sister. went to sven emarit. I’m very happey to see emarat in  one holiday.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is for the bad files. There should be twelve of them!\n",
    "mini = {}\n",
    "for item in clean_files:\n",
    "    if any(search in item for search in bad_files):\n",
    "        x = re.findall(r'\\d/\\d+', item)   # gets the level (/\\d/) and the file (\\d+) information\n",
    "        x = x[0]\n",
    "        x = x.split('/')\n",
    "        fn = cor_dir+\"CEPA \"+str(x[0])+\" \"+str(x[1])+\".txt\"\n",
    "        print(x, fn)\n",
    "        f = open(fn, encoding='utf-8-sig')\n",
    "        txt = f.read()\n",
    "        f.close()\n",
    "        mini[x[1]] = {\"Level\": x[0], \"Text\": txt}\n",
    "len(mini.keys())\n",
    "mini['200607777'][\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1664"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating corpus_dict and mini, making sure they have the right amount of keys\n",
    "z = {**corpus_dict, **mini}\n",
    "len(z.keys())\n",
    "\n",
    "# Yes! We have gone down from the original 1676 files found in the total and individual cepa_# folders, but that's because we took out the duplicates in those folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setting up the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1664, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200607296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200607457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200600487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200608016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200611825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Filename\n",
       "0  200607296\n",
       "1  200607457\n",
       "2  200600487\n",
       "3  200608016\n",
       "4  200611825"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cepa_df = pd.DataFrame(list(z.keys()), columns={\"Filename\"})\n",
    "cepa_df.shape\n",
    "cepa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDFs to get the level and the original text\n",
    "def get_level(file):\n",
    "    return z[file][\"Level\"]\n",
    "\n",
    "def get_text(file):\n",
    "    return z[file][\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Level</th>\n",
       "      <th>Original_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200607296</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200607296\\n\\n\\n\\nNow I tell you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200607457</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200607457\\n\\n\\n\\n              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200600487</td>\n",
       "      <td>5</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 5 200600487\\n\\n\\n\\n\\nEvery body i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200608016</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200608016\\n\\n\\n\\nEvery body hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200611825</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200611825\\n\\n\\n\\nyou go in the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Filename Level                                      Original_Text\n",
       "0  200607296     3  \\t\\t\\t\\tCEPA 3 200607296\\n\\n\\n\\nNow I tell you...\n",
       "1  200607457     4  \\t\\t\\t\\tCEPA 4 200607457\\n\\n\\n\\n              ...\n",
       "2  200600487     5  \\t\\t\\t\\tCEPA 5 200600487\\n\\n\\n\\n\\nEvery body i...\n",
       "3  200608016     4  \\t\\t\\t\\tCEPA 4 200608016\\n\\n\\n\\nEvery body hav...\n",
       "4  200611825     1  \\t\\t\\t\\tCEPA 1 200611825\\n\\n\\n\\nyou go in the ..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cepa_df[\"Level\"] = cepa_df.Filename.apply(get_level)\n",
    "cepa_df[\"Original_Text\"] = cepa_df.Filename.apply(get_text)\n",
    "cepa_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data up\n",
    "It's time to get clean up these files! We can see from the DataFrame and dict entries above that these files all have a lot of white space, as well as a header with the file name. We don't need that! \n",
    "\n",
    "In this section, I:\n",
    "- Remove excessive whitespace\n",
    "- Remove things like curly quotes \n",
    "- Standardize tagging\n",
    "    - `<o>...</o>` -> `_`\n",
    "    - `<i>...</i>` -> `^`\n",
    "    - `<x>...</x>` -> `~~`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making UDFs to clean up data a bit\n",
    "\n",
    "# Remove whitespace and strange characters found in some of the files\n",
    "def clean_text(txt):\n",
    "    \"\"\"Removes excessive whitespace, backticks, and curly quotes from a text.\"\"\"\n",
    "    txt = re.sub(r'[\\n\\t ]+', ' ', txt)\n",
    "    txt = re.sub(r'^`', '', txt)\n",
    "    txt = txt.replace('“','\"').replace('”','\"').replace(\"’\", \"'\")\n",
    "    txt = txt.strip()\n",
    "    return txt\n",
    "\n",
    "# Retag files, dealing with unclosed tags, and making it easier to remove these tags later -- instead of looking for 6+ opening and closing tags, I just need to look for underscores, carrots, and the double tildes tags\n",
    "def retag(essay):\n",
    "    \"\"\"Replaces tags for student emphasis (<o>, </o>)with '_' and removes any unecessary spaces between emphasized \n",
    "    letters in words. Replaces tags for student insertions (<i>, </i>) with '^' and removes any unecessary spaces \n",
    "    between letters in inserted words. Replaces cross-out tags (<x>, </x>) with '~~' Removes any left over brackets.\"\"\"\n",
    "    essay = re.sub(r'\\<i +', '<i>', essay)\n",
    "    essay = re.sub(r'<x +', '<x>', essay)\n",
    "    essay = re.sub(r'<o +', '<o>', essay)\n",
    "    essay = re.sub(r'<\\/o[^>]', '</o>', essay)\n",
    "    essay = re.sub(r'<o>(\\s)?', '_', essay)\n",
    "    essay = re.sub(r'(\\s)?<\\/o>', '_', essay)\n",
    "    essay = re.sub(r'<i>(\\s)?', '^', essay)\n",
    "    essay = re.sub(r'(\\s)?<\\/i>', '^', essay)\n",
    "    essay = re.sub(r'</?x>', '~~', essay)\n",
    "    essay = re.sub(r'>', ' ', essay)\n",
    "    essay = re.sub(r'<', ' ', essay)\n",
    "#    essay = re.sub(r'__', '_ _', essay)\n",
    "    return essay\n",
    "\n",
    "# Remove heading from files\n",
    "def un_head(txt):\n",
    "    \"\"\"Removes headers from text that include the file name, as well as student's names, grades, schools, etc.\"\"\"\n",
    "    cepa = re.compile(r'^C(EPA|EPa|epa)')      # removes cepa headers\n",
    "    if cepa.search(txt):\n",
    "        txt = re.sub(r'CEPA.*?\\d{2,}( ?-)?', '', txt, flags=re.I)   # there was one file that was CEPA ! filename\n",
    "    return txt\n",
    "\n",
    "# Apply all of these UDFs in a convenient, single function!\n",
    "def normalize_essay(txt):\n",
    "    txt = clean_text(txt)\n",
    "    txt = retag(txt)\n",
    "    txt = un_head(txt)\n",
    "    txt = txt.strip()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the UDFs out to make sure they work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t\\t\\t\\tCEPA 1 2006625\\n\\n\\n\\nwhat the activity is to ideas and to Play foot ball in the clamp and see to manh in T.V of foot ball Games Iam we play move foor ball in the zoo with may fired XXX I paly in the\\nFradey <o>a</o>t 4 coclek is too storing of tham am and to ray ferar<o>d</o>e  in the Zoo as behtFele sprts in any am and I go may fired to see foot ball in clarp in LAwqL Calmp Iam more afeh see footpall amy fired and go to the coFe see foot ball an gierk to cof and wentes and go to may clamp see may pephel play fotb<o>a</o>ll im clamp and me I kan paly more Gams in the comfotre Games\\nin cold places they culive by living with humans.\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"\\t\\t\\t\\tCEPA 2 200606375\\n\\n\\n\\nthe most beau<o>u</otiful pla<x>c</x>e you know in <o>A</o>l Hilly . I like go to the allh<x>i</x>lly because I'm play geam and look in the most beautiful . I play foot ball , tens basketball and play geam to t<x>h</x>e car . I'm look to the sea. I'm go to the AllHilly on frend .your go in the play <o>and aet</o>. I should you go in the All Hilly sawfa li<x>k</x>e and love . because in the be<x>a</x>utiful place. why I like it so much ? <x>beaucse</x> beautiful place and I'm go to AllHelly only Time. what you do eat ?? I'm eat to Hamborgar , Frish and Asier .Thnke you.\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original file\n",
    "cepa_df.Original_Text[137]\n",
    "cepa_df.Original_Text[1524]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CEPA 1 2006625 what the activity is to ideas and to Play foot ball in the clamp and see to manh in T.V of foot ball Games Iam we play move foor ball in the zoo with may fired XXX I paly in the Fradey <o>a</o>t 4 coclek is too storing of tham am and to ray ferar<o>d</o>e in the Zoo as behtFele sprts in any am and I go may fired to see foot ball in clarp in LAwqL Calmp Iam more afeh see footpall amy fired and go to the coFe see foot ball an gierk to cof and wentes and go to may clamp see may pephel play fotb<o>a</o>ll im clamp and me I kan paly more Gams in the comfotre Games in cold places they culive by living with humans.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"CEPA 2 200606375 the most beau<o>u</otiful pla<x>c</x>e you know in <o>A</o>l Hilly . I like go to the allh<x>i</x>lly because I'm play geam and look in the most beautiful . I play foot ball , tens basketball and play geam to t<x>h</x>e car . I'm look to the sea. I'm go to the AllHilly on frend .your go in the play <o>and aet</o>. I should you go in the All Hilly sawfa li<x>k</x>e and love . because in the be<x>a</x>utiful place. why I like it so much ? <x>beaucse</x> beautiful place and I'm go to AllHelly only Time. what you do eat ?? I'm eat to Hamborgar , Frish and Asier .Thnke you.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning it\n",
    "clean_text(cepa_df.Original_Text[137])\n",
    "clean_text(cepa_df.Original_Text[1524])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' what the activity is to ideas and to Play foot ball in the clamp and see to manh in T.V of foot ball Games Iam we play move foor ball in the zoo with may fired XXX I paly in the Fradey <o>a</o>t 4 coclek is too storing of tham am and to ray ferar<o>d</o>e in the Zoo as behtFele sprts in any am and I go may fired to see foot ball in clarp in LAwqL Calmp Iam more afeh see footpall amy fired and go to the coFe see foot ball an gierk to cof and wentes and go to may clamp see may pephel play fotb<o>a</o>ll im clamp and me I kan paly more Gams in the comfotre Games in cold places they culive by living with humans.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\" the most beau<o>u</otiful pla<x>c</x>e you know in <o>A</o>l Hilly . I like go to the allh<x>i</x>lly because I'm play geam and look in the most beautiful . I play foot ball , tens basketball and play geam to t<x>h</x>e car . I'm look to the sea. I'm go to the AllHilly on frend .your go in the play <o>and aet</o>. I should you go in the All Hilly sawfa li<x>k</x>e and love . because in the be<x>a</x>utiful place. why I like it so much ? <x>beaucse</x> beautiful place and I'm go to AllHelly only Time. what you do eat ?? I'm eat to Hamborgar , Frish and Asier .Thnke you.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Un-heading the cleaned file\n",
    "un_head(clean_text(cepa_df.Original_Text[137]))\n",
    "un_head(clean_text(cepa_df.Original_Text[1524]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' what the activity is to ideas and to Play foot ball in the clamp and see to manh in T.V of foot ball Games Iam we play move foor ball in the zoo with may fired XXX I paly in the Fradey _a_t 4 coclek is too storing of tham am and to ray ferar_d_e in the Zoo as behtFele sprts in any am and I go may fired to see foot ball in clarp in LAwqL Calmp Iam more afeh see footpall amy fired and go to the coFe see foot ball an gierk to cof and wentes and go to may clamp see may pephel play fotb_a_ll im clamp and me I kan paly more Gams in the comfotre Games in cold places they culive by living with humans.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\" the most beau_u_iful pla~~c~~e you know in _A_l Hilly . I like go to the allh~~i~~lly because I'm play geam and look in the most beautiful . I play foot ball , tens basketball and play geam to t~~h~~e car . I'm look to the sea. I'm go to the AllHilly on frend .your go in the play _and aet_. I should you go in the All Hilly sawfa li~~k~~e and love . because in the be~~a~~utiful place. why I like it so much ? ~~beaucse~~ beautiful place and I'm go to AllHelly only Time. what you do eat ?? I'm eat to Hamborgar , Frish and Asier .Thnke you.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retag(un_head(clean_text(cepa_df.Original_Text[137])))\n",
    "retag(un_head(clean_text(cepa_df.Original_Text[1524])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check `normalize()`. It goes a step farther by retagging the HTML codes. This was giving me a lot of trouble earlier! Sometimes it will link words that shouldn't be linked (e.g. \"\\_to_do\" instead of \"\\_to_ do\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Topic _A_ I will writing this a Prerf in far_mer_ in AlAin, who you were of father and mother and ster You did Planing foot dall . happened to watah camall and ive it was so had XXX Donka you like it so much'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_essay(cepa_df.Original_Text[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what the activity is to ideas and to Play foot ball in the clamp and see to manh in T.V of foot ball Games Iam we play move foor ball in the zoo with may fired XXX I paly in the Fradey _a_t 4 coclek is too storing of tham am and to ray ferar_d_e in the Zoo as behtFele sprts in any am and I go may fired to see foot ball in clarp in LAwqL Calmp Iam more afeh see footpall amy fired and go to the coFe see foot ball an gierk to cof and wentes and go to may clamp see may pephel play fotb_a_ll im clamp and me I kan paly more Gams in the comfotre Games in cold places they culive by living with humans.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_essay(cepa_df.Original_Text[137])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding on cepa_df\n",
    "\n",
    "In this section, I will:\n",
    "- Normalize essays with UDFs created in previous section\n",
    "- Make \"Revised_essay\", which removes all tagging\n",
    "- Tokenize (based on revised_essay)\n",
    "- Get token counts (based on revised_essay)\n",
    "- Get TTR (based on revised_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Level</th>\n",
       "      <th>Original_Text</th>\n",
       "      <th>Normalized_Essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200607296</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200607296\\n\\n\\n\\nNow I tell you...</td>\n",
       "      <td>Now I tell you why my worst holiday ever in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200607457</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200607457\\n\\n\\n\\n              ...</td>\n",
       "      <td>My worst holiday Last year I have just had the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200600487</td>\n",
       "      <td>5</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 5 200600487\\n\\n\\n\\n\\nEvery body i...</td>\n",
       "      <td>Every body in this life have a favourite posse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200608016</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200608016\\n\\n\\n\\nEvery body hav...</td>\n",
       "      <td>Every body have a lot ofpossessions in this li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200611825</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200611825\\n\\n\\n\\nyou go in the ...</td>\n",
       "      <td>you go in the oman just had the perfect holida...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Filename Level                                      Original_Text  \\\n",
       "0  200607296     3  \\t\\t\\t\\tCEPA 3 200607296\\n\\n\\n\\nNow I tell you...   \n",
       "1  200607457     4  \\t\\t\\t\\tCEPA 4 200607457\\n\\n\\n\\n              ...   \n",
       "2  200600487     5  \\t\\t\\t\\tCEPA 5 200600487\\n\\n\\n\\n\\nEvery body i...   \n",
       "3  200608016     4  \\t\\t\\t\\tCEPA 4 200608016\\n\\n\\n\\nEvery body hav...   \n",
       "4  200611825     1  \\t\\t\\t\\tCEPA 1 200611825\\n\\n\\n\\nyou go in the ...   \n",
       "\n",
       "                                    Normalized_Essay  \n",
       "0  Now I tell you why my worst holiday ever in th...  \n",
       "1  My worst holiday Last year I have just had the...  \n",
       "2  Every body in this life have a favourite posse...  \n",
       "3  Every body have a lot ofpossessions in this li...  \n",
       "4  you go in the oman just had the perfect holida...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cepa_df['Normalized_Essay'] = cepa_df.Original_Text.apply(normalize_essay)\n",
    "cepa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Level</th>\n",
       "      <th>Original_Text</th>\n",
       "      <th>Normalized_Essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200607296</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200607296\\n\\n\\n\\nNow I tell you...</td>\n",
       "      <td>Now I tell you why my worst holiday ever in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200607457</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200607457\\n\\n\\n\\n              ...</td>\n",
       "      <td>My worst holiday Last year I have just had the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200608016</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200608016\\n\\n\\n\\nEvery body hav...</td>\n",
       "      <td>Every body have a lot ofpossessions in this li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200611351</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200611351\\n\\n\\n\\nAl_ain mall is...</td>\n",
       "      <td>Al_ain mall is the most beautiful place. First...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>200603206</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\t\\t\\t\\tCEPA 1 200603206\\n\\n\\n\\n        Topi...</td>\n",
       "      <td>Topic _A_ I will writing this a Prerf in far_m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>200611379</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200611379\\n\\n\\n\\n\\nI spend my w...</td>\n",
       "      <td>I spend my weekend in Dubai. I go with my fami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>200611190</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200611190\\n\\n\\n\\nThere are many...</td>\n",
       "      <td>There are many things make the worst holiday f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>200606353</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200606353\\n\\n\\n\\nI m Love the U...</td>\n",
       "      <td>I m Love the UAE the UAE I am very very Love t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>200601400</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200601400\\n\\n\\n\\nI have just ha...</td>\n",
       "      <td>I have just had the worst holiday ever . The h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>200601414</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200601414\\n\\n\\n\\n&lt;o&gt;I am go to ...</td>\n",
       "      <td>_I am go to the_sumar holabuy in The Oman in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>200601954</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200601954\\n\\n\\n\\nI am go with m...</td>\n",
       "      <td>I am go with may friene_d_ to Oman. I am go wX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>200607721</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200607721\\n\\n\\n\\nIn the last ho...</td>\n",
       "      <td>In the last holid_a_y my family and my uncles ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>200600121</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200600121\\n\\n\\n\\nI’m going and ...</td>\n",
       "      <td>I'm going and _zoom_ there _with_ fimi_l_ you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>200606833</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200606833\\n\\n\\n\\nI went the Ome...</td>\n",
       "      <td>I went the Omen. Because is wonderful and a go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>200611556</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200611556\\n\\n\\n\\nI spend my hol...</td>\n",
       "      <td>I spend my holiday is Sauid Arabia. I want wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>200607306</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200607306\\n\\n\\n\\nI waite trevel...</td>\n",
       "      <td>I waite trevel wath my famliy to AlAin to see ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>200606756</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200606756\\n\\n\\n\\nTopic B : We g...</td>\n",
       "      <td>Topic B : We go with my family by car in a ^Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>200605920</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200605920\\n\\n\\n\\nMy live in the...</td>\n",
       "      <td>My live in the Shajohe, A house is very beauti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>200605692</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200605692\\n\\n\\n\\nEvery body kno...</td>\n",
       "      <td>Every body know that the summer holiday is the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>200611608</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200611608\\n\\n\\n\\nLast weekend I...</td>\n",
       "      <td>Last weekend Im went to Dudai with my Frinds i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>200611835</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200611835\\n\\n\\n\\nIs vary bast f...</td>\n",
       "      <td>Is vary bast filme you have seen recentry. The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>200605673</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200605673\\n\\n\\n\\n        Sport....</td>\n",
       "      <td>Sport.... The Sport is very important in peopl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>200601415</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200601415\\n\\n\\n\\nlast week &lt;o&gt;i...</td>\n",
       "      <td>last week _in the End i_t is verey bad the hol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>200612469</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200612469\\n\\n\\n\\nI writ thise t...</td>\n",
       "      <td>I writ thise topic about Imagin you have just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>200605115</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200605115\\n\\n\\n\\nThe last holid...</td>\n",
       "      <td>The last holiday Imagine went you _have_ just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>200607898</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200607898\\n\\n\\n\\nIn the last su...</td>\n",
       "      <td>In the last summar I have spent awery wonderfu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>200607268</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200607268\\n\\n\\n\\nIn the summer ...</td>\n",
       "      <td>In the summer 2004 I went to bahrain with my f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>200610880</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200610880\\n\\n\\n\\n\\n\\nIn  the ho...</td>\n",
       "      <td>In the holiday , My family decided go to the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>200611385</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200611385\\n\\n\\n\\nIn summer holi...</td>\n",
       "      <td>In summer holiday I went with my fami_lly_ to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>200607326</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200607326\\n\\n\\n\\nLast summer &lt;x...</td>\n",
       "      <td>Last summer ~~I weih~~ I went to Syria with my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>200607854</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200607854\\n\\n\\n\\nIn the last ye...</td>\n",
       "      <td>In the last year I have jast had the worst hol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>200607840</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200607840\\n\\n\\n\\nReally I love ...</td>\n",
       "      <td>Really I love the Indian films because it took...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>200606375</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200606375\\n\\n\\n\\nthe most beau&lt;...</td>\n",
       "      <td>the most beau_u_iful pla~~c~~e you know in _A_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>200602894</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCepa 1 200602894\\n\\n\\n\\nYou went In Om...</td>\n",
       "      <td>You went In Oman you have just has the perfect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>200607303</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200607303\\n\\n\\n\\nThe main reaso...</td>\n",
       "      <td>The main reason that people travel in holidaye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>200606605</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200606605\\n\\n\\n\\nI love play fo...</td>\n",
       "      <td>I love play football . I play with my friend ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>200608018</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200608018\\n\\n\\n\\nThere are many...</td>\n",
       "      <td>There are many favourite thing`s in my Life bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>200609733</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200609733\\n\\n\\n\\n&lt;o&gt;Sea in UAE ...</td>\n",
       "      <td>_Sea in UAE I like the sea I look the fish and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>200611836</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200611836\\n\\n\\n\\n\\t\\t\\t\\tThe wo...</td>\n",
       "      <td>The worst holiday Imagine my holiday in the Ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>200612442</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200612442\\n\\n\\n\\nLast xxx week ...</td>\n",
       "      <td>Last xxx week I am see a good movie or film. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>200607842</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200607842\\n\\n\\n\\n&lt;o&gt;T&lt;/o&gt;hese  ...</td>\n",
       "      <td>_T_hese holiday is worst holiday I spend it . ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>200601958</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200601958\\n\\n\\n\\nMy car the my ...</td>\n",
       "      <td>My car the my car is ver_y_ fast the name my c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>200601970</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200601970\\n\\n\\n\\nYou have just ...</td>\n",
       "      <td>You have just had the perfect holiday you went...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>200600313</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200600313\\n\\n\\n\\nI me give a lo...</td>\n",
       "      <td>I me give a lot than Iam gin_e_ you have just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>200606782</td>\n",
       "      <td>3</td>\n",
       "      <td>CEPA 3 200606782\\n\\n\\n\\n\\nI go withe m...</td>\n",
       "      <td>I go withe my Frineds to USA and i go to hilto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>200612085</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200612085\\n\\n\\n\\n\\nMy perfect h...</td>\n",
       "      <td>My perfect holiday was in Last year. It was in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>200606796</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200606796\\n\\n\\n\\nThe weekend in...</td>\n",
       "      <td>The weekend in the summer I am go to visit , I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>200600677</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200600677\\n\\n\\n\\nshe have just ...</td>\n",
       "      <td>she have just ha_d_ the perfect. went go to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>200604411</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200604411\\n\\n\\n\\nThe ar&lt;o&gt;e&lt;/o&gt;...</td>\n",
       "      <td>The ar_e_ simr it flmatd . the you went smie ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>200607310</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200607310\\n\\n\\n\\nIn one day I w...</td>\n",
       "      <td>In one day I was in holiday . The holiday was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>200605068</td>\n",
       "      <td>1</td>\n",
       "      <td>`\\t\\t\\t\\tCEPA 1 200605068\\n\\n\\n\\nOn The Samir ...</td>\n",
       "      <td>On The Samir i go fo The sliame will i sea fal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>200605846</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200605846\\n\\n\\n\\n              ...</td>\n",
       "      <td>The most beautiful place I was choose this top...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>200601971</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA  1 200601971\\n\\n\\n\\nwhere you wen...</td>\n",
       "      <td>where you went in the we XXX. xxx in The zoo w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1628</th>\n",
       "      <td>200605336</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200605336\\n\\n\\nThe go in Sharja...</td>\n",
       "      <td>The go in Sharja sate to help you could descri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>20067753</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 20067753\\n\\n\\n\\nIn sutardy I wi...</td>\n",
       "      <td>In sutardy I will perfact happy holiday,..I we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>200600299</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200600299\\n\\n\\n\\nI magine you h...</td>\n",
       "      <td>I magine you hav_e_ just the P_er_fect holiday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>200610505</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200610505\\n\\n\\n\\nWhere you went...</td>\n",
       "      <td>Where you went, who you went there with, what ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>200608033</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200608033\\n\\n\\n\\n\\t\\t\\t\\tThe wo...</td>\n",
       "      <td>The worst holiday The worst holiday in my hole...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>200606606</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200606606\\n\\n\\n\\n        Topic ...</td>\n",
       "      <td>Topic A In the summer I went with my f~~ri~~nd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>200608027</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200608027\\n\\n\\n\\n\\t\\t\\t\\tThe pe...</td>\n",
       "      <td>The perfect holiday Summer holiday is the best...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>434 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Filename Level                                      Original_Text  \\\n",
       "0     200607296     3  \\t\\t\\t\\tCEPA 3 200607296\\n\\n\\n\\nNow I tell you...   \n",
       "1     200607457     4  \\t\\t\\t\\tCEPA 4 200607457\\n\\n\\n\\n              ...   \n",
       "3     200608016     4  \\t\\t\\t\\tCEPA 4 200608016\\n\\n\\n\\nEvery body hav...   \n",
       "9     200611351     3  \\t\\t\\t\\tCEPA 3 200611351\\n\\n\\n\\nAl_ain mall is...   \n",
       "15    200603206     1  \\n\\t\\t\\t\\tCEPA 1 200603206\\n\\n\\n\\n        Topi...   \n",
       "16    200611379     3  \\t\\t\\t\\tCEPA 3 200611379\\n\\n\\n\\n\\nI spend my w...   \n",
       "18    200611190     3  \\t\\t\\t\\tCEPA 3 200611190\\n\\n\\n\\nThere are many...   \n",
       "19    200606353     2  \\t\\t\\t\\tCEPA 2 200606353\\n\\n\\n\\nI m Love the U...   \n",
       "23    200601400     2  \\t\\t\\t\\tCEPA 2 200601400\\n\\n\\n\\nI have just ha...   \n",
       "24    200601414     2  \\t\\t\\t\\tCEPA 2 200601414\\n\\n\\n\\n<o>I am go to ...   \n",
       "30    200601954     1  \\t\\t\\t\\tCEPA 1 200601954\\n\\n\\n\\nI am go with m...   \n",
       "32    200607721     4  \\t\\t\\t\\tCEPA 4 200607721\\n\\n\\n\\nIn the last ho...   \n",
       "52    200600121     1  \\t\\t\\t\\tCEPA 1 200600121\\n\\n\\n\\nI’m going and ...   \n",
       "59    200606833     4  \\t\\t\\t\\tCEPA 4 200606833\\n\\n\\n\\nI went the Ome...   \n",
       "60    200611556     3  \\t\\t\\t\\tCEPA 3 200611556\\n\\n\\n\\nI spend my hol...   \n",
       "62    200607306     2  \\t\\t\\t\\tCEPA 2 200607306\\n\\n\\n\\nI waite trevel...   \n",
       "65    200606756     2  \\t\\t\\t\\tCEPA 2 200606756\\n\\n\\n\\nTopic B : We g...   \n",
       "69    200605920     2  \\t\\t\\t\\tCEPA 2 200605920\\n\\n\\n\\nMy live in the...   \n",
       "71    200605692     4  \\t\\t\\t\\tCEPA 4 200605692\\n\\n\\n\\nEvery body kno...   \n",
       "79    200611608     3  \\t\\t\\t\\tCEPA 3 200611608\\n\\n\\n\\nLast weekend I...   \n",
       "82    200611835     2  \\t\\t\\t\\tCEPA 2 200611835\\n\\n\\n\\nIs vary bast f...   \n",
       "83    200605673     2  \\t\\t\\t\\tCEPA 2 200605673\\n\\n\\n\\n        Sport....   \n",
       "84    200601415     2  \\t\\t\\t\\tCEPA 2 200601415\\n\\n\\n\\nlast week <o>i...   \n",
       "86    200612469     2  \\t\\t\\t\\tCEPA 2 200612469\\n\\n\\n\\nI writ thise t...   \n",
       "88    200605115     2  \\t\\t\\t\\tCEPA 2 200605115\\n\\n\\n\\nThe last holid...   \n",
       "92    200607898     4  \\t\\t\\t\\tCEPA 4 200607898\\n\\n\\n\\nIn the last su...   \n",
       "97    200607268     3  \\t\\t\\t\\tCEPA 3 200607268\\n\\n\\n\\nIn the summer ...   \n",
       "104   200610880     4  \\t\\t\\t\\tCEPA 4 200610880\\n\\n\\n\\n\\n\\nIn  the ho...   \n",
       "113   200611385     3  \\t\\t\\t\\tCEPA 3 200611385\\n\\n\\n\\nIn summer holi...   \n",
       "114   200607326     4  \\t\\t\\t\\tCEPA 4 200607326\\n\\n\\n\\nLast summer <x...   \n",
       "...         ...   ...                                                ...   \n",
       "1517  200607854     4  \\t\\t\\t\\tCEPA 4 200607854\\n\\n\\n\\nIn the last ye...   \n",
       "1519  200607840     4  \\t\\t\\t\\tCEPA 4 200607840\\n\\n\\n\\nReally I love ...   \n",
       "1524  200606375     2  \\t\\t\\t\\tCEPA 2 200606375\\n\\n\\n\\nthe most beau<...   \n",
       "1536  200602894     1  \\t\\t\\t\\tCepa 1 200602894\\n\\n\\n\\nYou went In Om...   \n",
       "1541  200607303     4  \\t\\t\\t\\tCEPA 4 200607303\\n\\n\\n\\nThe main reaso...   \n",
       "1542  200606605     2  \\t\\t\\t\\tCEPA 2 200606605\\n\\n\\n\\nI love play fo...   \n",
       "1546  200608018     4  \\t\\t\\t\\tCEPA 4 200608018\\n\\n\\n\\nThere are many...   \n",
       "1556  200609733     1  \\t\\t\\t\\tCEPA 1 200609733\\n\\n\\n\\n<o>Sea in UAE ...   \n",
       "1559  200611836     3  \\t\\t\\t\\tCEPA 3 200611836\\n\\n\\n\\n\\t\\t\\t\\tThe wo...   \n",
       "1563  200612442     3  \\t\\t\\t\\tCEPA 3 200612442\\n\\n\\n\\nLast xxx week ...   \n",
       "1566  200607842     4  \\t\\t\\t\\tCEPA 4 200607842\\n\\n\\n\\n<o>T</o>hese  ...   \n",
       "1572  200601958     1  \\t\\t\\t\\tCEPA 1 200601958\\n\\n\\n\\nMy car the my ...   \n",
       "1573  200601970     1  \\t\\t\\t\\tCEPA 1 200601970\\n\\n\\n\\nYou have just ...   \n",
       "1577  200600313     1  \\t\\t\\t\\tCEPA 1 200600313\\n\\n\\n\\nI me give a lo...   \n",
       "1581  200606782     3          CEPA 3 200606782\\n\\n\\n\\n\\nI go withe m...   \n",
       "1582  200612085     3  \\t\\t\\t\\tCEPA 3 200612085\\n\\n\\n\\n\\nMy perfect h...   \n",
       "1583  200606796     3  \\t\\t\\t\\tCEPA 3 200606796\\n\\n\\n\\nThe weekend in...   \n",
       "1589  200600677     1  \\t\\t\\t\\tCEPA 1 200600677\\n\\n\\n\\nshe have just ...   \n",
       "1592  200604411     1  \\t\\t\\t\\tCEPA 1 200604411\\n\\n\\n\\nThe ar<o>e</o>...   \n",
       "1598  200607310     3  \\t\\t\\t\\tCEPA 3 200607310\\n\\n\\n\\nIn one day I w...   \n",
       "1604  200605068     1  `\\t\\t\\t\\tCEPA 1 200605068\\n\\n\\n\\nOn The Samir ...   \n",
       "1611  200605846     4  \\t\\t\\t\\tCEPA 4 200605846\\n\\n\\n\\n              ...   \n",
       "1617  200601971     1  \\t\\t\\t\\tCEPA  1 200601971\\n\\n\\n\\nwhere you wen...   \n",
       "1628  200605336     1  \\t\\t\\t\\tCEPA 1 200605336\\n\\n\\nThe go in Sharja...   \n",
       "1630   20067753     3  \\t\\t\\t\\tCEPA 3 20067753\\n\\n\\n\\nIn sutardy I wi...   \n",
       "1635  200600299     1  \\t\\t\\t\\tCEPA 1 200600299\\n\\n\\n\\nI magine you h...   \n",
       "1639  200610505     1  \\t\\t\\t\\tCEPA 1 200610505\\n\\n\\n\\nWhere you went...   \n",
       "1647  200608033     4  \\t\\t\\t\\tCEPA 4 200608033\\n\\n\\n\\n\\t\\t\\t\\tThe wo...   \n",
       "1648  200606606     2  \\t\\t\\t\\tCEPA 2 200606606\\n\\n\\n\\n        Topic ...   \n",
       "1649  200608027     4  \\t\\t\\t\\tCEPA 4 200608027\\n\\n\\n\\n\\t\\t\\t\\tThe pe...   \n",
       "\n",
       "                                       Normalized_Essay  \n",
       "0     Now I tell you why my worst holiday ever in th...  \n",
       "1     My worst holiday Last year I have just had the...  \n",
       "3     Every body have a lot ofpossessions in this li...  \n",
       "9     Al_ain mall is the most beautiful place. First...  \n",
       "15    Topic _A_ I will writing this a Prerf in far_m...  \n",
       "16    I spend my weekend in Dubai. I go with my fami...  \n",
       "18    There are many things make the worst holiday f...  \n",
       "19    I m Love the UAE the UAE I am very very Love t...  \n",
       "23    I have just had the worst holiday ever . The h...  \n",
       "24    _I am go to the_sumar holabuy in The Oman in t...  \n",
       "30    I am go with may friene_d_ to Oman. I am go wX...  \n",
       "32    In the last holid_a_y my family and my uncles ...  \n",
       "52    I'm going and _zoom_ there _with_ fimi_l_ you ...  \n",
       "59    I went the Omen. Because is wonderful and a go...  \n",
       "60    I spend my holiday is Sauid Arabia. I want wit...  \n",
       "62    I waite trevel wath my famliy to AlAin to see ...  \n",
       "65    Topic B : We go with my family by car in a ^Al...  \n",
       "69    My live in the Shajohe, A house is very beauti...  \n",
       "71    Every body know that the summer holiday is the...  \n",
       "79    Last weekend Im went to Dudai with my Frinds i...  \n",
       "82    Is vary bast filme you have seen recentry. The...  \n",
       "83    Sport.... The Sport is very important in peopl...  \n",
       "84    last week _in the End i_t is verey bad the hol...  \n",
       "86    I writ thise topic about Imagin you have just ...  \n",
       "88    The last holiday Imagine went you _have_ just ...  \n",
       "92    In the last summar I have spent awery wonderfu...  \n",
       "97    In the summer 2004 I went to bahrain with my f...  \n",
       "104   In the holiday , My family decided go to the b...  \n",
       "113   In summer holiday I went with my fami_lly_ to ...  \n",
       "114   Last summer ~~I weih~~ I went to Syria with my...  \n",
       "...                                                 ...  \n",
       "1517  In the last year I have jast had the worst hol...  \n",
       "1519  Really I love the Indian films because it took...  \n",
       "1524  the most beau_u_iful pla~~c~~e you know in _A_...  \n",
       "1536  You went In Oman you have just has the perfect...  \n",
       "1541  The main reason that people travel in holidaye...  \n",
       "1542  I love play football . I play with my friend ....  \n",
       "1546  There are many favourite thing`s in my Life bu...  \n",
       "1556  _Sea in UAE I like the sea I look the fish and...  \n",
       "1559  The worst holiday Imagine my holiday in the Ho...  \n",
       "1563  Last xxx week I am see a good movie or film. I...  \n",
       "1566  _T_hese holiday is worst holiday I spend it . ...  \n",
       "1572  My car the my car is ver_y_ fast the name my c...  \n",
       "1573  You have just had the perfect holiday you went...  \n",
       "1577  I me give a lot than Iam gin_e_ you have just ...  \n",
       "1581  I go withe my Frineds to USA and i go to hilto...  \n",
       "1582  My perfect holiday was in Last year. It was in...  \n",
       "1583  The weekend in the summer I am go to visit , I...  \n",
       "1589  she have just ha_d_ the perfect. went go to th...  \n",
       "1592  The ar_e_ simr it flmatd . the you went smie ....  \n",
       "1598  In one day I was in holiday . The holiday was ...  \n",
       "1604  On The Samir i go fo The sliame will i sea fal...  \n",
       "1611  The most beautiful place I was choose this top...  \n",
       "1617  where you went in the we XXX. xxx in The zoo w...  \n",
       "1628  The go in Sharja sate to help you could descri...  \n",
       "1630  In sutardy I will perfact happy holiday,..I we...  \n",
       "1635  I magine you hav_e_ just the P_er_fect holiday...  \n",
       "1639  Where you went, who you went there with, what ...  \n",
       "1647  The worst holiday The worst holiday in my hole...  \n",
       "1648  Topic A In the summer I went with my f~~ri~~nd...  \n",
       "1649  The perfect holiday Summer holiday is the best...  \n",
       "\n",
       "[434 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cepa_df[cepa_df.Normalized_Essay.str.contains(r'_\\w{2,}')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Level</th>\n",
       "      <th>Original_Text</th>\n",
       "      <th>Normalized_Essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Filename, Level, Original_Text, Normalized_Essay]\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cepa_df[cepa_df.Normalized_Essay.str.contains(r'[<>]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t\\t\\t\\tCEPA 3 200606808\\n\\n\\n\\nA Good day ti go the picnicto a Al Ain, I am go wiht my friend , I say them \"i <x>s</x>ee you in the M<x>e</x>ga Mall t<x>o</x> begin th picnic toworoion\" In 8.30 A.M I meet my fi<x>r</x>iend he say \" I want breakfast I am don\\'t e<o>a</o>ten <o><it </o>\" , I say \"O.K le<x>t\\'s</x> go to the fastfood, i<x>t h</x>as eiar the mall \" we began the picnic, I the street my firend say\"Al Ain a wanderful cityI would go the AlHale park\", than I say \" It\\'s good iea<o>sc</o> let\\'s go ther\" than I <x>am</x> see Al Ain cit<x>y</x> than I go to xxx the park ,I around the AlHafet montrain in 6.00pm I say my fi<x>r</x>end \" let\\'s go bark and sharjah \"I say my selt \" The picnic it\\'s wonderful picnic I se<x>e</x> in the park a football ground, the cinldren play ground and an AlHafet mountrain I see the <x>w</x>o<x>n</x>derful reck I get them for my father he like the rock \"it\\'s my picnis .\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'A Good day ti go the picnicto a Al Ain, I am go wiht my friend , I say them \"i ~~s~~ee you in the M~~e~~ga Mall t~~o~~ begin th picnic toworoion\" In 8.30 A.M I meet my fi~~r~~iend he say \" I want breakfast I am don\\'t e_a_ten _ it_\" , I say \"O.K le~~t\\'s~~ go to the fastfood, i~~t h~~as eiar the mall \" we began the picnic, I the street my firend say\"Al Ain a wanderful cityI would go the AlHale park\", than I say \" It\\'s good iea_sc_ let\\'s go ther\" than I ~~am~~ see Al Ain cit~~y~~ than I go to xxx the park ,I around the AlHafet montrain in 6.00pm I say my fi~~r~~end \" let\\'s go bark and sharjah \"I say my selt \" The picnic it\\'s wonderful picnic I se~~e~~ in the park a football ground, the cinldren play ground and an AlHafet mountrain I see the ~~w~~o~~n~~derful reck I get them for my father he like the rock \"it\\'s my picnis .'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cepa_df.Original_Text[214]\n",
    "cepa_df.Normalized_Essay[214]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\t\\t\\t\\tCEPA 2 200606375\\n\\n\\n\\nthe most beau<o>u</otiful pla<x>c</x>e you know in <o>A</o>l Hilly . I like go to the allh<x>i</x>lly because I'm play geam and look in the most beautiful . I play foot ball , tens basketball and play geam to t<x>h</x>e car . I'm look to the sea. I'm go to the AllHilly on frend .your go in the play <o>and aet</o>. I should you go in the All Hilly sawfa li<x>k</x>e and love . because in the be<x>a</x>utiful place. why I like it so much ? <x>beaucse</x> beautiful place and I'm go to AllHelly only Time. what you do eat ?? I'm eat to Hamborgar , Frish and Asier .Thnke you.\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cepa_df.Original_Text[1524]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_I am go to the_sumar holabuy in The Oman in the Slala in my family the Salala is very haphe and go to The Oman in The car on my calal becoues is not go in the sun and becoues people go in Oman go to the sea and the swming ,eating in Faham beoue_s_ is bleshes food in the brth_a_r in the very nais in the swmming and go the play foot ple and go to the santar in the shoping and go to the Zoo is very naies and becoues in the manke and lion and tor to us in the anmils and go to The hotel go the slep. becous go to in The Famliy is very nais and is very haply'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\t\\t\\t\\tCEPA 2 200601414\\n\\n\\n\\n<o>I am go to the</o>sumar holabuy in The Oman in the Slala in my family the Salala is very haphe and go to The Oman in The car on my calal becoues is not go in the sun and becoues people go in Oman go to the sea and the swming ,eating in Faham beoue<o>s</o> is bleshes food in the brth<o>a</o>r in the very nais in the swmming and go the play foot ple and go to the santar in the shoping and go to the Zoo is very naies and becoues in the manke and lion and tor to us in the anmils and go to The hotel go the slep. becous go to in The Famliy is very nais and is very haply\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cepa_df.Normalized_Essay[24]\n",
    "cepa_df.Original_Text[24]\n",
    "\n",
    "# Yes, we can see \"the\" and \"sumar\" are connected in the phrase \"I am go in the sumar\" -- but that was in the original text, so that's probably not an error created by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t\\t\\t\\tCEPA 2 200605920\\n\\n\\n\\nMy live in the Shajohe, A house is very beautiful, 2 rezens  <i>for righting</i>\\nThe houses in the <o>fife</o> romes, I like rome is me .\\nFirse, be<x>c</x>uos a beautiful my rome, I like the rome becous the see are me bar play in rome and eat \\nSacend,\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'My live in the Shajohe, A house is very beautiful, 2 rezens ^for righting^ The houses in the _fife_ romes, I like rome is me . Firse, be~~c~~uos a beautiful my rome, I like the rome becous the see are me bar play in rome and eat Sacend,'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cepa_df.Original_Text[69]\n",
    "cepa_df.Normalized_Essay[69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF to remove tags -- removes tagging for insertions and emphasis, and removes anything within a cross-out tag\n",
    "def remove_tags(text):\n",
    "    \"\"\"Removes tags from Normalized_Essay. If there are tags indicating a student crossed something out, whatever is \n",
    "    enclosed in those tags is removed.\"\"\"\n",
    "    text = re.sub(r'~~.*?~~', '', text)         # delete tags for crossing out and the text in those tags\n",
    "    text = text.replace('_', '').replace('^', '')   # remove emphasis and insertion tags\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Level</th>\n",
       "      <th>Original_Text</th>\n",
       "      <th>Normalized_Essay</th>\n",
       "      <th>Revised_Essay</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>200601931</td>\n",
       "      <td>5</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 5 200601931\\n\\n\\n\\n              ...</td>\n",
       "      <td>Topic B There is no doubts that computer plays...</td>\n",
       "      <td>Topic B There is no doubts that computer plays...</td>\n",
       "      <td>[Topic, B, There, is, no, doubts, that, comput...</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>200612509</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200612509\\n\\n\\t\\t\\t\\t\\tA\\n\\n\\n ...</td>\n",
       "      <td>A My name is Ahmed. I m is ag 18 years. I fins...</td>\n",
       "      <td>A My name is Ahmed. I m is ag 18 years. I fins...</td>\n",
       "      <td>[A, My, name, is, Ahmed, ., I, m, is, ag, 18, ...</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>200612468</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200612468\\n\\n\\n\\nWhere you went...</td>\n",
       "      <td>Where you went go AlAin ^moon^ whoyou went the...</td>\n",
       "      <td>Where you went go AlAin moon whoyou went there...</td>\n",
       "      <td>[Where, you, went, go, AlAin, moon, whoyou, we...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>200606228</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200606228\\n\\n\\n\\nWhere you went...</td>\n",
       "      <td>Where you went go to the USA and Egbt Who you ...</td>\n",
       "      <td>Where you went go to the USA and Egbt Who you ...</td>\n",
       "      <td>[Where, you, went, go, to, the, USA, and, Egbt...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>200602273</td>\n",
       "      <td>5</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 5 200602273\\n\\n\\n\\nMy worst vacat...</td>\n",
       "      <td>My worst vacation in my life was when I went t...</td>\n",
       "      <td>My worst vacation in my life was when I went t...</td>\n",
       "      <td>[My, worst, vacation, in, my, life, was, when,...</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>200612437</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200612437\\n\\n\\n\\n\\t\\t\\t\\t\\tB\\n\\...</td>\n",
       "      <td>B The Al Ain in mall beauitiful. It is in the ...</td>\n",
       "      <td>B The Al Ain in mall beauitiful. It is in the ...</td>\n",
       "      <td>[B, The, Al, Ain, in, mall, beauitiful, ., It,...</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>200600663</td>\n",
       "      <td>5</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 5 200600663\\n\\n\\n\\nThis holiday i...</td>\n",
       "      <td>This holiday it was the best one that I had in...</td>\n",
       "      <td>This holiday it was the best one that I had in...</td>\n",
       "      <td>[This, holiday, it, was, the, best, one, that,...</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>200604486</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200604486\\n\\n\\n\\ngo too The Liw...</td>\n",
       "      <td>go too The Liwa my family too the house The ti...</td>\n",
       "      <td>go too The Liwa my family too the house The ti...</td>\n",
       "      <td>[go, too, The, Liwa, my, family, too, the, hou...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>200610788</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200610788\\n\\n\\n\\n              ...</td>\n",
       "      <td>Topic A Last week end I went to Dubai. I went ...</td>\n",
       "      <td>Topic A Last week end I went to Dubai. I went ...</td>\n",
       "      <td>[Topic, A, Last, week, end, I, went, to, Dubai...</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>200612260</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200612260\\n\\n\\n\\nI can and sala...</td>\n",
       "      <td>I can and salalh wor in Fantisti_c_ where you ...</td>\n",
       "      <td>I can and salalh wor in Fantistic where you we...</td>\n",
       "      <td>[I, can, and, salalh, wor, in, Fantistic, wher...</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>200612425</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200612425\\n\\n\\n\\nA    Tam very ...</td>\n",
       "      <td>A Tam very hapy becuseas travel in somer holip...</td>\n",
       "      <td>A Tam very hapy becuseas travel in somer holip...</td>\n",
       "      <td>[A, Tam, very, hapy, becuseas, travel, in, som...</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>200607385</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200607385\\n\\n\\n\\nI had a very w...</td>\n",
       "      <td>I had a very wonderful holiday throught I hadn...</td>\n",
       "      <td>I had a very wonderful holiday throught I hadn...</td>\n",
       "      <td>[I, had, a, very, wonderful, holiday, throught...</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>200605574</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200605574\\n\\n\\n\\nThe holiday in...</td>\n",
       "      <td>The holiday in this smmar it is not good and m...</td>\n",
       "      <td>The holiday in this smmar it is not good and m...</td>\n",
       "      <td>[The, holiday, in, this, smmar, it, is, not, g...</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>200607774</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200607774\\n\\n\\n\\nI'm very happy...</td>\n",
       "      <td>I'm very happy to write thes topic from my sum...</td>\n",
       "      <td>I'm very happy to write thes topic from my sum...</td>\n",
       "      <td>[I, 'm, very, happy, to, write, thes, topic, f...</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>200607309</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3200607309\\n\\n\\n\\nI have a perfec...</td>\n",
       "      <td>I have a perfect holiday win I wint to Tailand...</td>\n",
       "      <td>I have a perfect holiday win I wint to Tailand...</td>\n",
       "      <td>[I, have, a, perfect, holiday, win, I, wint, t...</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Filename Level                                      Original_Text  \\\n",
       "741   200601931     5  \\t\\t\\t\\tCEPA 5 200601931\\n\\n\\n\\n              ...   \n",
       "74    200612509     2  \\t\\t\\t\\tCEPA 2 200612509\\n\\n\\t\\t\\t\\t\\tA\\n\\n\\n ...   \n",
       "26    200612468     2  \\t\\t\\t\\tCEPA 2 200612468\\n\\n\\n\\nWhere you went...   \n",
       "1391  200606228     1  \\t\\t\\t\\tCEPA 1 200606228\\n\\n\\n\\nWhere you went...   \n",
       "1348  200602273     5  \\t\\t\\t\\tCEPA 5 200602273\\n\\n\\n\\nMy worst vacat...   \n",
       "421   200612437     2  \\t\\t\\t\\tCEPA 2 200612437\\n\\n\\n\\n\\t\\t\\t\\t\\tB\\n\\...   \n",
       "1626  200600663     5  \\t\\t\\t\\tCEPA 5 200600663\\n\\n\\n\\nThis holiday i...   \n",
       "704   200604486     1  \\t\\t\\t\\tCEPA 1 200604486\\n\\n\\n\\ngo too The Liw...   \n",
       "803   200610788     4  \\t\\t\\t\\tCEPA 4 200610788\\n\\n\\n\\n              ...   \n",
       "1402  200612260     1  \\t\\t\\t\\tCEPA 1 200612260\\n\\n\\n\\nI can and sala...   \n",
       "668   200612425     2  \\t\\t\\t\\tCEPA 2 200612425\\n\\n\\n\\nA    Tam very ...   \n",
       "412   200607385     4  \\t\\t\\t\\tCEPA 4 200607385\\n\\n\\n\\nI had a very w...   \n",
       "705   200605574     2  \\t\\t\\t\\tCEPA 2 200605574\\n\\n\\n\\nThe holiday in...   \n",
       "1133  200607774     3  \\t\\t\\t\\tCEPA 3 200607774\\n\\n\\n\\nI'm very happy...   \n",
       "55    200607309     3  \\t\\t\\t\\tCEPA 3200607309\\n\\n\\n\\nI have a perfec...   \n",
       "\n",
       "                                       Normalized_Essay  \\\n",
       "741   Topic B There is no doubts that computer plays...   \n",
       "74    A My name is Ahmed. I m is ag 18 years. I fins...   \n",
       "26    Where you went go AlAin ^moon^ whoyou went the...   \n",
       "1391  Where you went go to the USA and Egbt Who you ...   \n",
       "1348  My worst vacation in my life was when I went t...   \n",
       "421   B The Al Ain in mall beauitiful. It is in the ...   \n",
       "1626  This holiday it was the best one that I had in...   \n",
       "704   go too The Liwa my family too the house The ti...   \n",
       "803   Topic A Last week end I went to Dubai. I went ...   \n",
       "1402  I can and salalh wor in Fantisti_c_ where you ...   \n",
       "668   A Tam very hapy becuseas travel in somer holip...   \n",
       "412   I had a very wonderful holiday throught I hadn...   \n",
       "705   The holiday in this smmar it is not good and m...   \n",
       "1133  I'm very happy to write thes topic from my sum...   \n",
       "55    I have a perfect holiday win I wint to Tailand...   \n",
       "\n",
       "                                          Revised_Essay  \\\n",
       "741   Topic B There is no doubts that computer plays...   \n",
       "74    A My name is Ahmed. I m is ag 18 years. I fins...   \n",
       "26    Where you went go AlAin moon whoyou went there...   \n",
       "1391  Where you went go to the USA and Egbt Who you ...   \n",
       "1348  My worst vacation in my life was when I went t...   \n",
       "421   B The Al Ain in mall beauitiful. It is in the ...   \n",
       "1626  This holiday it was the best one that I had in...   \n",
       "704   go too The Liwa my family too the house The ti...   \n",
       "803   Topic A Last week end I went to Dubai. I went ...   \n",
       "1402  I can and salalh wor in Fantistic where you we...   \n",
       "668   A Tam very hapy becuseas travel in somer holip...   \n",
       "412   I had a very wonderful holiday throught I hadn...   \n",
       "705   The holiday in this smmar it is not good and m...   \n",
       "1133  I'm very happy to write thes topic from my sum...   \n",
       "55    I have a perfect holiday win I wint to Tailand...   \n",
       "\n",
       "                                                 tokens  token_count  \n",
       "741   [Topic, B, There, is, no, doubts, that, comput...          192  \n",
       "74    [A, My, name, is, Ahmed, ., I, m, is, ag, 18, ...          176  \n",
       "26    [Where, you, went, go, AlAin, moon, whoyou, we...           40  \n",
       "1391  [Where, you, went, go, to, the, USA, and, Egbt...           56  \n",
       "1348  [My, worst, vacation, in, my, life, was, when,...          259  \n",
       "421   [B, The, Al, Ain, in, mall, beauitiful, ., It,...           69  \n",
       "1626  [This, holiday, it, was, the, best, one, that,...          268  \n",
       "704   [go, too, The, Liwa, my, family, too, the, hou...           21  \n",
       "803   [Topic, A, Last, week, end, I, went, to, Dubai...          188  \n",
       "1402  [I, can, and, salalh, wor, in, Fantistic, wher...           44  \n",
       "668   [A, Tam, very, hapy, becuseas, travel, in, som...           80  \n",
       "412   [I, had, a, very, wonderful, holiday, throught...          165  \n",
       "705   [The, holiday, in, this, smmar, it, is, not, g...          126  \n",
       "1133  [I, 'm, very, happy, to, write, thes, topic, f...          117  \n",
       "55    [I, have, a, perfect, holiday, win, I, wint, t...          183  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove_tags applied to creat \"Revised_Essay\"\n",
    "# Tokenize revised_essay\n",
    "# Get token count\n",
    "cepa_df['Revised_Essay'] = cepa_df.Normalized_Essay.apply(remove_tags)\n",
    "cepa_df['tokens'] = cepa_df.Revised_Essay.apply(nltk.word_tokenize)\n",
    "cepa_df['token_count'] = cepa_df.tokens.map(len)\n",
    "cepa_df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Level</th>\n",
       "      <th>Original_Text</th>\n",
       "      <th>Normalized_Essay</th>\n",
       "      <th>Revised_Essay</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>200608959</td>\n",
       "      <td>5</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 5 200608959\\n\\n\\n\\nI have just ha...</td>\n",
       "      <td>I have just had the perfect and the best holid...</td>\n",
       "      <td>I have just had the perfect and the best holid...</td>\n",
       "      <td>[I, have, just, had, the, perfect, and, the, b...</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>200612443</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200612443\\n\\n\\n\\nI’m writing To...</td>\n",
       "      <td>I'm writing Topic B to Describe the best film ...</td>\n",
       "      <td>I'm writing Topic B to Describe the best film ...</td>\n",
       "      <td>[I, 'm, writing, Topic, B, to, Describe, the, ...</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>200611155</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200611155\\n\\n\\n\\nIn the last su...</td>\n",
       "      <td>In the last summer holiday, I hade the worest ...</td>\n",
       "      <td>In the last summer holiday, I hade the worest ...</td>\n",
       "      <td>[In, the, last, summer, holiday, ,, I, hade, t...</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>200612584</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200612584\\n\\n\\n\\nThese weekend ...</td>\n",
       "      <td>These weekend the worst. Because I stayd at ho...</td>\n",
       "      <td>These weekend the worst. Because I stayd at ho...</td>\n",
       "      <td>[These, weekend, the, worst, ., Because, I, st...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>200611303</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200611303\\n\\n\\n\\nThere are many...</td>\n",
       "      <td>There are many qualites the worst weekend bad....</td>\n",
       "      <td>There are many qualites the worst weekend bad....</td>\n",
       "      <td>[There, are, many, qualites, the, worst, weeke...</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>200604402</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200604402\\n\\n\\n\\n&lt;x&gt;me&lt;/x&gt; &lt;o&gt;m...</td>\n",
       "      <td>~~me~~ _my you went ar_wous. went ~~my~~ and f...</td>\n",
       "      <td>my you went arwous. went  and fraund  Yor  ne...</td>\n",
       "      <td>[my, you, went, arwous, ., went, and, fraund, ...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Filename Level                                      Original_Text  \\\n",
       "6     200608959     5  \\t\\t\\t\\tCEPA 5 200608959\\n\\n\\n\\nI have just ha...   \n",
       "128   200612443     2  \\t\\t\\t\\tCEPA 2 200612443\\n\\n\\n\\nI’m writing To...   \n",
       "237   200611155     3  \\t\\t\\t\\tCEPA 3 200611155\\n\\n\\n\\nIn the last su...   \n",
       "812   200612584     2  \\t\\t\\t\\tCEPA 2 200612584\\n\\n\\n\\nThese weekend ...   \n",
       "1251  200611303     3  \\t\\t\\t\\tCEPA 3 200611303\\n\\n\\n\\nThere are many...   \n",
       "1401  200604402     1  \\t\\t\\t\\tCEPA 1 200604402\\n\\n\\n\\n<x>me</x> <o>m...   \n",
       "\n",
       "                                       Normalized_Essay  \\\n",
       "6     I have just had the perfect and the best holid...   \n",
       "128   I'm writing Topic B to Describe the best film ...   \n",
       "237   In the last summer holiday, I hade the worest ...   \n",
       "812   These weekend the worst. Because I stayd at ho...   \n",
       "1251  There are many qualites the worst weekend bad....   \n",
       "1401  ~~me~~ _my you went ar_wous. went ~~my~~ and f...   \n",
       "\n",
       "                                          Revised_Essay  \\\n",
       "6     I have just had the perfect and the best holid...   \n",
       "128   I'm writing Topic B to Describe the best film ...   \n",
       "237   In the last summer holiday, I hade the worest ...   \n",
       "812   These weekend the worst. Because I stayd at ho...   \n",
       "1251  There are many qualites the worst weekend bad....   \n",
       "1401   my you went arwous. went  and fraund  Yor  ne...   \n",
       "\n",
       "                                                 tokens  token_count  \n",
       "6     [I, have, just, had, the, perfect, and, the, b...          277  \n",
       "128   [I, 'm, writing, Topic, B, to, Describe, the, ...          310  \n",
       "237   [In, the, last, summer, holiday, ,, I, hade, t...          264  \n",
       "812   [These, weekend, the, worst, ., Because, I, st...           52  \n",
       "1251  [There, are, many, qualites, the, worst, weeke...          199  \n",
       "1401  [my, you, went, arwous, ., went, and, fraund, ...           42  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking to make sure there are no \"cepa\" tokens included from incorrect/incomplete heading removal\n",
    "cepa_df[cepa_df.tokens.apply(lambda x: 'cepa' in [y.lower() for y in x])]\n",
    "\n",
    "# returns are essays that talk about cepa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's keep truckin' along!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get TTR\n",
    "def get_TTR(toks):\n",
    "    \"\"\"All tokens are lowercased, punctuation is included.\n",
    "    Get TTR by dividing set of lowercased tokens by length of tokens.\"\"\"\n",
    "    all_toks = [x.lower() for x in toks]\n",
    "    if len(all_toks) == 0:   # at least one file has 0 tokens, so I got an error about dividing by zero without this\n",
    "        return 0\n",
    "    else: return len(set(all_toks))/len(all_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7619047619047619"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking UDF\n",
    "foo = \"My name is Elena and I am a Junior Linguist. I study applied linguistics, and i want to be a linguist.\"\n",
    "get_TTR(foo.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7619047619047619"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking UDF by hand\n",
    "lil = [x.lower() for x in foo.split()]\n",
    "len(set(lil))/len(lil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking UDF on an empty string\n",
    "foo = \"\"\n",
    "get_TTR(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cepa_df['TTR'] = cepa_df.tokens.apply(get_TTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Level</th>\n",
       "      <th>Original_Text</th>\n",
       "      <th>Normalized_Essay</th>\n",
       "      <th>Revised_Essay</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_count</th>\n",
       "      <th>TTR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200607296</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200607296\\n\\n\\n\\nNow I tell you...</td>\n",
       "      <td>Now I tell you why my worst holiday ever in th...</td>\n",
       "      <td>Now I tell you why my worst holiday ever in th...</td>\n",
       "      <td>[Now, I, tell, you, why, my, worst, holiday, e...</td>\n",
       "      <td>207</td>\n",
       "      <td>0.492754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200607457</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200607457\\n\\n\\n\\n              ...</td>\n",
       "      <td>My worst holiday Last year I have just had the...</td>\n",
       "      <td>My worst holiday Last year I have just had the...</td>\n",
       "      <td>[My, worst, holiday, Last, year, I, have, just...</td>\n",
       "      <td>180</td>\n",
       "      <td>0.572222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200600487</td>\n",
       "      <td>5</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 5 200600487\\n\\n\\n\\n\\nEvery body i...</td>\n",
       "      <td>Every body in this life have a favourite posse...</td>\n",
       "      <td>Every body in this life have a favourite posse...</td>\n",
       "      <td>[Every, body, in, this, life, have, a, favouri...</td>\n",
       "      <td>229</td>\n",
       "      <td>0.445415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200608016</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200608016\\n\\n\\n\\nEvery body hav...</td>\n",
       "      <td>Every body have a lot ofpossessions in this li...</td>\n",
       "      <td>Every body have a lot ofpossessions in this li...</td>\n",
       "      <td>[Every, body, have, a, lot, ofpossessions, in,...</td>\n",
       "      <td>156</td>\n",
       "      <td>0.608974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200611825</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200611825\\n\\n\\n\\nyou go in the ...</td>\n",
       "      <td>you go in the oman just had the perfect holida...</td>\n",
       "      <td>you go in the oman just had the perfect holida...</td>\n",
       "      <td>[you, go, in, the, oman, just, had, the, perfe...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.629630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Filename Level                                      Original_Text  \\\n",
       "0  200607296     3  \\t\\t\\t\\tCEPA 3 200607296\\n\\n\\n\\nNow I tell you...   \n",
       "1  200607457     4  \\t\\t\\t\\tCEPA 4 200607457\\n\\n\\n\\n              ...   \n",
       "2  200600487     5  \\t\\t\\t\\tCEPA 5 200600487\\n\\n\\n\\n\\nEvery body i...   \n",
       "3  200608016     4  \\t\\t\\t\\tCEPA 4 200608016\\n\\n\\n\\nEvery body hav...   \n",
       "4  200611825     1  \\t\\t\\t\\tCEPA 1 200611825\\n\\n\\n\\nyou go in the ...   \n",
       "\n",
       "                                    Normalized_Essay  \\\n",
       "0  Now I tell you why my worst holiday ever in th...   \n",
       "1  My worst holiday Last year I have just had the...   \n",
       "2  Every body in this life have a favourite posse...   \n",
       "3  Every body have a lot ofpossessions in this li...   \n",
       "4  you go in the oman just had the perfect holida...   \n",
       "\n",
       "                                       Revised_Essay  \\\n",
       "0  Now I tell you why my worst holiday ever in th...   \n",
       "1  My worst holiday Last year I have just had the...   \n",
       "2  Every body in this life have a favourite posse...   \n",
       "3  Every body have a lot ofpossessions in this li...   \n",
       "4  you go in the oman just had the perfect holida...   \n",
       "\n",
       "                                              tokens  token_count       TTR  \n",
       "0  [Now, I, tell, you, why, my, worst, holiday, e...          207  0.492754  \n",
       "1  [My, worst, holiday, Last, year, I, have, just...          180  0.572222  \n",
       "2  [Every, body, in, this, life, have, a, favouri...          229  0.445415  \n",
       "3  [Every, body, have, a, lot, ofpossessions, in,...          156  0.608974  \n",
       "4  [you, go, in, the, oman, just, had, the, perfe...           27  0.629630  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cepa_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptives\n",
    "Here, I do two things: I make sure everything is read in correctly, and I look at some basic descriptive stats for the DataFrame, such as the descriptions of token count, TTR, and value counts for the different levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all filenames should have only 1 value\n",
    "all(cepa_df.Filename.value_counts() == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1664, 8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of the DataFrame as is\n",
    "cepa_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2', '5', '4', '6', '1', '3'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Levels should be 1-6 only\n",
    "set(cepa_df.Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_count</th>\n",
       "      <th>TTR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1664.000000</td>\n",
       "      <td>1664.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>160.146635</td>\n",
       "      <td>0.546312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>86.137210</td>\n",
       "      <td>0.115905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>85.000000</td>\n",
       "      <td>0.471569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>164.000000</td>\n",
       "      <td>0.522902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>224.000000</td>\n",
       "      <td>0.590166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>397.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token_count          TTR\n",
       "count  1664.000000  1664.000000\n",
       "mean    160.146635     0.546312\n",
       "std      86.137210     0.115905\n",
       "min       0.000000     0.000000\n",
       "25%      85.000000     0.471569\n",
       "50%     164.000000     0.522902\n",
       "75%     224.000000     0.590166\n",
       "max     397.000000     1.000000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should be 1664 counts for everything -- at least 1 token_count of 0\n",
    "cepa_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    299\n",
       "4    297\n",
       "2    292\n",
       "1    276\n",
       "3    250\n",
       "6    250\n",
       "Name: Level, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's review the number of files for each level\n",
    "cepa_df.Level.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Level</th>\n",
       "      <th>Original_Text</th>\n",
       "      <th>Normalized_Essay</th>\n",
       "      <th>Revised_Essay</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_count</th>\n",
       "      <th>TTR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>200604511</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200604511\\n\\n\\n\\n\\n</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Filename Level                       Original_Text Normalized_Essay  \\\n",
       "1079  200604511     1  \\t\\t\\t\\tCEPA 1 200604511\\n\\n\\n\\n\\n                    \n",
       "\n",
       "     Revised_Essay tokens  token_count  TTR  \n",
       "1079                   []            0  0.0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how many files have no tokens\n",
    "cepa_df[cepa_df.token_count == 0]\n",
    "\n",
    "# Just 1! and it's a level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Level</th>\n",
       "      <th>Original_Text</th>\n",
       "      <th>Normalized_Essay</th>\n",
       "      <th>Revised_Essay</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_count</th>\n",
       "      <th>TTR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>200608341</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200608341\\n\\n\\n\\nI don’t now\\n</td>\n",
       "      <td>I don't now</td>\n",
       "      <td>I don't now</td>\n",
       "      <td>[I, do, n't, now]</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>200606487</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200606487\\n\\n\\n\\nThe mask\\nThes...</td>\n",
       "      <td>The mask These film is Dangrs</td>\n",
       "      <td>The mask These film is Dangrs</td>\n",
       "      <td>[The, mask, These, film, is, Dangrs]</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>200611449</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200611449\\n\\n\\n\\nOman it agood ...</td>\n",
       "      <td>Oman it agood cantry.</td>\n",
       "      <td>Oman it agood cantry.</td>\n",
       "      <td>[Oman, it, agood, cantry, .]</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>200611925</td>\n",
       "      <td>2</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 2 200611925\\n\\n\\n\\nIm KHALED ABDU...</td>\n",
       "      <td>Im KHALED ABDULLA AL SHAMISI from the UAE. ALAin</td>\n",
       "      <td>Im KHALED ABDULLA AL SHAMISI from the UAE. ALAin</td>\n",
       "      <td>[Im, KHALED, ABDULLA, AL, SHAMISI, from, the, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>200611666</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200611666\\n\\n\\n\\nSo good weathe...</td>\n",
       "      <td>So good weather in the summer</td>\n",
       "      <td>So good weather in the summer</td>\n",
       "      <td>[So, good, weather, in, the, summer]</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>200604490</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200604490\\n\\n\\n\\nyo to &lt;o&gt;AL&lt;/o...</td>\n",
       "      <td>yo to _AL_Ain an sammr not Haeep</td>\n",
       "      <td>yo to ALAin an sammr not Haeep</td>\n",
       "      <td>[yo, to, ALAin, an, sammr, not, Haeep]</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>200606507</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200606507\\n\\n\\n\\nThe Al Ain in ...</td>\n",
       "      <td>The Al Ain in no To fot xxx EYecPh</td>\n",
       "      <td>The Al Ain in no To fot xxx EYecPh</td>\n",
       "      <td>[The, Al, Ain, in, no, To, fot, xxx, EYecPh]</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>200608510</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200608510\\n\\n\\n\\nI you a Oman ....</td>\n",
       "      <td>I you a Oman .I are went there faiml</td>\n",
       "      <td>I you a Oman .I are went there faiml</td>\n",
       "      <td>[I, you, a, Oman, .I, are, went, there, faiml]</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>200612172</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200612172\\n\\n\\n\\nOne fire day w...</td>\n",
       "      <td>One fire day waent to the _cea_nima with ferin...</td>\n",
       "      <td>One fire day waent to the ceanima with ferinds...</td>\n",
       "      <td>[One, fire, day, waent, to, the, ceanima, with...</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>200603781</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200603781\\n\\n\\n\\nI go to the Du...</td>\n",
       "      <td>I go to the Dubai</td>\n",
       "      <td>I go to the Dubai</td>\n",
       "      <td>[I, go, to, the, Dubai]</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>200612328</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200612328\\n\\n\\n\\nmy freid a&lt;x&gt;s...</td>\n",
       "      <td>my freid a~~s~~ the s footboul,</td>\n",
       "      <td>my freid a the s footboul,</td>\n",
       "      <td>[my, freid, a, the, s, footboul, ,]</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>200603779</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200603779\\n\\n\\n\\nwhere you went...</td>\n",
       "      <td>where you went to Al Ain abd Oman Go people an...</td>\n",
       "      <td>where you went to Al Ain abd Oman Go people an...</td>\n",
       "      <td>[where, you, went, to, Al, Ain, abd, Oman, Go,...</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>200608835</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200608835\\n\\n\\n\\nI go in the sa...</td>\n",
       "      <td>I go in the safa park</td>\n",
       "      <td>I go in the safa park</td>\n",
       "      <td>[I, go, in, the, safa, park]</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>200606988</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200606988\\n\\n\\nHandwriting not ...</td>\n",
       "      <td>Handwriting not clear, legible</td>\n",
       "      <td>Handwriting not clear, legible</td>\n",
       "      <td>[Handwriting, not, clear, ,, legible]</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Filename Level                                      Original_Text  \\\n",
       "227   200608341     1      \\t\\t\\t\\tCEPA 1 200608341\\n\\n\\n\\nI don’t now\\n   \n",
       "538   200606487     1  \\t\\t\\t\\tCEPA 1 200606487\\n\\n\\n\\nThe mask\\nThes...   \n",
       "620   200611449     1  \\t\\t\\t\\tCEPA 1 200611449\\n\\n\\n\\nOman it agood ...   \n",
       "684   200611925     2  \\t\\t\\t\\tCEPA 2 200611925\\n\\n\\n\\nIm KHALED ABDU...   \n",
       "736   200611666     1  \\t\\t\\t\\tCEPA 1 200611666\\n\\n\\n\\nSo good weathe...   \n",
       "794   200604490     1  \\t\\t\\t\\tCEPA 1 200604490\\n\\n\\n\\nyo to <o>AL</o...   \n",
       "922   200606507     1  \\t\\t\\t\\tCEPA 1 200606507\\n\\n\\n\\nThe Al Ain in ...   \n",
       "1115  200608510     1  \\t\\t\\t\\tCEPA 1 200608510\\n\\n\\n\\nI you a Oman ....   \n",
       "1187  200612172     1  \\t\\t\\t\\tCEPA 1 200612172\\n\\n\\n\\nOne fire day w...   \n",
       "1337  200603781     1  \\t\\t\\t\\tCEPA 1 200603781\\n\\n\\n\\nI go to the Du...   \n",
       "1426  200612328     1  \\t\\t\\t\\tCEPA 1 200612328\\n\\n\\n\\nmy freid a<x>s...   \n",
       "1457  200603779     1  \\t\\t\\t\\tCEPA 1 200603779\\n\\n\\n\\nwhere you went...   \n",
       "1633  200608835     1  \\t\\t\\t\\tCEPA 1 200608835\\n\\n\\n\\nI go in the sa...   \n",
       "1651  200606988     4  \\t\\t\\t\\tCEPA 4 200606988\\n\\n\\nHandwriting not ...   \n",
       "\n",
       "                                       Normalized_Essay  \\\n",
       "227                                         I don't now   \n",
       "538                       The mask These film is Dangrs   \n",
       "620                               Oman it agood cantry.   \n",
       "684    Im KHALED ABDULLA AL SHAMISI from the UAE. ALAin   \n",
       "736                       So good weather in the summer   \n",
       "794                    yo to _AL_Ain an sammr not Haeep   \n",
       "922                  The Al Ain in no To fot xxx EYecPh   \n",
       "1115               I you a Oman .I are went there faiml   \n",
       "1187  One fire day waent to the _cea_nima with ferin...   \n",
       "1337                                  I go to the Dubai   \n",
       "1426                    my freid a~~s~~ the s footboul,   \n",
       "1457  where you went to Al Ain abd Oman Go people an...   \n",
       "1633                              I go in the safa park   \n",
       "1651                     Handwriting not clear, legible   \n",
       "\n",
       "                                          Revised_Essay  \\\n",
       "227                                         I don't now   \n",
       "538                       The mask These film is Dangrs   \n",
       "620                               Oman it agood cantry.   \n",
       "684    Im KHALED ABDULLA AL SHAMISI from the UAE. ALAin   \n",
       "736                       So good weather in the summer   \n",
       "794                      yo to ALAin an sammr not Haeep   \n",
       "922                  The Al Ain in no To fot xxx EYecPh   \n",
       "1115               I you a Oman .I are went there faiml   \n",
       "1187  One fire day waent to the ceanima with ferinds...   \n",
       "1337                                  I go to the Dubai   \n",
       "1426                         my freid a the s footboul,   \n",
       "1457  where you went to Al Ain abd Oman Go people an...   \n",
       "1633                              I go in the safa park   \n",
       "1651                     Handwriting not clear, legible   \n",
       "\n",
       "                                                 tokens  token_count  TTR  \n",
       "227                                   [I, do, n't, now]            4  1.0  \n",
       "538                [The, mask, These, film, is, Dangrs]            6  1.0  \n",
       "620                        [Oman, it, agood, cantry, .]            5  1.0  \n",
       "684   [Im, KHALED, ABDULLA, AL, SHAMISI, from, the, ...           10  1.0  \n",
       "736                [So, good, weather, in, the, summer]            6  1.0  \n",
       "794              [yo, to, ALAin, an, sammr, not, Haeep]            7  1.0  \n",
       "922        [The, Al, Ain, in, no, To, fot, xxx, EYecPh]            9  1.0  \n",
       "1115     [I, you, a, Oman, .I, are, went, there, faiml]            9  1.0  \n",
       "1187  [One, fire, day, waent, to, the, ceanima, with...           13  1.0  \n",
       "1337                            [I, go, to, the, Dubai]            5  1.0  \n",
       "1426                [my, freid, a, the, s, footboul, ,]            7  1.0  \n",
       "1457  [where, you, went, to, Al, Ain, abd, Oman, Go,...           14  1.0  \n",
       "1633                       [I, go, in, the, safa, park]            6  1.0  \n",
       "1651              [Handwriting, not, clear, ,, legible]            5  1.0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, let's see how many TTRs of 1.0 there are \n",
    "cepa_df[cepa_df.TTR == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few of these should probably be thrown out -- \"I don't (k)now\", \"Handwriting not clear, legible\". Or maybe the cutoff for token_count in analysis should be greater than 4 -- it would get rid of the empty file above, as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic visualization and analysis\n",
    "Now I want to look at some graphs quickly before wrapping up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot object at 0x1a23a7e898>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's visualize the amount of files in each level\n",
    "sns.countplot(x=\"Level\", data=cepa_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know the value counts of the levels, but it doesn't hurt to visualize it! Levels 2, 4, and 5 have the most files, getting near to 300. It looks like 3 and 6 have the least amount of files.\n",
    "\n",
    "Normally, we would expect the least amount of files in higher proficiencies. We would expect to see more files around a medium proficiency, and more low-level files than high-level files. We did see that for the most part in our original .jpg data: Levels 1, 2, 3, and 5 had 300 files each, Level 4 had 298, and Level 6 had 252 files.\n",
    "\n",
    "Therefore, it looks like we \"lost\" the most data in Levels 1 and 3. Only a handful of files were dropped from the other levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid object at 0x1a23d418d0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's look at general length by level\n",
    "sns.catplot(x='Level', y='token_count', kind='box', data=cepa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F_onewayResult(statistic=764.373757392868, pvalue=0.0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lv1_tokct = cepa_df[cepa_df.Level == '1'].token_count # token count for level 1\n",
    "lv2_tokct = cepa_df[cepa_df.Level == '2'].token_count # token count for level 2\n",
    "lv3_tokct = cepa_df[cepa_df.Level == '3'].token_count # token count for level 3\n",
    "lv4_tokct = cepa_df[cepa_df.Level == '4'].token_count # token count for level 4\n",
    "lv5_tokct = cepa_df[cepa_df.Level == '5'].token_count # token count for level 5\n",
    "lv6_tokct = cepa_df[cepa_df.Level == '6'].token_count # token count for level 6\n",
    "\n",
    "stats.f_oneway(lv1_tokct, lv2_tokct, lv3_tokct, lv4_tokct, lv5_tokct, lv6_tokct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see something that's pretty expected! The higher level students typically write more than the lower level students. There's a handful of outliers, mostly in Level 1. The one-way ANOVA suggests that there is a significant difference between groups here, but we will have to investigate a little bit more to pinpoint exactly where/what it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid object at 0x1a23ec0978>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot(x='Level', y='TTR', kind='box', data=cepa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F_onewayResult(statistic=208.1989467995349, pvalue=1.6028012530002684e-172)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lv1_ttr = cepa_df[cepa_df.Level == '1'].TTR # ttr for level 1\n",
    "lv2_ttr = cepa_df[cepa_df.Level == '2'].TTR # ttr for level 2\n",
    "lv3_ttr = cepa_df[cepa_df.Level == '3'].TTR # ttr for level 3\n",
    "lv4_ttr = cepa_df[cepa_df.Level == '4'].TTR # ttr for level 4\n",
    "lv5_ttr = cepa_df[cepa_df.Level == '5'].TTR # ttr for level 5\n",
    "lv6_ttr = cepa_df[cepa_df.Level == '6'].TTR # ttr for level 6\n",
    "\n",
    "stats.f_oneway(lv1_ttr, lv2_ttr, lv3_ttr, lv4_ttr, lv5_ttr, lv6_ttr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This really isn't too surprising! The higher the (perceived) level is, the more students seem to write. Conversely, becuase they're writing more, there's more of a likelihood that they'll repeat their words, especially when compared to much lower levels (1, 2). Of course, TTR tells us more when it's normalized/controlled for length, so maybe we can return to this later.\n",
    "\n",
    "There's a few TTRs of or near 1.0, which means students didn't repeat _any_ words in their essays. Again, that's not too hard to do when you're writing very short essays! However, there's another thing to take into account here: we have seen from our work above that students make a _lot_ of spelling errors, and sometimes the same student will spell the same word several different ways in the same essay! So this measure of TTR still may not have the highest accuracy because students _may actually be repeating words_, but it isn't caught by the TTR function because they're misspelling the word, so they may appear to be different words. Just something to think about!\n",
    "\n",
    "As with the token count, the one-way ANOVA suggests that there is a significant difference between groups here, though we'll still need to do some digging!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "So, I've done quite a bit here, I think! I read in all the CEPA files, accounting for duplicates; standardized the tagging in essays, and then removed it; and now I have some specific linguistic data to look at! The essays, tokens, token counts, and TTR so far. I would like to POS tag and possibly lemmatize still, but I think this is a good start.\n",
    "\n",
    "From my brief (and admittedly pretty shallow) descriptive and quantitative analysis, I found that we still have a pretty good distribution of texts across the different levels. We found that get higher in (perceived) quality/proficiency, they tend to get longer, and their TTR tends to get lower. Again, this is expected, so that's not bad! Both of these differences appear to be signficant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updates: 2019.02.22 - 2019.03.18\n",
    "\n",
    "I think I'll need to lemmatize and POS-tag to get the most common lemmas. I'd like to use the same count and mass nouns across the two corpora analyses. In this update, I compare NLTK and Spacy POS-tagging and lemmatization on essay samples from my corpus, lemmatize and pos-tag the essays using spacy, and get the 3000 most frequent lemmas in the BALC corpus. Those are saved out in csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Load in the lemmatizer from NLTK\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now now ADV RB\n",
      "I -PRON- PRON PRP\n",
      "tell tell VERB VBP\n",
      "you -PRON- PRON PRP\n",
      "why why ADV WRB\n",
      "my -PRON- ADJ PRP$\n",
      "worst bad ADJ JJS\n",
      "holiday holiday NOUN NN\n",
      "ever ever ADV RB\n",
      "in in ADP IN\n",
      "the the DET DT\n",
      "last last ADJ JJ\n",
      "summer summer NOUN NN\n",
      "I -PRON- PRON PRP\n",
      "wented went VERB VBD\n",
      "withe withe VERB VBP\n",
      "my -PRON- ADJ PRP$\n",
      "family family NOUN NN\n",
      "in in ADP IN\n",
      "the the DET DT\n",
      "India india PROPN NNP\n",
      "and and CCONJ CC\n",
      "this this DET DT\n",
      "story story NOUN NN\n",
      "I -PRON- PRON PRP\n",
      "will will VERB MD\n",
      "tell tell VERB VB\n",
      "you -PRON- PRON PRP\n",
      "what what NOUN WP\n",
      "happened happen VERB VBD\n",
      "for for ADP IN\n",
      "the the DET DT\n",
      "short short ADJ JJ\n",
      "story story NOUN NN\n",
      "when when ADV WRB\n",
      "I -PRON- PRON PRP\n",
      "go go VERB VBP\n",
      "the the DET DT\n",
      "first first ADJ JJ\n",
      "the the DET DT\n",
      "weathe weathe NOUN NN\n",
      "is be VERB VBZ\n",
      "very very ADV RB\n",
      "very very ADV RB\n",
      "rain rain NOUN NN\n",
      "now now ADV RB\n",
      "bady bady VERB VBP\n",
      "for for ADP IN\n",
      "the the DET DT\n",
      "children child NOUN NNS\n",
      "play play VERB VB\n",
      "out out PART RP\n",
      "when when ADV WRB\n",
      "I -PRON- PRON PRP\n",
      "go go VERB VBP\n",
      "in in ADP IN\n",
      "the the DET DT\n",
      "hotel hotel NOUN NN\n",
      "all all DET DT\n",
      "may may VERB MD\n",
      "family family NOUN NN\n",
      "was be VERB VBD\n",
      "have have VERB VB\n",
      "the the DET DT\n",
      "headk headk NOUN NN\n",
      "in in ADP IN\n",
      "there there ADV RB\n",
      "and and CCONJ CC\n",
      "all all DET DT\n",
      "was be VERB VBD\n",
      "sleep sleep NOUN NN\n",
      "put put VERB VBN\n",
      "for for ADP IN\n",
      "my -PRON- ADJ PRP$\n",
      "I -PRON- PRON PRP\n",
      "can;t can;t NOUN NN\n",
      "sleep sleep NOUN NN\n",
      "because because ADP IN\n",
      "I -PRON- PRON PRP\n",
      "not not ADV RB\n",
      "love love VERB VBP\n",
      "the the DET DT\n",
      "area area NOUN NN\n",
      "in in ADP IN\n",
      "the the DET DT\n",
      "morning morning NOUN NN\n",
      "all all ADJ PDT\n",
      "the the DET DT\n",
      "my -PRON- ADJ PRP$\n",
      "family family NOUN NN\n",
      "weak weak ADJ JJ\n",
      "up up PART RP\n",
      "and and CCONJ CC\n",
      "going go VERB VBG\n",
      "irant irant ADJ JJ\n",
      "but but CCONJ CC\n",
      "is be VERB VBZ\n",
      "the the DET DT\n",
      "strees stree NOUN NNS\n",
      ", , PUNCT ,\n",
      "children child NOUN NNS\n",
      "and and CCONJ CC\n",
      "the the DET DT\n",
      "food food NOUN NN\n",
      "is be VERB VBZ\n",
      "very very ADV RB\n",
      "dearty dearty ADJ JJR\n",
      "earia earia NOUN NN\n",
      "I -PRON- PRON PRP\n",
      "not not ADV RB\n",
      "liked like VERB VBD\n",
      "becuse becuse NOUN NN\n",
      "is be VERB VBZ\n",
      "not not ADV RB\n",
      "nice nice ADJ JJ\n",
      "area area NOUN NN\n",
      "so so ADV RB\n",
      "darty darty NOUN NN\n",
      "and and CCONJ CC\n",
      "people people NOUN NNS\n",
      "there there ADV EX\n",
      "is be VERB VBZ\n",
      "not not ADV RB\n",
      "nice nice ADJ JJ\n",
      "all all ADV RB\n",
      "there there ADV EX\n",
      "have have VERB VBP\n",
      "not not ADV RB\n",
      "happy happy ADJ JJ\n",
      "only only ADV RB\n",
      "sawted sawted ADJ JJ\n",
      "for for ADP IN\n",
      "my -PRON- ADJ PRP$\n",
      "sister sister NOUN NN\n",
      "whem whem PRON PRP\n",
      "she -PRON- PRON PRP\n",
      "take take VERB VBP\n",
      "some some DET DT\n",
      "flower flower NOUN NN\n",
      "for for ADP IN\n",
      "the the DET DT\n",
      "mam mam NOUN NN\n",
      "and and CCONJ CC\n",
      "also also ADV RB\n",
      "when when ADV WRB\n",
      "you -PRON- PRON PRP\n",
      "there there ADV RB\n",
      "you -PRON- PRON PRP\n",
      "see see VERB VBP\n",
      "what what NOUN WP\n",
      "I -PRON- PRON PRP\n",
      "means mean VERB VBZ\n",
      "maby maby VERB VBP\n",
      "some some DET DT\n",
      "body body NOUN NN\n",
      "liked like VERB VBD\n",
      "go go VERB VB\n",
      "ther ther NOUN NN\n",
      "but but CCONJ CC\n",
      "for for ADP IN\n",
      "I -PRON- PRON PRP\n",
      "did do VERB VBD\n",
      "n't not ADV RB\n",
      "liked like VERB VBN\n",
      "And and CCONJ CC\n",
      "for for ADP IN\n",
      "what what ADJ WDT\n",
      "happand happand NOUN NN\n",
      "I -PRON- PRON PRP\n",
      "tell tell VERB VBP\n",
      "for for ADP IN\n",
      "my -PRON- ADJ PRP$\n",
      "family family NOUN NN\n",
      "I -PRON- PRON PRP\n",
      "wanted want VERB VBD\n",
      "to to PART TO\n",
      "go go VERB VB\n",
      "in in ADP IN\n",
      "my -PRON- ADJ PRP$\n",
      "country country NOUN NN\n",
      "I -PRON- PRON PRP\n",
      "did do VERB VBD\n",
      "n't not ADV RB\n",
      "like like VERB VB\n",
      "her -PRON- PRON PRP\n",
      "fainally fainally ADV RB\n",
      "I -PRON- PRON PRP\n",
      "wanted want VERB VBD\n",
      "tell tell VERB VB\n",
      "for for ADP IN\n",
      "hem hem PRON PRP\n",
      "why why ADV WRB\n",
      "your -PRON- ADJ PRP$\n",
      "earea earea ADJ JJ\n",
      "like like INTJ UH\n",
      "thes the NOUN NNS\n",
      "you -PRON- PRON PRP\n",
      "shold shold ADJ JJ\n",
      "clean clean ADJ JJ\n",
      "and and CCONJ CC\n",
      "your -PRON- ADJ PRP$\n",
      "people people NOUN NNS\n",
      "so so ADP IN\n",
      "nerves nerve NOUN NNS\n",
      "... ... PUNCT .\n"
     ]
    }
   ],
   "source": [
    "# Let's start with spacy, since it's new!\n",
    "y = cepa_df.Revised_Essay[0]\n",
    "doc = nlp(y)\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_) # tagging and lemmatizing built in to spacy commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Now', 'RB'), ('I', 'PRP'), ('tell', 'VBP'), ('you', 'PRP'), ('why', 'WRB'), ('my', 'PRP$'), ('worst', 'JJS'), ('holiday', 'NN'), ('ever', 'RB'), ('in', 'IN'), ('the', 'DT'), ('last', 'JJ'), ('summer', 'NN'), ('I', 'PRP'), ('wented', 'VBD'), ('withe', 'JJ'), ('my', 'PRP$'), ('family', 'NN'), ('in', 'IN'), ('the', 'DT'), ('India', 'NNP'), ('and', 'CC'), ('this', 'DT'), ('story', 'NN'), ('I', 'PRP'), ('will', 'MD'), ('tell', 'VB'), ('you', 'PRP'), ('what', 'WDT'), ('happened', 'VBD'), ('for', 'IN'), ('the', 'DT'), ('short', 'JJ'), ('story', 'NN'), ('when', 'WRB'), ('I', 'PRP'), ('go', 'VBP'), ('the', 'DT'), ('first', 'JJ'), ('the', 'DT'), ('weathe', 'NN'), ('is', 'VBZ'), ('very', 'RB'), ('very', 'RB'), ('rain', 'RB'), ('now', 'RB'), ('bady', 'VBZ'), ('for', 'IN'), ('the', 'DT'), ('children', 'NNS'), ('play', 'VBP'), ('out', 'RP'), ('when', 'WRB'), ('I', 'PRP'), ('go', 'VBP'), ('in', 'IN'), ('the', 'DT'), ('hotel', 'NN'), ('all', 'DT'), ('may', 'MD'), ('family', 'NN'), ('was', 'VBD'), ('have', 'VBP'), ('the', 'DT'), ('headk', 'NN'), ('in', 'IN'), ('there', 'EX'), ('and', 'CC'), ('all', 'DT'), ('was', 'VBD'), ('sleep', 'JJ'), ('put', 'NN'), ('for', 'IN'), ('my', 'PRP$'), ('I', 'PRP'), ('can', 'MD'), (';', ':'), ('t', 'VB'), ('sleep', 'NN'), ('because', 'IN'), ('I', 'PRP'), ('not', 'RB'), ('love', 'VB'), ('the', 'DT'), ('area', 'NN'), ('in', 'IN'), ('the', 'DT'), ('morning', 'NN'), ('all', 'PDT'), ('the', 'DT'), ('my', 'PRP$'), ('family', 'NN'), ('weak', 'JJ'), ('up', 'RB'), ('and', 'CC'), ('going', 'VBG'), ('irant', 'JJ'), ('but', 'CC'), ('is', 'VBZ'), ('the', 'DT'), ('strees', 'NNS'), (',', ','), ('children', 'NNS'), ('and', 'CC'), ('the', 'DT'), ('food', 'NN'), ('is', 'VBZ'), ('very', 'RB'), ('dearty', 'JJ'), ('earia', 'NN'), ('I', 'PRP'), ('not', 'RB'), ('liked', 'VBD'), ('becuse', 'NN'), ('is', 'VBZ'), ('not', 'RB'), ('nice', 'JJ'), ('area', 'NN'), ('so', 'RB'), ('darty', 'JJ'), ('and', 'CC'), ('people', 'NNS'), ('there', 'EX'), ('is', 'VBZ'), ('not', 'RB'), ('nice', 'JJ'), ('all', 'RB'), ('there', 'RB'), ('have', 'VBP'), ('not', 'RB'), ('happy', 'JJ'), ('only', 'RB'), ('sawted', 'VBD'), ('for', 'IN'), ('my', 'PRP$'), ('sister', 'NN'), ('whem', 'NN'), ('she', 'PRP'), ('take', 'VB'), ('some', 'DT'), ('flower', 'NN'), ('for', 'IN'), ('the', 'DT'), ('mam', 'NN'), ('and', 'CC'), ('also', 'RB'), ('when', 'WRB'), ('you', 'PRP'), ('there', 'EX'), ('you', 'PRP'), ('see', 'VBP'), ('what', 'WP'), ('I', 'PRP'), ('means', 'VBP'), ('maby', 'JJ'), ('some', 'DT'), ('body', 'NN'), ('liked', 'VBD'), ('go', 'VB'), ('ther', 'RB'), ('but', 'CC'), ('for', 'IN'), ('I', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('liked', 'VBN'), ('And', 'CC'), ('for', 'IN'), ('what', 'WP'), ('happand', 'NN'), ('I', 'PRP'), ('tell', 'VBP'), ('for', 'IN'), ('my', 'PRP$'), ('family', 'NN'), ('I', 'PRP'), ('wanted', 'VBD'), ('to', 'TO'), ('go', 'VB'), ('in', 'IN'), ('my', 'PRP$'), ('country', 'NN'), ('I', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('like', 'IN'), ('her', 'PRP$'), ('fainally', 'RB'), ('I', 'PRP'), ('wanted', 'VBD'), ('tell', 'NN'), ('for', 'IN'), ('hem', 'JJ'), ('why', 'WRB'), ('your', 'PRP$'), ('earea', 'NN'), ('like', 'IN'), ('thes', 'NNS'), ('you', 'PRP'), ('shold', 'VBP'), ('clean', 'JJ'), ('and', 'CC'), ('your', 'PRP$'), ('people', 'NNS'), ('so', 'RB'), ('nerves', 'NNS'), ('...', ':')]\n"
     ]
    }
   ],
   "source": [
    "# now looking at nltk\n",
    "for token in [cepa_df.tokens[0]]:\n",
    "    print(nltk.pos_tag(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Now\n",
      "I I\n",
      "tell tell\n",
      "you you\n",
      "why why\n",
      "my my\n",
      "worst worst\n",
      "holiday holiday\n",
      "ever ever\n",
      "in in\n",
      "the the\n",
      "last last\n",
      "summer summer\n",
      "I I\n",
      "wented wented\n",
      "withe withe\n",
      "my my\n",
      "family family\n",
      "in in\n",
      "the the\n",
      "India India\n",
      "and and\n",
      "this this\n",
      "story story\n",
      "I I\n",
      "will will\n",
      "tell tell\n",
      "you you\n",
      "what what\n",
      "happened happened\n",
      "for for\n",
      "the the\n",
      "short short\n",
      "story story\n",
      "when when\n",
      "I I\n",
      "go go\n",
      "the the\n",
      "first first\n",
      "the the\n",
      "weathe weathe\n",
      "is is\n",
      "very very\n",
      "very very\n",
      "rain rain\n",
      "now now\n",
      "bady bady\n",
      "for for\n",
      "the the\n",
      "children child\n",
      "play play\n",
      "out out\n",
      "when when\n",
      "I I\n",
      "go go\n",
      "in in\n",
      "the the\n",
      "hotel hotel\n",
      "all all\n",
      "may may\n",
      "family family\n",
      "was wa\n",
      "have have\n",
      "the the\n",
      "headk headk\n",
      "in in\n",
      "there there\n",
      "and and\n",
      "all all\n",
      "was wa\n",
      "sleep sleep\n",
      "put put\n",
      "for for\n",
      "my my\n",
      "I I\n",
      "can can\n",
      "; ;\n",
      "t t\n",
      "sleep sleep\n",
      "because because\n",
      "I I\n",
      "not not\n",
      "love love\n",
      "the the\n",
      "area area\n",
      "in in\n",
      "the the\n",
      "morning morning\n",
      "all all\n",
      "the the\n",
      "my my\n",
      "family family\n",
      "weak weak\n",
      "up up\n",
      "and and\n",
      "going going\n",
      "irant irant\n",
      "but but\n",
      "is is\n",
      "the the\n",
      "strees strees\n",
      ", ,\n",
      "children child\n",
      "and and\n",
      "the the\n",
      "food food\n",
      "is is\n",
      "very very\n",
      "dearty dearty\n",
      "earia earia\n",
      "I I\n",
      "not not\n",
      "liked liked\n",
      "becuse becuse\n",
      "is is\n",
      "not not\n",
      "nice nice\n",
      "area area\n",
      "so so\n",
      "darty darty\n",
      "and and\n",
      "people people\n",
      "there there\n",
      "is is\n",
      "not not\n",
      "nice nice\n",
      "all all\n",
      "there there\n",
      "have have\n",
      "not not\n",
      "happy happy\n",
      "only only\n",
      "sawted sawted\n",
      "for for\n",
      "my my\n",
      "sister sister\n",
      "whem whem\n",
      "she she\n",
      "take take\n",
      "some some\n",
      "flower flower\n",
      "for for\n",
      "the the\n",
      "mam mam\n",
      "and and\n",
      "also also\n",
      "when when\n",
      "you you\n",
      "there there\n",
      "you you\n",
      "see see\n",
      "what what\n",
      "I I\n",
      "means mean\n",
      "maby maby\n",
      "some some\n",
      "body body\n",
      "liked liked\n",
      "go go\n",
      "ther ther\n",
      "but but\n",
      "for for\n",
      "I I\n",
      "did did\n",
      "n't n't\n",
      "liked liked\n",
      "And And\n",
      "for for\n",
      "what what\n",
      "happand happand\n",
      "I I\n",
      "tell tell\n",
      "for for\n",
      "my my\n",
      "family family\n",
      "I I\n",
      "wanted wanted\n",
      "to to\n",
      "go go\n",
      "in in\n",
      "my my\n",
      "country country\n",
      "I I\n",
      "did did\n",
      "n't n't\n",
      "like like\n",
      "her her\n",
      "fainally fainally\n",
      "I I\n",
      "wanted wanted\n",
      "tell tell\n",
      "for for\n",
      "hem hem\n",
      "why why\n",
      "your your\n",
      "earea earea\n",
      "like like\n",
      "thes thes\n",
      "you you\n",
      "shold shold\n",
      "clean clean\n",
      "and and\n",
      "your your\n",
      "people people\n",
      "so so\n",
      "nerves nerve\n",
      "... ...\n"
     ]
    }
   ],
   "source": [
    "# This doesn't look as good as the spacy  lemmatizer\n",
    "for token in cepa_df.tokens[0]:\n",
    "    print(token, lemmatizer.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRP -PRON-\n",
      "like VBP like\n",
      "to TO to\n",
      "bow VB bow\n",
      "and CC and\n",
      "look VB look\n",
      "at IN at\n",
      "bows NNS bow\n",
      "on IN on\n",
      "presents NNS present\n",
      ". . .\n",
      "I PRP -PRON-\n",
      "wented VBD went\n",
      "to IN to\n",
      "a DT a\n",
      "store NN store\n",
      "on IN on\n",
      "fifth JJ fifth\n",
      "avenue NN avenue\n",
      ". . .\n",
      "I I\n",
      "like like\n",
      "to to\n",
      "bow bow\n",
      "and and\n",
      "look look\n",
      "at at\n",
      "bows bow\n",
      "on on\n",
      "presents present\n",
      ". .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'), ('like', 'VBP'), ('to', 'TO'), ('bow', 'VB'), ('and', 'CC'), ('look', 'VB'), ('at', 'IN'), ('bows', 'NNS'), ('on', 'IN'), ('presents.', 'NN')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I\n",
      "wented wented\n",
      "to to\n",
      "a a\n",
      "store store\n",
      "on on\n",
      "fifth fifth\n",
      "avenue avenue\n",
      ". .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'), ('wented', 'VBD'), ('to', 'TO'), ('a', 'DT'), ('store', 'NN'), ('on', 'IN'), ('fifth', 'JJ'), ('avenue.', 'NN')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make my own example, to test ambiguity\n",
    "test = \"I like to bow and look at bows on presents.\"\n",
    "test2 = \"I wented to a store on fifth avenue.\"\n",
    "\n",
    "# Spacy\n",
    "t = nlp(test)\n",
    "for tok in t:\n",
    "    print(tok, tok.tag_, tok.lemma_)\n",
    "    \n",
    "t = nlp(test2)\n",
    "for tok in t:\n",
    "    print(tok, tok.tag_, tok.lemma_)\n",
    "\n",
    "\n",
    "# NLTK\n",
    "for tok in nltk.word_tokenize(test):\n",
    "    print(tok, lemmatizer.lemmatize(tok))\n",
    "nltk.tag.pos_tag(test.split())\n",
    "\n",
    "for tok in nltk.word_tokenize(test2):\n",
    "    print(tok, lemmatizer.lemmatize(tok))\n",
    "nltk.tag.pos_tag(test2.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I think for my purposes, the spacy lemmatization seems a little more accurate -- the NLTK lemmatizer keeps inflections (e.g. `liked -> liked` instead of `liked -> like`). Spacy also account for things like `wented -> went`.\n",
    "\n",
    "It lemmatizes all pronouns as `-PRON-`, which is kind of annoying, but I'm not looking at pronouns, so I can throw this out for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty lists for lemmas and pos -- we don't need to tokenize, since that has already been done\n",
    "lemmas = []\n",
    "pos = []\n",
    "\n",
    "for doc in nlp.pipe(cepa_df.Revised_Essay.astype('unicode').values):\n",
    "    lemmas.append([n.lemma_ for n in doc])\n",
    "    pos.append([(n, n.tag_) for n in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['now', '-PRON-', 'tell', '-PRON-', 'why']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Level</th>\n",
       "      <th>Original_Text</th>\n",
       "      <th>Normalized_Essay</th>\n",
       "      <th>Revised_Essay</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_count</th>\n",
       "      <th>TTR</th>\n",
       "      <th>pos</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200607296</td>\n",
       "      <td>3</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 3 200607296\\n\\n\\n\\nNow I tell you...</td>\n",
       "      <td>Now I tell you why my worst holiday ever in th...</td>\n",
       "      <td>Now I tell you why my worst holiday ever in th...</td>\n",
       "      <td>[Now, I, tell, you, why, my, worst, holiday, e...</td>\n",
       "      <td>207</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>[(Now, RB), (I, PRP), (tell, VBP), (you, PRP),...</td>\n",
       "      <td>[now, -PRON-, tell, -PRON-, why, -PRON-, bad, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200607457</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200607457\\n\\n\\n\\n              ...</td>\n",
       "      <td>My worst holiday Last year I have just had the...</td>\n",
       "      <td>My worst holiday Last year I have just had the...</td>\n",
       "      <td>[My, worst, holiday, Last, year, I, have, just...</td>\n",
       "      <td>180</td>\n",
       "      <td>0.572222</td>\n",
       "      <td>[(My, PRP$), (worst, JJS), (holiday, NN), (Las...</td>\n",
       "      <td>[-PRON-, bad, holiday, last, year, -PRON-, hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200600487</td>\n",
       "      <td>5</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 5 200600487\\n\\n\\n\\n\\nEvery body i...</td>\n",
       "      <td>Every body in this life have a favourite posse...</td>\n",
       "      <td>Every body in this life have a favourite posse...</td>\n",
       "      <td>[Every, body, in, this, life, have, a, favouri...</td>\n",
       "      <td>229</td>\n",
       "      <td>0.445415</td>\n",
       "      <td>[(Every, DT), (body, NN), (in, IN), (this, DT)...</td>\n",
       "      <td>[every, body, in, this, life, have, a, favouri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200608016</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 4 200608016\\n\\n\\n\\nEvery body hav...</td>\n",
       "      <td>Every body have a lot ofpossessions in this li...</td>\n",
       "      <td>Every body have a lot ofpossessions in this li...</td>\n",
       "      <td>[Every, body, have, a, lot, ofpossessions, in,...</td>\n",
       "      <td>156</td>\n",
       "      <td>0.608974</td>\n",
       "      <td>[(Every, DT), (body, NN), (have, VBP), (a, DT)...</td>\n",
       "      <td>[every, body, have, a, lot, ofpossession, in, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200611825</td>\n",
       "      <td>1</td>\n",
       "      <td>\\t\\t\\t\\tCEPA 1 200611825\\n\\n\\n\\nyou go in the ...</td>\n",
       "      <td>you go in the oman just had the perfect holida...</td>\n",
       "      <td>you go in the oman just had the perfect holida...</td>\n",
       "      <td>[you, go, in, the, oman, just, had, the, perfe...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>[(you, PRP), (go, VBP), (in, IN), (the, DT), (...</td>\n",
       "      <td>[-PRON-, go, in, the, oman, just, have, the, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Filename Level                                      Original_Text  \\\n",
       "0  200607296     3  \\t\\t\\t\\tCEPA 3 200607296\\n\\n\\n\\nNow I tell you...   \n",
       "1  200607457     4  \\t\\t\\t\\tCEPA 4 200607457\\n\\n\\n\\n              ...   \n",
       "2  200600487     5  \\t\\t\\t\\tCEPA 5 200600487\\n\\n\\n\\n\\nEvery body i...   \n",
       "3  200608016     4  \\t\\t\\t\\tCEPA 4 200608016\\n\\n\\n\\nEvery body hav...   \n",
       "4  200611825     1  \\t\\t\\t\\tCEPA 1 200611825\\n\\n\\n\\nyou go in the ...   \n",
       "\n",
       "                                    Normalized_Essay  \\\n",
       "0  Now I tell you why my worst holiday ever in th...   \n",
       "1  My worst holiday Last year I have just had the...   \n",
       "2  Every body in this life have a favourite posse...   \n",
       "3  Every body have a lot ofpossessions in this li...   \n",
       "4  you go in the oman just had the perfect holida...   \n",
       "\n",
       "                                       Revised_Essay  \\\n",
       "0  Now I tell you why my worst holiday ever in th...   \n",
       "1  My worst holiday Last year I have just had the...   \n",
       "2  Every body in this life have a favourite posse...   \n",
       "3  Every body have a lot ofpossessions in this li...   \n",
       "4  you go in the oman just had the perfect holida...   \n",
       "\n",
       "                                              tokens  token_count       TTR  \\\n",
       "0  [Now, I, tell, you, why, my, worst, holiday, e...          207  0.492754   \n",
       "1  [My, worst, holiday, Last, year, I, have, just...          180  0.572222   \n",
       "2  [Every, body, in, this, life, have, a, favouri...          229  0.445415   \n",
       "3  [Every, body, have, a, lot, ofpossessions, in,...          156  0.608974   \n",
       "4  [you, go, in, the, oman, just, had, the, perfe...           27  0.629630   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  [(Now, RB), (I, PRP), (tell, VBP), (you, PRP),...   \n",
       "1  [(My, PRP$), (worst, JJS), (holiday, NN), (Las...   \n",
       "2  [(Every, DT), (body, NN), (in, IN), (this, DT)...   \n",
       "3  [(Every, DT), (body, NN), (have, VBP), (a, DT)...   \n",
       "4  [(you, PRP), (go, VBP), (in, IN), (the, DT), (...   \n",
       "\n",
       "                                              lemmas  \n",
       "0  [now, -PRON-, tell, -PRON-, why, -PRON-, bad, ...  \n",
       "1  [-PRON-, bad, holiday, last, year, -PRON-, hav...  \n",
       "2  [every, body, in, this, life, have, a, favouri...  \n",
       "3  [every, body, have, a, lot, ofpossession, in, ...  \n",
       "4  [-PRON-, go, in, the, oman, just, have, the, p...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cepa_df['pos'] = pos\n",
    "cepa_df['lemmas'] = lemmas\n",
    "cepa_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to take a closer look at the lemmas... Our lemma list is nested, so we'll need to flatten it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['now', '-PRON-', 'tell', '-PRON-', 'why', '-PRON-', 'bad', 'holiday', 'ever', 'in', 'the', 'last', 'summer', '-PRON-', 'went', 'withe', '-PRON-', 'family', 'in', 'the', 'india', 'and', 'this', 'story', '-PRON-', 'will', 'tell', '-PRON-', 'what', 'happen', 'for', 'the', 'short', 'story', 'when', '-PRON-', 'go', 'the', 'first', 'the', 'weathe', 'be', 'very', 'very', 'rain', 'now', 'bady', 'for', 'the', 'child', 'play', 'out', 'when', '-PRON-', 'go', 'in', 'the', 'hotel', 'all', 'may', 'family', 'be', 'have', 'the', 'headk', 'in', 'there', 'and', 'all', 'be', 'sleep', 'put', 'for', '-PRON-', '-PRON-', 'can;t', 'sleep', 'because', '-PRON-', 'not', 'love', 'the', 'area', 'in', 'the', 'morning', 'all', 'the', '-PRON-', 'family', 'weak', 'up', 'and', 'go', 'irant', 'but', 'be', 'the', 'stree', ',', 'child', 'and', 'the', 'food', 'be', 'very', 'dearty', 'earia', '-PRON-', 'not', 'like', 'becuse', 'be', 'not', 'nice', 'area', 'so', 'darty', 'and', 'people', 'there', 'be', 'not', 'nice', 'all', 'there', 'have', 'not', 'happy', 'only', 'sawted', 'for', '-PRON-', 'sister', 'whem', '-PRON-', 'take', 'some', 'flower', 'for', 'the', 'mam', 'and', 'also', 'when', '-PRON-', 'there', '-PRON-', 'see', 'what', '-PRON-', 'mean', 'maby', 'some', 'body', 'like', 'go', 'ther', 'but', 'for', '-PRON-', 'do', 'not', 'like', 'and', 'for', 'what', 'happand', '-PRON-', 'tell', 'for', '-PRON-', 'family', '-PRON-', 'want', 'to', 'go', 'in', '-PRON-', 'country', '-PRON-', 'do', 'not', 'like', '-PRON-', 'fainally', '-PRON-', 'want', 'tell', 'for', 'hem', 'why', '-PRON-', 'earea', 'like', 'the', '-PRON-', 'shold', 'clean', 'and', '-PRON-', 'people', 'so', 'nerve', '...']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['-PRON-', 'bad', 'holiday', 'last', 'year', '-PRON-', 'have', 'just', 'have', 'the', 'bad', 'holiday', 'ever', '.', '-PRON-', 'be', 'too', 'board', '.', '-PRON-', 'lonely', 'sister', 'have', 'get', 'marry', '.', '-PRON-', 'be', 'make', '-PRON-', 'laugh', 'and', 'play', 'with', '-PRON-', '.', 'but', 'now', 'i`m', 'alone', 'with', '-PRON-', 'male', 'brother', '.', '-PRON-', 'can', 'not', 'stand', '-PRON-', '-PRON-', 'be', 'too', 'noisy', '.', 'in', 'the', 'spring', 'holiday', '-PRON-', 'brother', 'and', '-PRON-', 'travel', 'to', 'australia', 'with', '-PRON-', 'parent', '.', '-PRON-', 'be', 'really', 'great', 'and', 'nice', 'place', ',', 'but', '-PRON-', 'didn`t', 'enjoy', '-PRON-', 'because', 'no', 'girl', 'be', 'with', '-PRON-', '.', '-PRON-', 'ask', '-PRON-', 'cousin', 'to', 'come', 'with', '-PRON-', 'but', '-PRON-', 'refuse', 'that', 'because', '-PRON-', 'join', 'a', 'sport', 'club', '.there', 'be', 'alot', 'of', 'strange', 'animal', '.', '-PRON-', 'brother', 'be', 'take', 'photoe', 'for', 'the', 'animal', 'and', '-PRON-', 'be', 'too', 'board', '.', '-PRON-', 'be', 'walk', 'like', 'a', 'sick', 'person', '.', '-PRON-', 'hat', '-PRON-', 'self', 'and', 'pray', 'to', 'go', 'back', 'to', '-PRON-', 'country', 'to', 'see', '-PRON-', 'sister', '.', '-PRON-', 'miss', '-PRON-', 'so', 'much', '.', 'after', 'few', 'day', '-PRON-', 'come', 'back', 'home', 'then', '-PRON-', 'become', 'the', 'happy', 'person', 'in', 'the', 'world', '.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a small version of lemmas\n",
    "small = lemmas[:3]\n",
    "small[0]\n",
    "small[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'now'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['now', '-PRON-', 'tell', '-PRON-', 'why', '-PRON-', 'bad', 'holiday', 'ever', 'in', 'the', 'last', 'summer', '-PRON-', 'went', 'withe', '-PRON-', 'family', 'in', 'the', 'india', 'and', 'this', 'story', '-PRON-', 'will', 'tell', '-PRON-', 'what', 'happen', 'for', 'the', 'short', 'story', 'when', '-PRON-', 'go', 'the', 'first', 'the', 'weathe', 'be', 'very', 'very', 'rain', 'now', 'bady', 'for', 'the', 'child', 'play', 'out', 'when', '-PRON-', 'go', 'in', 'the', 'hotel', 'all', 'may', 'family', 'be', 'have', 'the', 'headk', 'in', 'there', 'and', 'all', 'be', 'sleep', 'put', 'for', '-PRON-', '-PRON-', 'can;t', 'sleep', 'because', '-PRON-', 'not', 'love', 'the', 'area', 'in', 'the', 'morning', 'all', 'the', '-PRON-', 'family', 'weak', 'up', 'and', 'go', 'irant', 'but', 'be', 'the', 'stree', ',']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking that this is flat\n",
    "small_flat = [x for y in small for x in y]\n",
    "small_flat[0]\n",
    "small_flat[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['now', '-PRON-', 'tell', '-PRON-', 'why', '-PRON-', 'bad', 'holiday', 'ever', 'in', 'the', 'last', 'summer', '-PRON-', 'went', 'withe', '-PRON-', 'family', 'in', 'the', 'india', 'and', 'this', 'story', '-PRON-', 'will', 'tell', '-PRON-', 'what', 'happen', 'for', 'the', 'short', 'story', 'when', '-PRON-', 'go', 'the', 'first', 'the', 'weathe', 'be', 'very', 'very', 'rain', 'now', 'bady', 'for', 'the', 'child', 'play', 'out', 'when', '-PRON-', 'go', 'in', 'the', 'hotel', 'all', 'may', 'family', 'be', 'have', 'the', 'headk', 'in', 'there', 'and', 'all', 'be', 'sleep', 'put', 'for', '-PRON-', '-PRON-', 'can;t', 'sleep', 'because', '-PRON-', 'not', 'love', 'the', 'area', 'in', 'the', 'morning', 'all', 'the', '-PRON-', 'family', 'weak', 'up', 'and', 'go', 'irant', 'but', 'be', 'the', 'stree', ',']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "267270"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# throwing out essays lower than 6 tokens \n",
    "filtered = (cepa_df.token_count >= 6)\n",
    "\n",
    "# flattening all of the lemmas - this will include punctuation and -PRON-\n",
    "all_lemmas_flat = [x for y in cepa_df[filtered].lemmas for x in y]\n",
    "all_lemmas_flat[:100]\n",
    "len(all_lemmas_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing string to throw out punctuation from lemmas\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['now', '-PRON-', 'tell', '-PRON-', 'why', '-PRON-', 'bad', 'holiday', 'ever', 'in', 'the', 'last', 'summer', '-PRON-', 'went', 'withe', '-PRON-', 'family', 'in', 'the', 'india', 'and', 'this', 'story', '-PRON-', 'will', 'tell', '-PRON-', 'what', 'happen', 'for', 'the', 'short', 'story', 'when', '-PRON-', 'go', 'the', 'first', 'the', 'weathe', 'be', 'very', 'very', 'rain', 'now', 'bady', 'for', 'the', 'child', 'play', 'out', 'when', '-PRON-', 'go', 'in', 'the', 'hotel', 'all', 'may', 'family', 'be', 'have', 'the', 'headk', 'in', 'there', 'and', 'all', 'be', 'sleep', 'put', 'for', '-PRON-', '-PRON-', 'can;t', 'sleep', 'because', '-PRON-', 'not', 'love', 'the', 'area', 'in', 'the', 'morning', 'all', 'the', '-PRON-', 'family', 'weak', 'up', 'and', 'go', 'irant', 'but', 'be', 'the', 'stree', ',']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['now', 'tell', 'why', 'bad', 'holiday', 'ever', 'in', 'the', 'last', 'summer', 'went', 'withe', 'family', 'in', 'the', 'india', 'and', 'this', 'story', 'will', 'tell', 'what', 'happen', 'for', 'the', 'short', 'story', 'when', 'go', 'the', 'first', 'the', 'weathe', 'be', 'very', 'very', 'rain', 'now', 'bady', 'for', 'the', 'child', 'play', 'out', 'when', 'go', 'in', 'the', 'hotel', 'all', 'may', 'family', 'be', 'have', 'the', 'headk', 'in', 'there', 'and', 'all', 'be', 'sleep', 'put', 'for', 'can;t', 'sleep', 'because', 'not', 'love', 'the', 'area', 'in', 'the', 'morning', 'all', 'the', 'family', 'weak', 'up', 'and', 'go', 'irant', 'but', 'be', 'the', 'stree', 'child', 'and', 'the', 'food', 'be', 'very', 'dearty', 'earia', 'not', 'like', 'becuse', 'be', 'not', 'nice']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an empty list \n",
    "filtered_lemmas_flat = []\n",
    "\n",
    "for lem in all_lemmas_flat:\n",
    "    if lem not in string.punctuation:  # ignoring punctuation\n",
    "        if lem != '-PRON-':            # ignoring -PRON-\n",
    "            filtered_lemmas_flat.append(lem)\n",
    "\n",
    "all_lemmas_flat[:100]\n",
    "filtered_lemmas_flat[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267270"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "208645"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_lemmas_flat)\n",
    "len(filtered_lemmas_flat)\n",
    "# So we lost about 60,000 lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 13677), ('and', 10492), ('be', 10213), ('to', 8692), ('in', 6122), ('go', 4995), ('a', 3913), ('have', 2823), ('that', 2504), ('of', 2411), ('with', 2404), ('very', 2100), ('holiday', 2067), ('so', 1836), ('for', 1622), ('see', 1582), ('there', 1539), ('not', 1454), ('because', 1447), ('family', 1261), ('do', 1254), ('like', 1168), ('this', 1047), ('can', 1047), ('day', 1028), ('good', 1026), ('but', 994), ('time', 984), ('when', 977), ('all', 898), ('after', 874), ('bad', 863), ('father', 861), ('many', 857), ('on', 850), ('friend', 844), ('one', 788), ('place', 770), ('thing', 741), ('at', 730), ('play', 728), ('people', 724), ('take', 714), ('about', 644), ('will', 621), ('some', 611), ('last', 607), ('happy', 590), ('from', 588), ('ever', 580), ('spend', 580), ('sister', 572), ('nice', 559), ('get', 559), ('come', 555), ('summer', 553), ('wonderful', 550), ('mother', 526), ('also', 524), ('brother', 486), ('enjoy', 485), ('car', 485), ('love', 483), ('film', 471), ('back', 467), ('then', 467), ('visit', 467), ('al', 466), ('xxx', 463), ('what', 451), ('as', 451), ('life', 444), ('want', 437), ('make', 432), ('park', 431), ('by', 429), ('much', 427), ('hotel', 421), ('movie', 412), ('first', 406), ('just', 405), ('really', 401), ('home', 398), ('travel', 395), ('eat', 394), ('country', 391), ('big', 382), ('tell', 367), ('most', 366), ('beautiful', 363), ('which', 351), ('think', 350), ('weekend', 347), ('who', 345), ('stay', 343), ('an', 338), ('perfect', 337), ('dubai', 334), ('know', 330), ('oman', 327)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at a Frequency Distribution\n",
    "filtered_lemma_fd = nltk.FreqDist(filtered_lemmas_flat)\n",
    "filtered_lemma_fd.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a smaller lemmas\n",
    "small_lemmas = filtered_lemma_fd.most_common(3000)\n",
    "\n",
    "# I couldn't make a pandas dataframe straight from small_lemmas\n",
    "balc_lemmas = [[(lem, count) for (lem, count) in small_lemmas]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('the', 13677), ('and', 10492), ('be', 10213), ('to', 8692), ('in', 6122), ('go', 4995), ('a', 3913), ('have', 2823), ('that', 2504), ('of', 2411), ('with', 2404), ('very', 2100), ('holiday', 2067), ('so', 1836), ('for', 1622), ('see', 1582), ('there', 1539), ('not', 1454), ('because', 1447), ('family', 1261), ('do', 1254), ('like', 1168), ('this', 1047), ('can', 1047), ('day', 1028), ('good', 1026), ('but', 994), ('time', 984), ('when', 977), ('all', 898), ('after', 874), ('bad', 863), ('father', 861), ('many', 857), ('on', 850), ('friend', 844), ('one', 788), ('place', 770), ('thing', 741), ('at', 730), ('play', 728), ('people', 724), ('take', 714), ('about', 644), ('will', 621), ('some', 611), ('last', 607), ('happy', 590), ('from', 588), ('ever', 580), ('spend', 580), ('sister', 572), ('nice', 559), ('get', 559), ('come', 555), ('summer', 553), ('wonderful', 550), ('mother', 526), ('also', 524), ('brother', 486), ('enjoy', 485), ('car', 485), ('love', 483), ('film', 471), ('back', 467), ('then', 467), ('visit', 467), ('al', 466), ('xxx', 463), ('what', 451), ('as', 451), ('life', 444), ('want', 437), ('make', 432), ('park', 431), ('by', 429), ('much', 427), ('hotel', 421), ('movie', 412), ('first', 406), ('just', 405), ('really', 401), ('home', 398), ('travel', 395), ('eat', 394), ('country', 391), ('big', 382), ('tell', 367), ('most', 366), ('beautiful', 363), ('which', 351), ('think', 350), ('weekend', 347), ('who', 345), ('stay', 343), ('an', 338), ('perfect', 337), ('dubai', 334), ('know', 330), ('oman', 327), ('every', 320), ('where', 317), ('year', 315), ('city', 310), ('week', 308), ('start', 306), ('sea', 301), ('feel', 299), ('two', 295), ('next', 293), ('or', 291), ('other', 288), ('buy', 286), ('lot', 284), ('shopping', 282), ('call', 280), ('could', 278), ('out', 276), ('more', 269), ('mall', 268), ('food', 267), ('say', 267), ('help', 266), ('i', 265), ('no', 264), ('house', 260), ('watch', 255), ('weather', 253), ('ain', 252), ('would', 251), ('live', 251), ('look', 247), ('zoo', 246), ('school', 243), ('fun', 239), ('decide', 237), ('how', 234), ('any', 234), (\"'s\", 228), ('becouse', 220), (' ', 219), ('shop', 218), ('use', 217), ('why', 216), ('up', 215), ('london', 205), ('uae', 203), ('give', 203), ('finally', 201), ('..', 201), ('great', 199), ('arrive', 198), ('room', 196), ('find', 192), ('sleep', 188), ('animal', 188), ('world', 188), ('hour', 187), ('way', 185), ('football', 185), ('old', 183), ('story', 182), ('work', 179), ('happen', 172), ('talk', 172), ('sad', 172), ('trip', 171), ('if', 170), ('new', 167), ('swim', 167), ('beach', 165), ('small', 164), ('game', 163), ('airport', 161), ('always', 160), ('end', 160), ('finish', 157), ('man', 157), ('now', 156), ('hope', 155), ('walk', 154), ('forget', 154), ('than', 151), ('alain', 149), ('india', 148), ('only', 148), ('topic', 147), ('ask', 144), ('uncle', 142), ('too', 139), ('frind', 139), ('night', 139), ('should', 138), ('before', 136), ('swimming', 135), ('never', 134), ('clothe', 131), ('favourite', 130), ('try', 130), ('plan', 130), ('reach', 129), ('water', 125), ('morning', 124), ('important', 124), ('hot', 123), ('mather', 122), ('learn', 121), ('book', 121), ('three', 121), ('possession', 120), ('bag', 120), ('wait', 120), ('child', 119), ('need', 119), ('computer', 119), ('may', 118), ('such', 118), ('set', 118), ('girl', 117), ('again', 116), ('read', 116), ('describe', 115), ('hospital', 115), ('leave', 113), ('long', 113), ('plane', 112), ('lunch', 112), ('keep', 109), ('another', 109), ('well', 109), ('problem', 109), ('meet', 108), ('vacation', 108), ('while', 107), ('different', 107), ('ther', 106), ('around', 106), ('parent', 104), ('name', 104), ('become', 103), ('2', 103), ('gift', 102), ('write', 101), ('cry', 100), ('wish', 99), ('sit', 99), ('amazing', 99), ('mountain', 98), ('kind', 98), ('grand', 98), ('tired', 98), ('green', 97), ('person', 96), ('stop', 96), ('these', 96), ('cinema', 96), ('body', 95), ('cousin', 95), ('holidy', 95), ('over', 94), ('die', 94), ('something', 93), ('move', 93), ('everything', 93), ('didn`t', 92), ('center', 92), ('tree', 91), ('run', 91), ('month', 90), ('near', 90), ('show', 89), ('imagine', 89), ('put', 88), ('picture', 87), ('little', 87), ('even', 87), ('idea', 87), ('cold', 86), ('reason', 85), ('street', 85), ('rest', 84), ('same', 84), ('without', 83), ('remember', 83), ('dinner', 83), ('each', 83), ('full', 82), ('interesting', 80), ('hand', 80), ('desert', 80), ('second', 78), ('fish', 78), ('whole', 78), ('study', 78), ('club', 77), ('lose', 77), ('3', 77), ('return', 76), ('anything', 75), ('rain', 74), ('alot', 74), ('sick', 74), ('abu', 73), ('face', 72), ('actor', 72), ('egypt', 72), ('it`s', 71), ('must', 71), ('down', 71), ('relax', 71), ('lovely', 70), ('grandmother', 70), ('since', 69), ('here', 69), ('change', 69), ('until', 68), ('pass', 68), ('famous', 68), ('during', 68), ('sharjah', 68), ('self', 67), ('english', 67), ('france', 67), ('boy', 67), ('s', 66), ('hear', 66), ('ball', 66), ('police', 66), ('few', 65), ('away', 65), ('together', 65), ('activity', 64), ('drive', 64), ('sport', 63), ('firstly', 63), ('avery', 63), ('brather', 63), ('withe', 61), ('don`t', 59), ('.i', 59), ('prepare', 58), ('paris', 58), ('god', 58), ('doctor', 58), ('cook', 58), ('five', 58), ('money', 58), ('into', 58), ('recently', 58), ('ago', 57), ('pool', 57), ('exam', 57), ('bring', 57), ('dream', 57), ('although', 56), ('dhabi', 56), ('thank', 56), ('part', 56), ('fall', 56), ('foot', 55), ('land', 55), ('grandfather', 55), ('wake', 55), ('area', 54), ('break', 54), ('fast', 54), ('hard', 54), ('beatiful', 54), ('malaysia', 54), ('everyone', 53), ('clean', 52), ('moreover', 52), ('bird', 52), ('iam', 52), ('head', 52), ('bed', 52), ('camel', 52), ('moment', 52), ('alone', 51), ('care', 51), ('relative', 51), ('b', 51), ('kill', 51), ('woman', 51), ('phone', 51), ('i`m', 50), ('becous', 50), ('aunt', 50), ('drink', 50), ('view', 50), ('museum', 49), ('funny', 49), ('resturant', 49), ('still', 49), ('air', 49), ('freind', 49), ('course', 49), ('own', 48), ('experience', 48), ('indian', 48), ('ali', 48), ('early', 48), ('however', 48), ('5', 48), ('open', 48), ('begin', 48), ('sun', 47), ('flight', 47), ('young', 47), ('win', 47), ('pray', 46), ('special', 46), ('accident', 46), ('star', 46), ('off', 46), ('party', 46), ('dad', 46), ('past', 46), ('ready', 45), ('passport', 45), ('huge', 45), ('opinion', 45), ('side', 45), ('germany', 45), ('let', 44), ('cool', 44), ('soon', 44), ('mobile', 44), ('road', 44), ('england', 44), ('wear', 44), ('order', 44), ('flower', 43), ('th', 43), ('sky', 43), ('ticket', 43), ('advise', 43), ('between', 43), ('town', 43), ('group', 43), ('minute', 43), ('internet', 42), ('high', 42), ('lion', 42), ('fire', 42), ('addition', 42), ('australia', 41), ('speak', 41), ('journey', 41), ('taxi', 41), ('u.a.e', 41), ('dress', 41), ('through', 41), ('sand', 41), ('salalah', 41), ('eye', 41), ('war', 41), ('mean', 40), ('under', 40), ('playing', 40), ('fill', 40), ('turn', 40), ('station', 40), ('believe', 39), ('ride', 39), ('far', 39), ('free', 39), ('history', 39), ('word', 39), ('teacher', 39), ('exciting', 38), ('ice', 38), ('excited', 38), ('happend', 38), ('frend', 38), ('later', 38), ('outside', 38), ('train', 38), ('main', 38), ('thailand', 38), ('culture', 38), ('picnic', 38), ('building', 37), ('example', 37), ('fantastic', 37), ('snow', 37), ('island', 37), ('enter', 37), ('italy', 37), ('fact', 37), ('late', 37), ('useful', 36), ('once', 36), ('usually', 36), ('seem', 36), ('tent', 36), ('camera', 36), ('close', 36), ('heart', 36), ('wife', 36), ('sow', 36), ('photo', 35), ('both', 35), ('along', 35), ('bark', 35), ('befor', 35), ('market', 35), ('wen', 35), ('lake', 35), ('eygpt', 35), ('laugh', 34), ('miss', 34), ('breakfast', 34), ('check', 34), ('tower', 34), ('river', 34), ('information', 34), ('ring', 34), ('poor', 34), ('pm', 33), ('vere', 33), ('angry', 33), ('wrong', 33), ('son', 33), ('choose', 33), ('shout', 33), ('mind', 33), ('…', 33), ('farm', 33), ('real', 33), ('student', 33), ('.and', 33), ('boring', 33), ('becuse', 32), ('join', 32), ('afraid', 32), ('feeling', 32), ('evening', 32), ('bus', 32), ('four', 32), ('agree', 32), ('comfortable', 32), ('those', 32), ('store', 32), ('build', 32), ('4', 32), ('boat', 32), ('tv', 32), ('modern', 32), ('horse', 31), ('secondly', 31), ('village', 31), ('m', 31), ('.we', 31), ('hoilday', 31), ('2005', 31), ('birthday', 31), ('driver', 31), ('send', 31), ('site', 31), ('right', 31), ('teach', 31), ('favorite', 31), ('nature', 31), ('becaus', 31), ('top', 31), ('...', 30), ('stand', 30), ('job', 30), ('vist', 30), ('catch', 30), ('bout', 30), ('.the', 30), ('ahmed', 30), ('cover', 30), ('steal', 30), ('cat', 30), ('advice', 30), ('match', 30), ('swam', 30), ('vary', 30), ('fly', 30), ('tall', 30), ('memory', 30), ('lebanon', 30), ('capital', 29), ('bay', 29), ('camp', 29), ('till', 29), ('large', 29), ('search', 29), ('fine', 29), ('ship', 29), ('fathe', 29), ('present', 29), ('invite', 29), ('king', 29), ('can`t', 29), ('.after', 29), ('sometimes', 29), ('fight', 29), ('enjoyable', 28), ('chance', 28), ('wonder', 28), ('rome', 28), ('1', 28), ('restaurant', 28), ('leg', 28), ('arab', 28), ('dog', 28), ('usa', 28), ('finsh', 27), ('camping', 27), ('nothing', 27), ('cam', 27), ('hafeet', 27), ('verey', 27), ('expect', 27), ('sweet', 27), ('chat', 26), ('strong', 26), ('kid', 26), ('surprise', 26), ('instead', 26), ('inside', 26), ('meat', 26), ('third', 26), ('shoe', 26), ('death', 26), ('mom', 26), ('realy', 26), ('baby', 26), ('suddenly', 25), ('10', 25), ('anice', 25), ('someone', 25), ('syria', 25), ('japan', 25), ('arabia', 25), ('matter', 25), ('finaly', 25), ('frand', 25), ('beatch', 25), ('wonderfull', 25), ('toke', 25), ('familly', 25), ('maybe', 25), ('ok', 25), ('oh', 25), ('butiful', 25), ('blue', 25), ('interst', 25), ('laptop', 24), ('famly', 24), ('travell', 24), ('discover', 24), ('sure', 24), ('quality', 24), ('t', 24), ('friendly', 24), ('thi', 24), ('tow', 24), ('dark', 24), ('door', 24), ('news', 24), ('ten', 24), ('anyone', 24), ('couple', 24), ('music', 24), ('thier', 24), ('diving', 24), ('6', 24), ('diary', 24), ('afternoon', 24), ('wich', 23), ('conclusion', 23), ('taj', 23), ('lik', 23), ('rent', 23), ('dangerous', 23), ('mothe', 23), ('smile', 23), ('hold', 23), ('my', 23), ('event', 23), ('delicious', 23), ('nise', 23), ('hem', 22), ('hobby', 22), ('tour', 22), ('behind', 22), ('climb', 22), ('crowded', 22), ('abroad', 22), ('7', 22), ('fresh', 22), ('act', 22), ('action', 22), ('everyday', 22), ('u.a.e.', 22), ('quite', 22), ('scary', 22), ('wood', 22), ('.it', 22), ('pack', 22), ('seven', 22), ('pla', 22), ('plant', 22), ('carry', 22), ('complete', 22), ('almost', 22), ('remind', 22), ('mounten', 22), ('machine', 21), ('yet', 21), ('friends', 21), ('light', 21), ('contry', 21), ('cantry', 21), ('salala', 21), ('famliy', 21), ('enough', 21), ('america', 21), ('clear', 21), ('today', 21), ('able', 21), ('improve', 21), ('al-', 21), ('bear', 21), ('company', 21), ('save', 21), ('wondeful', 21), ('role', 21), ('terrible', 21), ('historical', 21), ('marry', 20), ('hat', 20), ('share', 20), ('beutiful', 20), ('member', 20), ('winter', 20), ('hungry', 20), ('evil', 20), ('anywhere', 20), ('agood', 20), ('tourist', 20), ('qatar', 20), ('united', 20), ('am', 20), ('pick', 20), ('t.v', 20), ('saudi', 20), ('cray', 20), ('treat', 20), ('box', 20), (\"o'clock\", 20), ('draw', 20), ('seat', 20), ('short', 19), ('expensive', 19), ('team', 19), ('daughter', 19), ('listen', 19), ('luck', 19), ('quickly', 19), ('stress', 19), ('suggest', 19), ('feed', 19), ('dudai', 19), ('traditional', 19), ('garden', 19), ('maka', 19), ('evry', 19), ('peace', 19), ('lesson', 19), ('sunny', 19), ('hate', 19), ('europe', 19), ('theis', 19), ('kuwait', 19), ('festival', 19), ('true', 19), ('rid', 19), ('everybody', 19), ('wather', 19), ('harry', 19), ('fraind', 19), ('centre', 19), ('wedding', 19), ('nile', 19), ('tha', 19), ('spring', 18), ('reson', 18), ('future', 18), ('easy', 18), ('crowd', 18), ('meal', 18), ('sorry', 18), ('hatta', 18), ('cow', 18), ('stuff', 18), ('bored', 18), ('step', 18), ('normal', 18), ('comedy', 18), ('deal', 18), ('fruit', 18), ('gold', 18), ('especially', 18), ('joy', 18), ('though', 18), ('becase', 18), ('famile', 18), ('moll', 18), ('actually', 18), ('omra', 18), ('rich', 18), ('cartoon', 18), ('dirty', 18), ('famaly', 18), ('famely', 18), ('landan', 18), ('ski', 18), ('necklace', 18), ('strange', 17), ('language', 17), ('healthy', 17), ('throw', 17), ('husband', 17), ('wor', 17), ('include', 17), ('untill', 17), ('sara', 17), ('tierd', 17), ('intrest', 17), ('quran', 17), ('warm', 17), ('type', 17), ('contain', 17), ('forward', 17), ('hide', 17), ('art', 17), ('sell', 17), ('firend', 17), ('zayed', 17), ('crash', 17), ('china', 17), ('wether', 17), ('song', 17), ('personal', 17), ('heat', 17), ('black', 17), ('director', 17), ('tom', 17), ('troy', 17), ('board', 16), ('.my', 16), ('hop', 16), ('frin', 16), ('holeday', 16), ('paper', 16), ('least', 16), ('12', 16), ('point', 16), ('due', 16), ('already', 16), ('white', 16), ('natural', 16), ('thir', 16), ('situation', 16), ('wath', 16), ('realize', 16), ('simple', 16), ('pleace', 16), ('lady', 16), ('red', 16), ('sae', 16), ('sing', 16), ('notice', 16), ('italian', 16), ('ill', 16), ('library', 16), ('took', 16), ('brad', 16), ('peach', 16), ('switzerland', 16), ('bady', 15), ('fainally', 15), ('noisy', 15), ('cute', 15), ('please', 15), ('answer', 15), ('mahal', 15), ('fishing', 15), ('.in', 15), ('horrible', 15), ('scared', 15), ('policeman', 15), ('six', 15), ('pretty', 15), ('bat', 15), ('prefer', 15), ('cut', 15), ('toy', 15), ('gather', 15), ('snake', 15), ('dear', 15), ('dubia', 15), ('shocked', 15), ('n', 15), ('beuteful', 15), ('hang', 15), ('8', 15), ('letter', 15), ('ver', 15), ('cruise', 15), ('university', 15), ('beside', 15), ('unfortunately', 15), ('class', 15), ('test', 14), ('finesh', 14), ('summar', 14), ('wasn`t', 14), ('scene', 14), ('towards', 14), ('lie', 14), ('net', 14), ('blood', 14), ('beause', 14), ('busy', 14), ('sound', 14), ('cooking', 14), ('friday', 14), ('final', 14), ('.when', 14), ('bit', 14), ('understand', 14), ('traveld', 14), ('milan', 14), ('york', 14), ('san', 14), ('state', 14), ('holy', 14), ('kan', 14), ('essay', 14), (\"i'am\", 14), ('tiger', 14), ('treasure', 14), ('9', 14), ('belive', 14), ('filme', 14), ('frinde', 14), ('desart', 14), ('human', 14), ('monkey', 14), ('unhappy', 14), ('character', 14), ('consider', 14), ('pay', 14), ('sharja', 14), ('bake', 14), ('deny', 14), ('musiam', 14), ('muslim', 14), ('mecca', 14), ('floor', 14), ('create', 14), ('africa', 14), ('couldn`t', 14), ('form', 14), ('ksa', 14), ('writing', 13), ('mark', 13), ('mathe', 13), ('rice', 13), ('flat', 13), ('chocolate', 13), ('ar', 13), ('rush', 13), ('color', 13), ('sight', 13), ('disney', 13), ('adventure', 13), ('kaba', 13), ('case', 13), ('vegetable', 13), ('race', 13), ('suitcase', 13), ('prefect', 13), ('touch', 13), ('wither', 13), ('oll', 13), ('collect', 13), ('dubi', 13), ('ear', 13), ('health', 13), ('som', 13), ('soldier', 13), ('allah', 13), ('age', 13), ('roma', 13), ('shock', 13), ('travil', 13), ('mistake', 13), ('global', 13), ('pyramid', 13), ('ground', 13), ('amd', 13), ('togather', 13), ('robot', 13), ('subject', 13), ('elephant', 13), ('plase', 13), ('20', 13), ('remmber', 13), ('dead', 13), ('perfume', 13), ('geneva', 13), ('everywhere', 13), ('port', 13), ('protect', 13), ('nanny', 13), ('brothe', 13), ('half', 13), ('card', 12), ('beginning', 12), ('ajman', 12), ('video', 12), ('buteful', 12), ('line', 12), ('safe', 12), ('destination', 12), ('cost', 12), ('mosque', 12), ('rainy', 12), ('polite', 12), ('retern', 12), ('mohammed', 12), ('stai', 12), ('zoom', 12), ('habby', 12), ('bast', 12), ('japanese', 12), ('eventually', 12), ('madina', 12), ('guide', 12), ('hous', 12), ('hotil', 12), ('tradition', 12), ('number', 12), ('active', 12), ('cause', 12), ('nobody', 12), ('30', 12), ('soo', 12), ('jordan', 12), ('exactly', 12), ('mountin', 12), ('better', 12), ('exit', 12), ('gulf', 12), ('nd', 12), ('explore', 12), ('.also', 12), ('several', 12), ('diffrent', 12), ('speed', 12), ('restorant', 12), ('slalah', 12), ('thie', 12), ('pizza', 12), ('hottel', 12), ('ran', 12), ('theme', 12), ('mountine', 12), ('grandparent', 12), ('coll', 12), ('faimly', 12), ('criminal', 12), ('wiht', 12), ('happiness', 12), ('else', 12), ('middle', 12), ('photograph', 12), ('scream', 12), ('yo', 12), ('emirates', 12), ('wash', 12), ('fimaly', 12), ('leader', 12), ('ful', 12), ('solve', 12), ('beacuse', 12), ('sistar', 12), ('refuse', 11), ('forever', 11), ('wet', 11), ('mane', 11), ('blay', 11), ('helpful', 11), ('gran', 11), ('exercise', 11), ('moon', 11), ('traffic', 11), ('delicous', 11), ('surprised', 11), ('omrah', 11), ('makah', 11), ('childhood', 11), ('anth', 11), ('pain', 11), ('mention', 11), ('personality', 11), ('spot', 11), ('aftar', 11), ('butifull', 11), ('quit', 11), ('films', 11), ('tennis', 11), ('thise', 11), ('whenever', 11), ('forest', 11), ('breath', 11), ('secret', 11), ('hair', 11), ('bore', 11), ('that`s', 11), ('visite', 11), ('period', 11), ('precious', 11), ('education', 11), ('connect', 11), ('wat', 11), ('front', 11), ('funy', 11), ('wild', 11), ('table', 11), ('journy', 11), ('ras', 11), ('hurt', 11), ('entertainment', 11), ('respect', 11), ('happ', 11), ('cinter', 11), ('thim', 11), ('castle', 11), ('interested', 11), ('available', 11), ('bak', 11), ('effect', 11), ('frome', 11), ('iran', 11), ('cheap', 11), ('bracelet', 11), ('shape', 11), ('cream', 11), ('tripe', 11), ('worried', 11), ('result', 11), ('fail', 11), ('pen', 11), ('hole', 11), ('whith', 11), ('allow', 11), ('army', 11), ('gardan', 11), ('weak', 10), ('design', 10), ('surf', 10), ('relationship', 10), ('cepa', 10), ('loud', 10), ('their', 10), ('fort', 10), ('weakend', 10), ('fater', 10), ('holday', 10), ('shoop', 10), ('noise', 10), ('e', 10), ('worry', 10), ('thursday', 10), ('voice', 10), ('popular', 10), ('hyde', 10), ('ail', 10), ('grill', 10), ('welcome', 10), ('p.m.', 10), ('certain', 10), ('coold', 10), ('grow', 10), ('prison', 10), ('games', 10), ('airplane', 10), ('sadness', 10), ('continue', 10), ('homework', 10), ('rather', 10), ('deep', 10), ('importance', 10), ('bahrain', 10), ('surround', 10), ('meaning', 10), ('skill', 10), ('grass', 10), ('turkey', 10), ('aftr', 10), ('ant', 10), ('peaple', 10), ('cultural', 10), ('famouse', 10), ('wall', 10), ('ray', 10), ('therefore', 10), ('coffee', 10), ('develop', 10), ('w', 10), ('beg', 10), ('promise', 10), ('image', 10), ('frined', 10), ('dance', 10), ('sometime', 10), ('against', 10), ('pepole', 10), ('kong', 10), ('dry', 10), ('above', 10), ('magic', 10), ('rak', 10), ('french', 10), ('magnificent', 10), ('proud', 10), ('puteful', 10), ('smell', 10), ('magical', 10), ('malysia', 10), ('soudia', 10), ('secound', 10), ('badly', 10), ('mr', 10), ('salalh', 10), ('calm', 10), ('single', 10), ('anyway', 10), ('cenima', 10), ('date', 10), ('window', 10), ('peple', 10), ('jabal', 10), ('franch', 10), ('perant', 10), ('energy', 10), ('landen', 10), ('rohet', 10), ('weathe', 9), ('photoe', 9), ('trainer', 9), ('fit', 9), ('sester', 9), ('larg', 9), ('consist', 9), ('wind', 9), ('crazy', 9), ('sheet', 9), ('cinma', 9), ('tim', 9), ('bet', 9), ('infront', 9), ('mum', 9), ('intresting', 9), ('relaxing', 9), ('peaceful', 9), ('wer', 9), ('clock', 9), ('villa', 9), ('dive', 9), ('qater', 9), ('unte', 9), ('yes', 9), ('ariv', 9), ('pleased', 9), ('earth', 9), ('tailand', 9), ('joke', 9), ('count', 9), ('captain', 9), ('mool', 9), ('stud', 9), ('hourse', 9), ('uk', 9), ('coast', 9), ('nearly', 9), ('math', 9), ('desid', 9), ('manage', 9), ('guess', 9), ('daily', 9), ('fimly', 9), ('guy', 9), ('beauty', 9), ('saw', 9), ('july', 9), ('nightmare', 9), ('safa', 9), ('a.m.', 9), ('fathar', 9), ('lord', 9), ('lead', 9), ('ilike', 9), ('fatima', 9), ('littel', 9), ('taste', 9), ('note', 9), ('labtop', 9), ('tea', 9), ('wide', 9), ('saad', 9), ('cup', 9), ('50', 9), ('o', 9), ('cherish', 9), ('receive', 9), ('lock', 9), ('mona', 9), ('hit', 9), ('whit', 9), ('bathroom', 9), ('dabai', 9), ('colour', 9), ('question', 9), ('worst', 9), ('anew', 9), ('ben', 9), ('bank', 9), (\"i'v\", 9), ('cairo', 9), ('beautifull', 9), ('liwa', 9), ('communicate', 9), ('basketball', 9), ('lucky', 9), ('malls', 9), ('choice', 9), ('whatev', 9), ('becose', 9), ('bother', 9), ('footboll', 9), ('abad', 9), ('clup', 9), ('waste', 9), ('left', 9), ('ather', 9), ('tire', 9), ('sead', 9), ('anther', 9), ('fary', 9), ('chess', 9), ('drawing', 9), ('bin', 9), ('safari', 9), ('abig', 9), ('ahmad', 9), ('stick', 9), ('factory', 9), ('queen', 9), ('viset', 9), ('calendar', 9), ('went', 8), ('benefit', 8), ('skate', 8), ('sammar', 8), ('prath', 8), ('happe', 8), ('footbool', 8), ('blow', 8), ('explain', 8), ('locate', 8), ('wafi', 8), ('agra', 8), ('shoping', 8), ('mouse', 8), ('despite', 8), ('wher', 8), ('cloudy', 8), ('panic', 8), ('mama', 8), ('apark', 8), ('ocean', 8), ('mad', 8), ('easily', 8), ('luckily', 8), ('tird', 8), ('beutifull', 8), ('often', 8), ('states', 8), ('golden', 8), ('gate', 8), ('sunset', 8), ('fan', 8), ('drama', 8), ('international', 8), ('dig', 8), ('com', 8), ('eating', 8), ('poeple', 8), ('century', 8), ('alia', 8), ('sunday', 8), ('space', 8), ('tought', 8), ('restrant', 8), ('value', 8), ('either', 8), ('....', 8), ('imagin', 8), ('arabic', 8), ('item', 8), ('feature', 8), ('shore', 8), ('thire', 8), ('2004', 8), ('society', 8), ('summary', 8), ('vication', 8), ('cancer', 8), ('day`s', 8), ('maney', 8), ('rock', 8), ('beloved', 8), ('cancel', 8), ('actress', 8), ('base', 8), ('message', 8), ('attention', 8), ('accedent', 8), ('mail', 8), ('trap', 8), ('furthermore', 8), ('painting', 8), ('icecream', 8), ('besides', 8), ('chinese', 8), ('.so', 8), ('cd', 8), ('excellent', 8), ('plae', 8), ('among', 8), ('childern', 8), ('got', 8), ('imagination', 8), ('munich', 8), ('pink', 8), ('nowadays', 8), ('travl', 8), ('latifa', 8), ('iwent', 8), ('.they', 8), ('egyption', 8), ('wander', 8), ('hero', 8), ('immediately', 8), ('canter', 8), ('goto', 8), ('personally', 8), ('decied', 8), ('everytime', 8), ('awful', 8), ('t.v.', 8), ('.but', 8), ('ancient', 8), ('cusin', 8), ('clouth', 8), ('becue', 8), ('agine', 8), ('worke', 8), ('valuable', 8), ('apartment', 8), ('olso', 8), ('grade', 8), ('trouble', 8), ('tak', 8), ('mush', 8), ('accord', 8), ('kindly', 8), ('gam', 8), ('burn', 8), ('burj', 8), ('direct', 8), ('super', 8), ('aet', 8), ('fel', 8), ('pitt', 8), ('animel', 8), ('sa', 8), ('round', 8), ('admire', 8), ('nic', 8), ('musuem', 8), ('truly', 8), ('parke', 8), ('suppose', 8), ('windy', 8), ('climp', 8), ('.then', 8), ('jamie', 8), ('fever', 8), ('cousine', 8), ('safely', 8), ('dicid', 8), ('pest', 8), ('malisya', 8), ('felm', 8), ('ay', 8), ('fujairah', 8), ('pepol', 8), ('marina', 8), ('salem', 8), ('bike', 8), ('islam', 8), ('samurai', 8), ('dubiy', 8), ('jackson', 8), ('cood', 7), ('program', 7), ('programme', 7), ('10:00', 7), ('plea', 7), ('disappointed', 7), ('mostly', 7), ('omen', 7), ('.he', 7), ('punish', 7), ('freeze', 7), ('vaction', 7), ('entire', 7), ('11', 7), ('june', 7), ('slowly', 7), ('reading', 7), ('difficult', 7), ('duck', 7), ('except', 7), ('ruin', 7), ('thus', 7), ('chair', 7), ('ware', 7), ('statue', 7), ('haram', 7), ('kabah', 7), ('haj', 7), ('breeze', 7), ('wint', 7), ('mobail', 7), ('survive', 7), ('weird', 7), ('whom', 7), ('glass', 7), ('memorable', 7), ('battle', 7), ('alin', 7), ('whene', 7), ('witch', 7), ('goat', 7), ('duty', 7), ('anybody', 7), ('interest', 7), ('nem', 7), ('rare', 7), ('zoro', 7), ('describle', 7), ('smoll', 7), ('writ', 7), ('mounth', 7), ('frish', 7), ('specially', 7), ('wort', 7), ('sign', 7), ('wanderful', 7), ('bright', 7), ('sew', 7), ('upon', 7), ('ane', 7), ('goo', 7), ('monky', 7), ('document', 7), ('flay', 7), ('artist', 7), ('west', 7), ('romantic', 7), ('fat', 7), ('paly', 7), ('fotball', 7), ('cinama', 7), ('londen', 7), ('unique', 7), ('ending', 7), ('historic', 7), ('quiet', 7), ('unfortunatly', 7), ('thes', 7), ('gear', 7), ('injoy', 7), ('petrol', 7), ('fantastk', 7), ('destroy', 7), ('jail', 7), ('shower', 7), ('r', 7), ('milk', 7), ('ate', 7), ('igo', 7), ('bool', 7), ('mathar', 7), ('nervous', 7), ('beco', 7), ('staff', 7), ('fry', 7), ('17', 7), ('anti', 7), ('illness', 7), ('hug', 7), ('gain', 7), ('a.m', 7), ('se', 7), ('power', 7), ('maid', 7), ('won`t', 7), ('fountain', 7), ('amaiz', 7), ('corner', 7), ('jump', 7), ('happily', 7), ('fo', 7), ('rubbish', 7), ('fantasy', 7), ('fourth', 7), ('tournament', 7), ('cross', 7), ('force', 7), ('stone', 7), ('ibn', 7), ('mountian', 7), ('musium', 7), ('londan', 7), ('hill', 7), ('tear', 7), ('extra', 7), ('lanch', 7), ('inthe', 7), ('classic', 7), ('indai', 7), ('marwa', 7), ('mol', 7), ('hom', 7), ('nine', 7), ('perfact', 7), ('wok', 7), ('hurry', 7), ('mach', 7), ('hyena', 7), ('divide', 7), ('sydney', 7), ('alon', 7), ('empty', 7), ('thirdly', 7), ('egept', 7), ('favourit', 7), ('alkuwit', 7), ('desk', 7), ('luggage', 7), ('follow', 7), ('magine', 7), ('hapy', 7), ('cloth', 7), ('maryam', 7), ('cinima', 7), ('cantre', 7), ('pocket', 7), ('shooping', 7), ('treatment', 7), ('rise', 7), ('vecation', 7), ('none', 7), ('beautifal', 7), ('lay', 7), ('storm', 7), ('belgium', 7), ('prize', 7), ('operation', 7), ('hamda', 7), ('size', 7), ('john', 7), ('apple', 7), ('ha', 7), ('verry', 7), ('donkey', 7), ('service', 7), ('whe', 7), ('possesion', 7), ('veer', 7), ('zara', 7), ('mam', 6), ('introduce', 6), ('skating', 6), ('wednesday', 6), ('ot', 6), ('mahel', 6), ('travill', 6), ('landon', 6), ('slala', 6), ('scare', 6), ('climate', 6), ('shot', 6), ('hospitel', 6), ('kick', 6), ('condition', 6), ('within', 6), ('piece', 6), ('extremely', 6), ('incident', 6), ('sail', 6), ('amna', 6), ('bough', 6), ('amazed', 6), ('location', 6), ('aday', 6), ('electronic', 6), ('wit', 6), ('hose', 6), ('farther', 6), ('ime', 6), ('geisha', 6), ('friendship', 6), ('fascinating', 6), ('gorgeous', 6), ('recommend', 6), ('organize', 6), ('neer', 6), ('unknown', 6), ('freedom', 6), ('revenge', 6), ('yesterday', 6), ('hav', 6), ('globe', 6), ('gun', 6), (\"did'nt\", 6), ('heavy', 6), ('fear', 6), ('18', 6), ('award', 6), ('comfort', 6), ('sudenly', 6), ('hert', 6), ('anb', 6), ('realise', 6), ('increase', 6), ('delay', 6), ('august', 6), ('farend', 6), ('maha', 6), ('palm', 6), ('skiing', 6), ('colorful', 6), ('church', 6), ('prophet', 6), ('grandmather', 6), ('2006', 6), ('drop', 6), ('doubt', 6), ('writer', 6), ('appear', 6), ('grab', 6), ('sum', 6), ('amany', 6), ('criy', 6), ('hotal', 6), ('sort', 6), ('‘', 6), ('thin', 6), ('hoppy', 6), ('d', 6), ('simply', 6), ('lack', 6), ('freand', 6), ('opportunity', 6), ('bedroom', 6), ('control', 6), ('generation', 6), ('dancing', 6), ('player', 6), ('basket', 6), ('unt', 6), ('preper', 6), ('unless', 6), ('12:00', 6), ('famlay', 6), ('potter', 6), ('cent', 6), ('sik', 6), ('push', 6), ('judge', 6), ('eager', 6), ('beat', 6), ('mainly', 6), ('soul', 6), ('various', 6), ('insist', 6), ('fancy', 6), ('american', 6), ('irport', 6), ('p.m', 6), ('husbend', 6), ('acter', 6), ('humen', 6), ('fond', 6), ('online', 6), ('ancle', 6), ('wa', 6), ('volleyball', 6), ('magazine', 6), ('interisting', 6), ('1999', 6), ('hogwarts', 6), ('capture', 6), ('produce', 6), ('belonging', 6), ('spectacular', 6), ('whose', 6), ('shake', 6), ('throughout', 6), ('whatch', 6), ('chicken', 6), ('y', 6), ('thinking', 6), ('serious', 6), ('spain', 6), ('arabian', 6), ('naice', 6), ('8:00', 6), ('beautful', 6), ('abha', 6), ('kaaba', 6), ('scarf', 6), ('sahara', 6), ('zone', 6), ('disert', 6), ('oclock', 6), ('proplem', 6), ('sudia', 6), ('surgery', 6), ('pc', 6), ('7th', 6), ('hafet', 6), ('wonderland', 6), ('relate', 6), ('wave', 6), ('dabia', 6), ('docter', 6), ('brok', 6), ('aboute', 6), ('hoiday', 6), ('keen', 6), ('dabi', 6), ('un', 6), ('mess', 6), ('hotle', 6), ('butterfly', 6), ('mine', 6), ('relly', 6), ('ra', 6), ('twin', 6), ('hunt', 6), ('plain', 6), ('pencil', 6), ('faith', 6), ('buetiful', 6), ('happay', 6), ('fir', 6), ('maximos', 6), ('fatar', 6), ('samer', 6), ('.because', 6), ('suffer', 6), ('diamond', 6), ('rashid', 6), ('placess', 6), ('exhaust', 6), ('seeing', 6), ('gacehun', 6), ('osaka', 6), ('fotbool', 6), ('resterant', 6), ('saturday', 6), ('whent', 6), ('saty', 6), ('boston', 6), ('senter', 6), ('doughter', 6), ('bridge', 6), ('hell', 6), ('minut', 6), ('oxford', 6), ('inform', 6), ('fix', 6), ('smill', 6), ('vimpyr', 6), ('dibba', 6), ('sudai', 6), ('don', 6), ('east', 6), ('parant', 6), ('beth', 6), ('spicy', 6), ('canada', 6), ('accept', 6), ('eight', 6), ('wondrfful', 6), ('amisha', 6), ('nora', 6), ('kl', 6), ('cand', 6), ('rajo', 6), ('eggpt', 6), ('nerve', 5), ('lonely', 5), ('programe', 5), ('project', 5), ('variety', 5), ('successful', 5), ('sammer', 5), ('cantrey', 5), ('l', 5), ('noon', 5), ('singing', 5), ('farmer', 5), ('remamber', 5), ('nex', 5), ('habit', 5), ('smoke', 5), ('nai', 5), ('nay', 5), ('lest', 5), ('wonderfal', 5), ('religion', 5), ('rainn', 5), ('comp', 5), ('sty', 5), ('indeed', 5), ('depressed', 5), ('baba', 5), ('resort', 5), ('pople', 5), ('huda', 5), ('popcorn', 5), ('ungry', 5), ('9:00', 5), ('miserable', 5), ('washington', 5), ('masjed', 5), ('sandy', 5), ('bar', 5), ('struggle', 5), ('alkaba', 5), ('horror', 5), ('technology', 5), ('sailor', 5), ('seek', 5), ('purpose', 5), ('ca', 5), ('bread', 5), ('map', 5), ('rarely', 5), ('college', 5), ('cycle', 5), ('bach', 5), ('faster', 5), ('source', 5), ('release', 5), ('cave', 5), ('patience', 5), ('jon', 5), ('mary', 5), ('weath', 5), ('u.s.a.', 5), ('reflect', 5), ('living', 5), ('cinme', 5), ('upset', 5), ('toward', 5), ('bath', 5), ('directly', 5), ('specialy', 5), ('honestly', 5), ('cloud', 5), ('wathe', 5), ('hight', 5), ('sean', 5), ('whet', 5), ('creature', 5), ('straight', 5), ('key', 5), ('belong', 5), ('musum', 5), ('further', 5), ('desire', 5), ('mohamed', 5), ('hir', 5), ('adore', 5), ('persone', 5), ('theater', 5), ('distroy', 5), ('tech', 5), ('common', 5), ('unity', 5), ('plot', 5), ('perform', 5), ('el', 5), ('escape', 5), ('tack', 5), ('ii', 5), ('fashion', 5), ('bass', 5), ('serve', 5), ('fery', 5), ('thet', 5), ('culb', 5), ('walking', 5), ('afra', 5), ('salt', 5), ('barely', 5), ('us', 5), ('shos', 5), ('scar', 5), ('cure', 5), ('wan', 5), ('fimely', 5), ('bard', 5), ('sheep', 5), ('dish', 5), ('clim', 5), ('camil', 5), ('barj', 5), ('cary', 5), ('ham', 5), ('swimm', 5), ('brown', 5), ('goblet', 5), ('ignore', 5), ('leat', 5), ('trafic', 5), ('wate', 5), ('angre', 5), ('dubie', 5), ('repair', 5), ('othe', 5), ('clud', 5), ('cantery', 5), ('tray', 5), ('nor', 5), ('issue', 5), ('entertain', 5), ('moral', 5), ('exist', 5), ('pillow', 5), ('ti', 5), ('pressure', 5), ('saster', 5), ('shy', 5), ('conversation', 5), ('retur', 5), ('juice', 5), ('ac', 5), ('achieve', 5), ('asia', 5), ('relaxed', 5), ('mony', 5), ('cake', 5), ('hotell', 5), ('orlando', 5), ('frodo', 5), ('snak', 5), ('agen', 5), ('stable', 5), ('secoundly', 5), ('creative', 5), ('apart', 5), ('hall', 5), ('thear', 5), ('polic', 5), ('enjoyment', 5), ('horn', 5), ('refreshing', 5), ('object', 5), ('avoid', 5), ('deliver', 5), ('excitement', 5), ('careful', 5), ('wadi', 5), ('chiken', 5), ('gang', 5), ('contact', 5), ('tink', 5), ('cell', 5), ('bangkok', 5), ('purchase', 5), ('uncel', 5), ('hafit', 5), ('enjo', 5), ('handball', 5), ('ths', 5), ('swiss', 5), ('becus', 5), ('naic', 5), ('beutful', 5), ('interist', 5), ('sunrise', 5), ('fainlly', 5), ('totally', 5), ('clook', 5), ('becauce', 5), ('remmeber', 5), ('nicely', 5), ('c', 5), ('fastival', 5), ('relation', 5), ('macca', 5), ('begining', 5), ('2003', 5), ('asleep', 5), ('boss', 5), ('plenty', 5), ('nevertheless', 5), ('hundred', 5), ('sop', 5), ('slope', 5), ('sightseeing', 5), ('wnt', 5), ('funniest', 5), ('sultan', 5), ('page', 5), ('adress', 5), ('cast', 5), ('married', 5), ('sumer', 5), ('londone', 5), ('f', 5), ('brain', 5), ('leisure', 5), ('parking', 5), ('abdulla', 5), ('.at', 5), ('kuwit', 5), ('mai', 5), ('monday', 5), ('drow', 5), ('moza', 5), ('o`clock', 5), ('imagen', 5), ('paint', 5), ('travll', 5), ('annoying', 5), ('combine', 5), ('trust', 5), ('sale', 5), ('aisha', 5), ('omer', 5), ('bo', 5), ('animale', 5), ('african', 5), ('attend', 5), ('fran', 5), ('manth', 5), ('ment', 5), ('soft', 5), ('x', 5), ('youer', 5), ('settle', 5), ('abook', 5), ('hove', 5), ('faiut', 5), ('sam', 5), ('weekand', 5), ('dobai', 5), ('success', 5), ('diffirent', 5), ('classmate', 5), ('mosqe', 5), ('afri', 5), ('occur', 5), ('meny', 5), ('islamic', 5), ('goal', 5), ('hollyday', 5), ('inorder', 5), ('moore', 5), ('butefull', 5), ('er', 5), ('sate', 5), ('palace', 5), ('presentation', 5), ('sarah', 5), ('kitchen', 5), ('fathier', 5), ('muh', 5), ('diffcult', 5), ('mak', 5), ('you', 5), ('accessory', 5), ('becaue', 5), ('ancal', 5), ('poster', 5), ('tanzania', 5), ('terry', 5), ('worset', 5), ('contrie', 5), ('playstation', 5), ('rusell', 5), ('d15', 5), ('confused', 5), ('palestinian', 5), ('salam', 5), ('guideman', 5), ('forst', 5), ('duadi', 5), ('sharukhan', 5), ('rifle', 5), ('fared', 5), ('stree', 4), ('shold', 4), ('litter', 4), ('institute', 4), ('riding', 4), ('professional', 4), ('famli', 4), ('23', 4), ('beging', 4), ('avrey', 4), ('practice', 4), ('mumbai', 4), ('attractive', 4), ('forword', 4), ('vry', 4), ('smmer', 4), ('wach', 4), ('organise', 4), ('shoud', 4), ('becoue', 4), ('swmming', 4), ('manke', 4), ('.she', 4), ('mickey', 4), ('  ', 4), ('hapend', 4), ('tool', 4), ('plate', 4), ('support', 4), ('surprisingly', 4), ('blame', 4), ('booking', 4), ('disneyland', 4), ('lover', 4), ('hours', 4), ('mud', 4), ('reem', 4), ('seychelles', 4), ('bicycle', 4), ('provide', 4), ('funtastic', 4), ('cousen', 4), ('agin', 4), ('holidays', 4), ('los', 4), ('sin', 4), ('region', 4), ('singer', 4), ('atmosphere', 4), ('becouz', 4), ('sinter', 4), ('noore', 4), ('pak', 4), ('directing', 4), ('tie', 4), ('zamzam', 4), ('thriller', 4), ('fiance', 4), ('fake', 4), ('fifteen', 4), ('swime', 4), ('minet', 4), ('drank', 4), ('alhili', 4), ('played', 4), ('babys', 4), ('streat', 4), ('englis', 4), ('hen', 4), ('afride', 4), ('helthy', 4), ('drag', 4), ('scenery', 4), ('local', 4), ('educational', 4), ('canda', 4), ('louvre', 4), ('suit', 4), ('frinds', 4), ('salim', 4), ('add', 4), ('farst', 4), ('sword', 4), ('alive', 4), ('attract', 4), ('masirah', 4), ('colourful', 4), ('ou', 4), ('ferst', 4), ('tomb', 4), ('massive', 4), ('cab', 4), ('reservation', 4), ('metal', 4), ('shirt', 4), ('falim', 4), ('plac', 4), ('mor', 4), ('beacause', 4), ('loss', 4), ('mimzer', 4), ('ara', 4), ('cenema', 4), ('countre', 4), ('famaliy', 4), ('fin', 4), ('mi', 4), ('beatful', 4), ('fourteen', 4), ('toll', 4), ('prepair', 4), ('sue', 4), ('social', 4), ('oscar', 4), ('beache', 4), ('monke', 4), ('emarat', 4), ('clamp', 4), ('foreign', 4), ('nae', 4), ('wekend', 4), ('kidnap', 4), ('mood', 4), ('thousand', 4), ('isee', 4), ('baut', 4), ('recall', 4), ('whin', 4), ('prather', 4), ('caw', 4), ('twenty', 4), ('15th', 4), ('rule', 4), ('universe', 4), ('rings', 4), ('worth', 4), ('fins', 4), ('vilige', 4), ('anger', 4), ('anemal', 4), ('becuose', 4), ('feeld', 4), ('deid', 4), ('sailing', 4), ('roud', 4), ('cultur', 4), ('novel', 4), ('mohamm', 4), ('emirate', 4), ('grean', 4), ('summur', 4), ('brathe', 4), ('tayer', 4), ('encourage', 4), ('seed', 4), ('agent', 4), ('muslims', 4), ('arabs', 4), ('rose', 4), ('priest', 4), ('possess', 4), ('demon', 4), ('permission', 4), ('warn', 4), ('somewhere', 4), ('mariam', 4), ('medical', 4), ('knowledge', 4), ('remove', 4), ('medicine', 4), ('afew', 4), ('musume', 4), ('nada', 4), ('isaw', 4), ('afther', 4), ('custom', 4), ('tourism', 4), ('thair', 4), ('toye', 4), ('café', 4), ('mome', 4), ('fainaly', 4), ('pad', 4), ('eid', 4), ('import', 4), ('5:00', 4), ('oure', 4), ('file', 4), ('heaven', 4), ('houme', 4), ('travelling', 4), ('north', 4), ('borrow', 4), ('arm', 4), ('thre', 4), ('alexandria', 4), ('gloomy', 4), ('motel', 4), ('famaily', 4), ('colleg', 4), ('longer', 4), ('brothr', 4), ('connection', 4), ('j', 4), ('cock', 4), ('fair', 4), ('lave', 4), ('pictur', 4), ('list', 4), ('thought', 4), ('beath', 4), ('disapoint', 4), ('our', 4), ('temple', 4), ('3:00', 4), ('hata', 4), ('laughter', 4), ('perfectly', 4), ('countinu', 4), ('closer', 4), ('tal', 4), ('anymore', 4), ('clearly', 4), ('bougth', 4), ('december', 4), ('dosent', 4), ('reader', 4), ('stol', 4), ('g', 4), ('climbing', 4), ('usefull', 4), ('nap', 4), ('best', 4), ('feell', 4), ('geam', 4), ('sastar', 4), ('ho', 4), ('remmember', 4), ('siter', 4), ('damage', 4), ('manager', 4), ('injure', 4), ('murder', 4), ('sk', 4), ('u.k', 4), ('jabl', 4), ('monten', 4), ('2002', 4), ('courchevel', 4), ('brand', 4), ('television', 4), ('glad', 4), ('emam', 4), ('veare', 4), ('stard', 4), ('veary', 4), ('notebook', 4), ('south', 4), ('abdabi', 4), ('tag', 4), ('hime', 4), ('lote', 4), ('business', 4), ('monthe', 4), ('smith', 4), ('unforgetable', 4), ('lione', 4), ('dudie', 4), ('office', 4), ('si', 4), ('ad', 4), ('skit', 4), ('mc', 4), ('phee', 4), ('roam', 4), ('disaster', 4), ('visitor', 4), ('aswell', 4), ('ancul', 4), ('memorise', 4), ('wy', 4), ('franc', 4), ('impossible', 4), ('accompany', 4), ('pleasure', 4), ('unforgettable', 4), ('zebra', 4), ('satisfied', 4), ('vocation', 4), ('st', 4), ('southern', 4), ('describ', 4), ('forgat', 4), ('ab', 4), ('samr', 4), ('whan', 4), ('officer', 4), ('truth', 4), ('untel', 4), ('cheerful', 4), ('anather', 4), ('sitey', 4), ('diecid', 4), ('toe', 4), ('silly', 4), ('non', 4), ('siad', 4), ('ambulance', 4), ('patient', 4), ('fraiday', 4), ('awake', 4), ('flow', 4), ('primary', 4), ('bigan', 4), ('ren', 4), ('aba', 4), ('kingdom', 4), ('woter', 4), ('eara', 4), ('abudabi', 4), ('activetie', 4), ('wady', 4), ('poetry', 4), ('butifal', 4), ('hopy', 4), ('inspire', 4), ('appreciate', 4), ('smoking', 4), ('failm', 4), ('wenderful', 4), ('wonerful', 4), ('soil', 4), ('ply', 4), ('ina', 4), ('flim', 4), ('wondrful', 4), ('liar', 4), ('regret', 4), ('und', 4), ('brether', 4), ('awonderful', 4), ('sade', 4), ('ahotel', 4), ('pearl', 4), ('yacht', 4), ('infact', 4), ('eld', 4), ('doy', 4), ('midnight', 4), ('barke', 4), ('specific', 4), ('affect', 4), ('contribute', 4), ('million', 4), ('nationality', 4), ('hep', 4), ('hart', 4), ('barlk', 4), ('attack', 4), ('faruch', 4), ('areally', 4), ('restuarant', 4), ('antel', 4), ('whether', 4), ('skirt', 4), ('dvd', 4), ('stady', 4), ('denar', 4), ('sey', 4), ('beacous', 4), ('amal', 4), ('hind', 4), ('ie', 4), ('musim', 4), ('bangkook', 4), ('bite', 4), ('harrods', 4), ('boll', 4), ('onto', 4), ('thirty', 4), ('tam', 4), ('lok', 4), ('–', 4), ('mobazara', 4), ('pley', 4), ('hotiel', 4), ('forrest', 4), ('drug', 4), ('idia', 4), ('adel', 4), ('eirport', 4), ('abd', 4), ('hosbitel', 4), ('ner', 4), ('halthy', 4), ('ache', 4), ('score', 4), ('makkah', 4), ('previous', 4), ('egg', 4), ('reserve', 4), ('aclock', 4), ('transform', 4), ('sistere', 4), ('quter', 4), ('tofaha', 4), ('boyfriend', 4), ('masafi', 4), ('barther', 4), ('activite', 4), ('theree', 4), ('rember', 4), ('coffe', 4), ('nintendo', 4), ('carpet', 4), ('awar', 4), ('jast', 4), ('vennice', 4), ('um', 4), ('fog', 4), ('yeman', 4), ('lost', 4), ('khaima', 4), ('similar', 4), ('mountion', 4), ('rescue', 4), ('emarits', 4), ('hong', 4), ('etc', 4), ('rase', 4), ('gierl', 4), ('watr', 4), ('honest', 4), ('detail', 4), ('cluld', 4), ('supermarket', 4), ('jungle', 4), ('jop', 4), ('toilet', 4), ('mohammad', 4), ('auncle', 4), ('houston', 4), ('quicly', 4), ('clever', 4), ('amongst', 4), ('rean', 4), ('kalba', 4), ('greet', 4), ('dabbi', 4), ('volley', 4), ('khan', 4), ('acar', 4), ('madena', 4), ('saudia', 4), ('deseart', 4), ('samar', 4), ('dide', 4), ('crystal', 4), ('plying', 4), ('e70', 4), ('brathar', 4), ('stell', 4), ('seck', 4), ('brussels', 4), ('shadow', 4), ('opend', 4), ('giev', 4), ('bizou', 4), ('singapore', 4), ('moma', 4), ('profet', 4), ('furniture', 4), ('research', 4), ('excercic', 4), ('luxurious', 4), ('bestre', 4), ('amarican', 4), ('wad', 4), ('mater', 4), ('lindy', 4), ('hospittal', 4), ('jasmin', 4), ('rollerblade', 4), ('jun', 4), ('desairt', 4), ('antey', 4), ('hssne', 4), ('bangladesh', 4), ('mac', 4), ('shek', 4), ('sharm', 4), ('pecoze', 4), ('dearty', 3), ('earia', 3), ('maby', 3), ('.there', 3), ('vedio', 3), ('field', 3), ('concentrate', 3), ('section', 3), ('shoot', 3), ('fren', 3), ('headache', 3), ('noty', 3), ('bahrin', 3), ('cher', 3), ('tina', 3), ('disturb', 3), ('refresh', 3), ('dudi', 3), ('ster', 3), ('borther', 3), ('hape', 3), ('thired', 3), ('re', 3), ('saven', 3), ('bomb', 3), ('slep', 3), ('gril', 3), ('swimmg', 3), ('whoyou', 3), ('wounderland', 3), ('aunty', 3), ('luckly', 3), ('terrify', 3), ('\\x1e', 3), ('¦', 3), ('fruite', 3), ('hwo', 3), ('ta', 3), ('bee', 3), ('snap', 3), ('slip', 3), ('islands', 3), ('fantactic', 3), ('recognize', 3), ('dwon', 3), ('sould', 3), ('hor', 3), ('amusement', 3), ('ahead', 3), ('amusing', 3), ('national', 3), ('routine', 3), ('liberty', 3), ('la', 3), ('brothers', 3), ('universal', 3), ('dolphin', 3), ('snik', 3), ('fimily', 3), ('suadia', 3), ('lovly', 3), ('planet', 3), ('tokyo', 3), ('ig', 3), ('yer', 3), ('frenid', 3), ('rough', 3), ('acting', 3), ('possible', 3), ('bt', 3), ('secand', 3), ('monte', 3), ('importantly', 3), ('identity', 3), ('waite', 3), ('maive', 3), ('jone', 3), ('studing', 3), ('medicen', 3), ('gome', 3), ('exame', 3), ('entrance', 3), ('cafe', 3), ('exhausted', 3), ('preplem', 3), ('agane', 3), ('pole', 3), ('bage', 3), ('bal', 3), ('wnderful', 3), ('gree', 3), ('accedint', 3), ('frustrated', 3), ('places', 3), ('afun', 3), ('skateboard', 3), ('surface', 3), ('recover', 3), ('ability', 3), ('twice', 3), ('attempt', 3), ('trick', 3), ('farmar', 3), ('eivel', 3), ('buitful', 3), ('handle', 3), ('defend', 3), ('particular', 3), ('greatly', 3), ('strike', 3), ('mothar', 3), ('brank', 3), ('frande', 3), ('fistival', 3), ('matar', 3), ('hely', 3), ('aboard', 3), ('inspite', 3), ('usual', 3), ('lousy', 3), ('concert', 3), ('thif', 3), ('tiered', 3), ('betch', 3), ('famity', 3), ('anothe', 3), ('paradise', 3), ('austria', 3), ('transportation', 3), ('poland', 3), ('distance', 3), ('hote', 3), ('maisor', 3), ('mohammd', 3), ('jet', 3), ('relaxation', 3), ('swimmig', 3), ('duabi', 3), ('28th', 3), ('istanbul', 3), ('fnish', 3), ('shall', 3), ('setting', 3), ('ne', 3), ('fter', 3), ('roman', 3), ('sisiter', 3), ('mode', 3), ('tirde', 3), ('gress', 3), ('mar', 3), ('importan', 3), ('naise', 3), ('twelve', 3), ('loose', 3), ('liv', 3), ('senma', 3), ('nominate', 3), ('plaza', 3), ('snack', 3), ('mombai', 3), ('bly', 3), ('everday', 3), ('let`s', 3), ('wot', 3), ('sh', 3), ('swiming', 3), ('drea', 3), ('holidey', 3), ('hence', 3), ('mission', 3), ('definitely', 3), ('beacus', 3), ('keept', 3), ('khalid', 3), ('saeed', 3), ('roller', 3), ('coaster', 3), ('arcade', 3), ('beateful', 3), ('diver', 3), ('cour', 3), ('license', 3), ('thy', 3), ('il', 3), ('ithe', 3), ('sissy', 3), ('smelly', 3), ('prayer', 3), ('impact', 3), ('mutch', 3), ('email', 3), ('pull', 3), ('volly', 3), ('worm', 3), ('comedian', 3), ('guard', 3), ('accomplish', 3), ('uncomfortable', 3), ('lesen', 3), ('bod', 3), ('zoon', 3), ('defrent', 3), ('thenk', 3), ('tiland', 3), ('salad', 3), ('couse', 3), ('term', 3), ('cosen', 3), ('hospetal', 3), ('yellow', 3), ('boot', 3), ('comptition', 3), ('seventeen', 3), ('following', 3), ('taim', 3), ('dat', 3), ('activety', 3), ('machin', 3), ('thng', 3), ('begaining', 3), ('hay', 3), ('hangry', 3), ('mate', 3), ('clous', 3), ('israeli', 3), ('recent', 3), ('spirit', 3), ('wise', 3), ('inst', 3), ('harb', 3), ('eiffel', 3), ('aspect', 3), ('being', 3), ('ia', 3), ('resist', 3), ('somebody', 3), ('shark', 3), ('heir', 3), ('perent', 3), ('shin', 3), ('ourself', 3), ('arabi', 3), ('nous', 3), ('afried', 3), ('economy', 3), ('kindness', 3), ('depend', 3), ('friand', 3), ('mobil', 3), ('geat', 3), ('hollywood', 3), ('bloom', 3), ('devil', 3), ('deser', 3), ('gev', 3), ('onther', 3), ('hoildy', 3), ('salal', 3), ('howe', 3), ('occupy', 3), ('anthor', 3), ('tress', 3), ('wonderfol', 3), ('closed', 3), ('triwizard', 3), ('champion', 3), ('graphic', 3), ('deci', 3), ('untile', 3), ('teenager', 3), ('broke', 3), ('vaccation', 3), ('description', 3), ('wallet', 3), ('plice', 3), ('trovel', 3), ('responsible', 3)]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at the thing\n",
    "balc_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(the, 13677)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(and, 10492)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(be, 10213)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(to, 8692)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(in, 6122)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pair\n",
       "0  (the, 13677)\n",
       "1  (and, 10492)\n",
       "2   (be, 10213)\n",
       "3    (to, 8692)\n",
       "4    (in, 6122)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a pandas dataframe with the tuples of (lem, count)\n",
    "lemma_counts = pd.DataFrame(balc_lemmas, index=[0]).T\n",
    "lemma_counts.columns = ['pair']\n",
    "lemma_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "13677"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the lemma \n",
    "def get_lem(x):\n",
    "    return x[0]\n",
    "\n",
    "# get the count\n",
    "def get_ct(x):\n",
    "    return x[1]\n",
    "\n",
    "# test\n",
    "get_lem(('the', 13677))\n",
    "get_ct(('the', 13677))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair</th>\n",
       "      <th>lemma</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(the, 13677)</td>\n",
       "      <td>the</td>\n",
       "      <td>13677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(and, 10492)</td>\n",
       "      <td>and</td>\n",
       "      <td>10492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(be, 10213)</td>\n",
       "      <td>be</td>\n",
       "      <td>10213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(to, 8692)</td>\n",
       "      <td>to</td>\n",
       "      <td>8692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(in, 6122)</td>\n",
       "      <td>in</td>\n",
       "      <td>6122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pair lemma  counts\n",
       "0  (the, 13677)   the   13677\n",
       "1  (and, 10492)   and   10492\n",
       "2   (be, 10213)    be   10213\n",
       "3    (to, 8692)    to    8692\n",
       "4    (in, 6122)    in    6122"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new columns\n",
    "lemma_counts['lemma'] = lemma_counts.pair.map(get_lem)\n",
    "lemma_counts['counts'] = lemma_counts.pair.map(get_ct)\n",
    "lemma_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>13677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>10492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>be</td>\n",
       "      <td>10213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>8692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in</td>\n",
       "      <td>6122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lemma  counts\n",
       "0   the   13677\n",
       "1   and   10492\n",
       "2    be   10213\n",
       "3    to    8692\n",
       "4    in    6122"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop 'pair' column\n",
    "lemma_counts = lemma_counts.drop(columns=['pair'])\n",
    "lemma_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving lemma counts out to a csv file for later use\n",
    "lemma_counts.to_csv(\"../private/filtered_balc_lemmas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving cepa files out to csv file in the private folder\n",
    "cepa_df.to_csv(\"../private/cepa.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
